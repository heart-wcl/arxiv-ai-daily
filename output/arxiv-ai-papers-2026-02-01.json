{
  "timestamp": "2026-02-01T15:49:45.148Z",
  "count": 116,
  "papers": [
    {
      "id": "http://arxiv.org/abs/2601.22159v1",
      "title": "RedSage: A Cybersecurity Generalist LLM",
      "originalTitle": "RedSage: A Cybersecurity Generalist LLM",
      "summary": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, ...",
      "plainSummary": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation 数据处理或模型训练的完整流程（undefined） that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised 在预训练模型基础上进行小幅调整（undefined）. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a 用于比较性能的标准数据集或方法（undefined） with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the 用于对比的基准方法（undefined） models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.",
      "oneSentenceSummary": "【cs.CR】Naufal Suryanto等RedSage，在cs.CR取得新进展。",
      "authors": [
        {
          "original": "Naufal Suryanto",
          "chinese": null
        },
        {
          "original": "Muzammal Naseer",
          "chinese": null
        },
        {
          "original": "Pengfei Li",
          "chinese": null
        },
        {
          "original": "Syed Talal Wasim",
          "chinese": null
        },
        {
          "original": "Jinhui Yi",
          "chinese": null
        },
        {
          "original": "Juergen Gall",
          "chinese": null
        },
        {
          "original": "Paolo Ceravolo",
          "chinese": null
        },
        {
          "original": "Ernesto Damiani",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:59:57Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primaryCategory": "cs.CR",
      "pdfUrl": "https://arxiv.org/pdf/2601.22159v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22159v1",
      "keyInfo": {
        "contributions": [
          "Building on this, we design an agentic augmentation 数据处理或模型训练的完整流程（undefined） that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised 在预训练模型基础上进行小幅调整（undefined）",
          "To rigorously evaluate the models, we introduce RedSage-Bench, a 用于比较性能的标准数据集或方法（undefined） with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22158v1",
      "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "originalTitle": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "summary": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a 数据的压缩表示空间（undefined）. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" ...",
      "plainSummary": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a 数据的压缩表示空间（undefined）. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
      "oneSentenceSummary": "【cs.CV】Yiyang Lu等One-step Latent-free Image Generation with Pixel Mean Flows，使用Modern diffusion/flow-based mo...，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Yiyang Lu",
          "chinese": null
        },
        {
          "original": "Susie Lu",
          "chinese": null
        },
        {
          "original": "Qiao Sun",
          "chinese": null
        },
        {
          "original": "Hanhong Zhao",
          "chinese": null
        },
        {
          "original": "Zhicheng Jiang",
          "chinese": null
        },
        {
          "original": "Xianbang Wang",
          "chinese": null
        },
        {
          "original": "Tianhong Li",
          "chinese": null
        },
        {
          "original": "Zhengyang Geng",
          "chinese": null
        },
        {
          "original": "Kaiming He",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:59:56Z",
      "categories": [
        "cs.CV"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22158v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22158v1",
      "keyInfo": {
        "contributions": [
          "In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF)",
          "The network target is designed to be on a presumed low-dimensional image manifold (i"
        ],
        "methods": [
          "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a 数据的压缩表示空间（undefined）"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22157v1",
      "title": "Discovering Hidden Gems in Model Repositories",
      "originalTitle": "Discovering Hidden Gems in Model Repositories",
      "summary": "Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects 速度快、资源消耗少（undefined） market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", un...",
      "plainSummary": "Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects 速度快、资源消耗少（undefined） market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. 我们的方法 retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.",
      "oneSentenceSummary": "【cs.LG】Jonathan Kahana等Discovering Hidden Gems in Model Repositories，使用We therefore formulate model d...，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Jonathan Kahana",
          "chinese": null
        },
        {
          "original": "Eliahu Horwitz",
          "chinese": null
        },
        {
          "original": "Yedid Hoshen",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:59:55Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22157v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22157v1",
      "keyInfo": {
        "contributions": [],
        "methods": [
          "We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22156v1",
      "title": "Hybrid Linear Attention Done Right: 速度快、资源消耗少（undefined） Distillation and Effective Architectures for Extremely Long Contexts",
      "originalTitle": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
      "summary": "Hybrid 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks ...",
      "plainSummary": "Hybrid 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined）-based models. In this paper, we present HALO (Hybrid Attention via Layer 寻找最佳参数或解决方案的过程（undefined）), a 数据处理或模型训练的完整流程（undefined） for distilling 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a 创新的、前人未做过的（undefined） position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data",
      "oneSentenceSummary": "【cs.CL】Yingfa Chen等Hybrid Linear Attention Done Right，使用We convert the Qwen3 series in...，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Yingfa Chen",
          "chinese": null
        },
        {
          "original": "Zhen Leng Thai",
          "chinese": null
        },
        {
          "original": "Zihan Zhou",
          "chinese": null
        },
        {
          "original": "Zhu Zhang",
          "chinese": null
        },
        {
          "original": "Xingyu Shen",
          "chinese": null
        },
        {
          "original": "Shuo Wang",
          "chinese": null
        },
        {
          "original": "Chaojun Xiao",
          "chinese": null
        },
        {
          "original": "Xu Han",
          "chinese": null
        },
        {
          "original": "Zhiyuan Liu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:59:53Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22156v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22156v1",
      "keyInfo": {
        "contributions": [
          "In this paper, we present HALO (Hybrid Attention via Layer 寻找最佳参数或解决方案的过程（undefined）), a 数据处理或模型训练的完整流程（undefined） for distilling 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） models into RNN-attention hybrid models",
          "We then present HypeNet, a hybrid architecture with superior length generalization enabled by a 创新的、前人未做过的（undefined） position encoding scheme (named HyPE) and various architectural modifications"
        ],
        "methods": [
          "We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） models while enjoying superior long-context performance and efficiency"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22154v1",
      "title": "Exploring Reasoning Reward Model for Agents",
      "originalTitle": "Exploring Reasoning Reward Model for Agents",
      "summary": "Agentic 通过试错学习最佳策略的机器学习方法（undefined） (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), ...",
      "plainSummary": "Agentic 通过试错学习最佳策略的机器学习方法（undefined） (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.",
      "oneSentenceSummary": "【cs.AI】Kaixuan Fan等Exploring Reasoning Reward Model for Agents，在cs.AI取得新进展。",
      "authors": [
        {
          "original": "Kaixuan Fan",
          "chinese": null
        },
        {
          "original": "Kaituo Feng",
          "chinese": null
        },
        {
          "original": "Manyuan Zhang",
          "chinese": null
        },
        {
          "original": "Tianshuo Peng",
          "chinese": null
        },
        {
          "original": "Zhixun Li",
          "chinese": null
        },
        {
          "original": "Yilei Jiang",
          "chinese": null
        },
        {
          "original": "Shuang Chen",
          "chinese": null
        },
        {
          "original": "Peng Pei",
          "chinese": null
        },
        {
          "original": "Xunliang Cai",
          "chinese": null
        },
        {
          "original": "Xiangyu Yue",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:59:52Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2601.22154v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22154v1",
      "keyInfo": {
        "contributions": [
          "In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22155v1",
      "title": "UEval: A 用于比较性能的标准数据集或方法（undefined） for Unified Multimodal Generation",
      "originalTitle": "UEval: A Benchmark for Unified Multimodal Generation",
      "summary": "We introduce UEval, a 用于比较性能的标准数据集或方法（undefined） to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-end...",
      "plainSummary": "We introduce UEval, a 用于比较性能的标准数据集或方法（undefined） to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text 正确预测占总预测的比例（undefined）, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling 能够处理更大规模数据（undefined） and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.",
      "oneSentenceSummary": "【cs.CV】Bo Li等UEval，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Bo Li",
          "chinese": null
        },
        {
          "original": "Yida Yin",
          "chinese": null
        },
        {
          "original": "Wenhao Chai",
          "chinese": null
        },
        {
          "original": "Xingyu Fu",
          "chinese": null
        },
        {
          "original": "Zhuang Liu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:59:52Z",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22155v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22155v1",
      "keyInfo": {
        "contributions": [
          "We introduce UEval, a 用于比较性能的标准数据集或方法（undefined） to evaluate unified models, i",
          "Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text 正确预测占总预测的比例（undefined）, we design a rubric-based scoring system in UEval"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22153v1",
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "originalTitle": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a 提供结构的基础代码库（undefined） for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptati...",
      "plainSummary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a 提供结构的基础代码库（undefined） for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially 速度快、资源消耗少（undefined）, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) 用于比较性能的标准数据集或方法（undefined）, built from scratch with an auto data collection 数据处理或模型训练的完整流程（undefined） that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified 提供结构的基础代码库（undefined） for general dynamic object manipulation across embodiments.",
      "oneSentenceSummary": "【cs.RO】Haozhe Xie等DynamicVLA，使用4B VLA using a convolutional v...，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Haozhe Xie",
          "chinese": null
        },
        {
          "original": "Beichen Wen",
          "chinese": null
        },
        {
          "original": "Jiarui Zheng",
          "chinese": null
        },
        {
          "original": "Zhaoxi Chen",
          "chinese": null
        },
        {
          "original": "Fangzhou Hong",
          "chinese": null
        },
        {
          "original": "Haiwen Diao",
          "chinese": null
        },
        {
          "original": "Ziwei Liu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:59:51Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.22153v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22153v1",
      "keyInfo": {
        "contributions": [
          "We present DynamicVLA, a 提供结构的基础代码库（undefined） for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0",
          "To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) 用于比较性能的标准数据集或方法（undefined）, built from scratch with an auto data collection 数据处理或模型训练的完整流程（undefined） that efficiently gathers 200K synthetic episodes across 2"
        ],
        "methods": [
          "4B VLA using a convolutional vision encoder for spatially 速度快、资源消耗少（undefined）, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution"
        ],
        "applications": [
          "8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22151v1",
      "title": "Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing",
      "originalTitle": "Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing",
      "summary": "Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. 当前最好的、领先的方法（undefined） research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing suc...",
      "plainSummary": "Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. 当前最好的、领先的方法（undefined） research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, 我们提出 to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any 正确预测占总预测的比例（undefined） degradation. The code is open source at https://github.com/TUDa-HWAI/NN2Logic",
      "oneSentenceSummary": "【cs.LG】Daniel Stein等Late Breaking Results，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Daniel Stein",
          "chinese": null
        },
        {
          "original": "Shaoyi Huang",
          "chinese": null
        },
        {
          "original": "Rolf Drechsler",
          "chinese": null
        },
        {
          "original": "Bing Li",
          "chinese": null
        },
        {
          "original": "Grace Li Zhang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:59:50Z",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22151v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22151v1",
      "keyInfo": {
        "contributions": [
          "To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22150v1",
      "title": "Do VLMs Perceive or 真正正例中被正确预测的比例（undefined）? Probing Visual Perception vs. Memory with Classic Visual Illusions",
      "originalTitle": "Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions",
      "summary": "Large Vision-Language Models (VLMs) often answer classic visual illusions \"correctly\" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely 真正正例中被正确预测的比例（undefined） memorized patterns? While several studies have noted this phenomeno...",
      "plainSummary": "Large Vision-Language Models (VLMs) often answer classic visual illusions \"correctly\" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely 真正正例中被正确预测的比例（undefined） memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion 提供结构的基础代码库（undefined） with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven 真正正例中被正确预测的比例（undefined）. Unlike 观察到数据前的概率（undefined） work that focuses on averaged 正确预测占总预测的比例（undefined）, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.",
      "oneSentenceSummary": "【cs.CV】Xiaoxiao Sun等Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions，使用Unlike 观察到数据前的概率（undefined） wo...，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Xiaoxiao Sun",
          "chinese": null
        },
        {
          "original": "Mingyang Li",
          "chinese": null
        },
        {
          "original": "Kun yuan",
          "chinese": null
        },
        {
          "original": "Min Woo Sun",
          "chinese": null
        },
        {
          "original": "Mark Endo",
          "chinese": null
        },
        {
          "original": "Shengguang Wu",
          "chinese": null
        },
        {
          "original": "Changlin Li",
          "chinese": null
        },
        {
          "original": "Yuhui Zhang",
          "chinese": null
        },
        {
          "original": "Zeyu Wang",
          "chinese": null
        },
        {
          "original": "Serena Yeung-Levy",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:59:24Z",
      "categories": [
        "cs.CV"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22150v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22150v1",
      "keyInfo": {
        "contributions": [
          "To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion 提供结构的基础代码库（undefined） with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven 真正正例中被正确预测的比例（undefined）"
        ],
        "methods": [
          "Unlike 观察到数据前的概率（undefined） work that focuses on averaged 正确预测占总预测的比例（undefined）, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22149v1",
      "title": "DynaWeb: Model-Based 通过试错学习最佳策略的机器学习方法（undefined） of Web Agents",
      "originalTitle": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "summary": "The development of autonomous web agents, powered by Large Language Models (LLMs) and 通过试错学习最佳策略的机器学习方法（undefined） (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based 通过试错学习最佳策略的机器学习方法（undefined） (MBRL)...",
      "plainSummary": "The development of autonomous web agents, powered by Large Language Models (LLMs) and 通过试错学习最佳策略的机器学习方法（undefined） (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based 通过试错学习最佳策略的机器学习方法（undefined） (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a 创新的、前人未做过的（undefined） MBRL 提供结构的基础代码库（undefined） that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for 速度快、资源消耗少（undefined） online 通过试错学习最佳策略的机器学习方法（undefined）. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of 当前最好的、领先的方法（undefined） open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a 能够处理更大规模数据（undefined） and 速度快、资源消耗少（undefined） way to scale up online agentic RL.",
      "oneSentenceSummary": "【cs.CL】Hang Ding等DynaWeb，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Hang Ding",
          "chinese": null
        },
        {
          "original": "Peidong Liu",
          "chinese": null
        },
        {
          "original": "Junqiao Wang",
          "chinese": null
        },
        {
          "original": "Ziwei Ji",
          "chinese": null
        },
        {
          "original": "Meng Cao",
          "chinese": null
        },
        {
          "original": "Rongzhao Zhang",
          "chinese": null
        },
        {
          "original": "Lynn Ai",
          "chinese": null
        },
        {
          "original": "Eric Yang",
          "chinese": null
        },
        {
          "original": "Tianyu Shi",
          "chinese": null
        },
        {
          "original": "Lei Yu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:59:07Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22149v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22149v1",
      "keyInfo": {
        "contributions": [
          "The development of autonomous web agents, powered by Large Language Models (LLMs) and 通过试错学习最佳策略的机器学习方法（undefined） (RL), represents a significant step towards general-purpose AI assistants",
          "This paper introduces DynaWeb, a 创新的、前人未做过的（undefined） MBRL 提供结构的基础代码库（undefined） that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22146v1",
      "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
      "originalTitle": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
      "summary": "Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome...",
      "plainSummary": "Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, 我们提出 a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .",
      "oneSentenceSummary": "【cs.CL】Ajay Patel等FineInstructions，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Ajay Patel",
          "chinese": null
        },
        {
          "original": "Colin Raffel",
          "chinese": null
        },
        {
          "original": "Chris Callison-Burch",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:58:47Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22146v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22146v1",
      "keyInfo": {
        "contributions": [
          "To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs",
          "The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22143v1",
      "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
      "originalTitle": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
      "summary": "Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle i...",
      "plainSummary": "Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the 能够创建新数据的AI模型（undefined） itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative 观察到数据前的概率（undefined） of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining 对噪声和扰动不敏感（undefined） to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.",
      "oneSentenceSummary": "【cs.GR】Anthony Chen等JUST-DUB-IT，在cs.GR取得新进展。",
      "authors": [
        {
          "original": "Anthony Chen",
          "chinese": null
        },
        {
          "original": "Naomi Ken Korem",
          "chinese": null
        },
        {
          "original": "Tavi Halperin",
          "chinese": null
        },
        {
          "original": "Matan Ben Yosef",
          "chinese": null
        },
        {
          "original": "Urska Jelercic",
          "chinese": null
        },
        {
          "original": "Ofir Bibi",
          "chinese": null
        },
        {
          "original": "Or Patashnik",
          "chinese": null
        },
        {
          "original": "Daniel Cohen-Or",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:57:13Z",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primaryCategory": "cs.GR",
      "pdfUrl": "https://arxiv.org/pdf/2601.22143v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22143v1",
      "keyInfo": {
        "contributions": [
          "In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA"
        ],
        "methods": [],
        "applications": [
          "The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22141v1",
      "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
      "originalTitle": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
      "summary": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lo...",
      "plainSummary": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, 我们提出 Routing the Lottery (RTL), an adaptive pruning 提供结构的基础代码库（undefined） that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced 正确预测占总预测的比例（undefined） and 真正正例中被正确预测的比例（undefined）, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware 使用多层神经网络来处理复杂模式的技术（undefined）.",
      "oneSentenceSummary": "【cs.AI】Grzegorz Stefanski等Routing the Lottery，使用Across diverse datasets and ta...，在cs.AI取得新进展。",
      "authors": [
        {
          "original": "Grzegorz Stefanski",
          "chinese": null
        },
        {
          "original": "Alberto Presta",
          "chinese": null
        },
        {
          "original": "Michal Byra",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:56:41Z",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2601.22141v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22141v1",
      "keyInfo": {
        "contributions": [
          "In this work, we propose Routing the Lottery (RTL), an adaptive pruning 提供结构的基础代码库（undefined） that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition",
          "Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification"
        ],
        "methods": [
          "Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced 正确预测占总预测的比例（undefined） and 真正正例中被正确预测的比例（undefined）, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned"
        ],
        "applications": [
          "Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22139v1",
      "title": "Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers",
      "originalTitle": "Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers",
      "summary": "Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from pa...",
      "plainSummary": "Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. 我们提出 Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised 在预训练模型基础上进行小幅调整（undefined） procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy 寻找最佳参数或解决方案的过程（undefined） 提供结构的基础代码库（undefined） driven by a composite reward that aligns model behavior with user intent. 大量实验 on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\\% higher 正确预测占总预测的比例（undefined）, 22.90\\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, 理解问题并给出答案的AI系统（undefined）, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}",
      "oneSentenceSummary": "【cs.CL】Xin Chen等Reasoning While Asking，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Xin Chen",
          "chinese": null
        },
        {
          "original": "Feng Jiang",
          "chinese": null
        },
        {
          "original": "Yiqian Zhang",
          "chinese": null
        },
        {
          "original": "Hardy Chen",
          "chinese": null
        },
        {
          "original": "Shuo Yan",
          "chinese": null
        },
        {
          "original": "Wenya Xie",
          "chinese": null
        },
        {
          "original": "Min Yang",
          "chinese": null
        },
        {
          "original": "Shujian Huang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:56:12Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22139v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22139v1",
      "keyInfo": {
        "contributions": [
          "We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22137v1",
      "title": "PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） Training",
      "originalTitle": "PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training",
      "summary": "Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Pol...",
      "plainSummary": "Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general 提供结构的基础代码库（undefined） for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in 让计算机通过数据自动学习和改进的技术（undefined）. Unlike 观察到数据前的概率（undefined） methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.",
      "oneSentenceSummary": "【cs.LG】Shenghao Yang等PRISM，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Shenghao Yang",
          "chinese": null
        },
        {
          "original": "Zhichao Wang",
          "chinese": null
        },
        {
          "original": "Oleg Balabanov",
          "chinese": null
        },
        {
          "original": "N. Benjamin Erichson",
          "chinese": null
        },
        {
          "original": "Michael W. Mahoney",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:55:46Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA",
        "math.OC"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22137v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22137v1",
      "keyInfo": {
        "contributions": [
          "This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators",
          "We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general 提供结构的基础代码库（undefined） for accelerating iterative algorithms for computing matrix functions"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22136v1",
      "title": "StepShield: When, Not Whether to Intervene on Rogue Agents",
      "originalTitle": "StepShield: When, Not Whether to Intervene on Rogue Agents",
      "summary": "Existing agent safety benchmarks report binary 正确预测占总预测的比例（undefined）, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first 用于比较性能的标准数据集或方法（undefined） to evaluate...",
      "plainSummary": "Existing agent safety benchmarks report binary 正确预测占总预测的比例（undefined）, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first 用于比较性能的标准数据集或方法（undefined） to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. 我们提出 three 创新的、前人未做过的（undefined） temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard 正确预测占总预测的比例（undefined） metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.",
      "oneSentenceSummary": "【cs.LG】Gloria Felicia等StepShield，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Gloria Felicia",
          "chinese": null
        },
        {
          "original": "Michael Eniolade",
          "chinese": null
        },
        {
          "original": "Jinfeng He",
          "chinese": null
        },
        {
          "original": "Zitha Sasindran",
          "chinese": null
        },
        {
          "original": "Hemant Kumar",
          "chinese": null
        },
        {
          "original": "Milan Hussain Angati",
          "chinese": null
        },
        {
          "original": "Sandeep Bandarupalli",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:55:46Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22136v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22136v1",
      "keyInfo": {
        "contributions": [
          "We introduce StepShield, the first 用于比较性能的标准数据集或方法（undefined） to evaluate when violations are detected, not just whether",
          "We propose three 创新的、前人未做过的（undefined） temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved"
        ],
        "methods": [],
        "applications": [
          "A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value",
          "We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22135v1",
      "title": "PI-Light: Physics-Inspired Diffusion for Full-Image Relighting",
      "originalTitle": "PI-Light: Physics-Inspired Diffusion for Full-Image Relighting",
      "summary": "Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired di...",
      "plainSummary": "Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight (-Light, or PI-Light), a two-stage 提供结构的基础代码库（undefined） that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable 速度快、资源消耗少（undefined） finetuning of pretrained diffusion models while also providing a solid 用于比较性能的标准数据集或方法（undefined） for downstream evaluation. Experiments demonstrate that -Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with 观察到数据前的概率（undefined） approaches.",
      "oneSentenceSummary": "【cs.CV】Zhexin Liang等PI-Light，使用To tackle these challenges, we...，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Zhexin Liang",
          "chinese": null
        },
        {
          "original": "Zhaoxi Chen",
          "chinese": null
        },
        {
          "original": "Yongwei Chen",
          "chinese": null
        },
        {
          "original": "Tianyi Wei",
          "chinese": null
        },
        {
          "original": "Tengfei Wang",
          "chinese": null
        },
        {
          "original": "Xingang Pan",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:55:36Z",
      "categories": [
        "cs.CV"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22135v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22135v1",
      "keyInfo": {
        "contributions": [
          "To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($π$-Light, or PI-Light), a two-stage 提供结构的基础代码库（undefined） that leverages physics-inspired diffusion models",
          "Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions"
        ],
        "methods": [
          "To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($π$-Light, or PI-Light), a two-stage 提供结构的基础代码库（undefined） that leverages physics-inspired diffusion models"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22134v1",
      "title": "Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography",
      "originalTitle": "Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography",
      "summary": "Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system nam...",
      "plainSummary": "Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with 让机器模拟人类智能的技术（undefined）). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P < 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.",
      "oneSentenceSummary": "【cs.CV】Wenxuan Li等Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Wenxuan Li",
          "chinese": null
        },
        {
          "original": "Pedro R. A. S. Bassi",
          "chinese": null
        },
        {
          "original": "Lizhou Wu",
          "chinese": null
        },
        {
          "original": "Xinze Zhou",
          "chinese": null
        },
        {
          "original": "Yuxuan Zhao",
          "chinese": null
        },
        {
          "original": "Qi Chen",
          "chinese": null
        },
        {
          "original": "Szymon Plotka",
          "chinese": null
        },
        {
          "original": "Tianyu Lin",
          "chinese": null
        },
        {
          "original": "Zheren Zhu",
          "chinese": null
        },
        {
          "original": "Marisa Martin",
          "chinese": null
        },
        {
          "original": "Justin Caskey",
          "chinese": null
        },
        {
          "original": "Shanshan Jiang",
          "chinese": null
        },
        {
          "original": "Xiaoxi Chen",
          "chinese": null
        },
        {
          "original": "Jaroslaw B. Ćwikla",
          "chinese": null
        },
        {
          "original": "Artur Sankowski",
          "chinese": null
        },
        {
          "original": "Yaping Wu",
          "chinese": null
        },
        {
          "original": "Sergio Decherchi",
          "chinese": null
        },
        {
          "original": "Andrea Cavalli",
          "chinese": null
        },
        {
          "original": "Chandana Lall",
          "chinese": null
        },
        {
          "original": "Cristian Tomasetti",
          "chinese": null
        },
        {
          "original": "Yaxing Guo",
          "chinese": null
        },
        {
          "original": "Xuan Yu",
          "chinese": null
        },
        {
          "original": "Yuqing Cai",
          "chinese": null
        },
        {
          "original": "Hualin Qiao",
          "chinese": null
        },
        {
          "original": "Jie Bao",
          "chinese": null
        },
        {
          "original": "Chenhan Hu",
          "chinese": null
        },
        {
          "original": "Ximing Wang",
          "chinese": null
        },
        {
          "original": "Arkadiusz Sitek",
          "chinese": null
        },
        {
          "original": "Kai Ding",
          "chinese": null
        },
        {
          "original": "Heng Li",
          "chinese": null
        },
        {
          "original": "Meiyun Wang",
          "chinese": null
        },
        {
          "original": "Dexin Yu",
          "chinese": null
        },
        {
          "original": "Guang Zhang",
          "chinese": null
        },
        {
          "original": "Yang Yang",
          "chinese": null
        },
        {
          "original": "Kang Wang",
          "chinese": null
        },
        {
          "original": "Alan L. Yuille",
          "chinese": null
        },
        {
          "original": "Zongwei Zhou",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:55:23Z",
      "categories": [
        "cs.CV"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22134v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22134v1",
      "keyInfo": {
        "contributions": [
          "Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked",
          "To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with 让机器模拟人类智能的技术（undefined）)"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22132v1",
      "title": "Pay for Hints, Not Answers: LLM Shepherding for Cost-速度快、资源消耗少（undefined） Inference",
      "originalTitle": "Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference",
      "summary": "Large Language Models (LLMs) deliver 当前最好的、领先的方法（undefined） performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in 正确预测占总预测的比例（undefined）. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the...",
      "plainSummary": "Large Language Models (LLMs) deliver 当前最好的、领先的方法（undefined） performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in 正确预测占总预测的比例（undefined）. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a 提供结构的基础代码库（undefined） that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM 正确预测占总预测的比例（undefined） significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to 当前最好的、领先的方法（undefined） routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching 正确预测占总预测的比例（undefined）. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.",
      "oneSentenceSummary": "【cs.LG】Ziming Dong等Pay for Hints, Not Answers，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Ziming Dong",
          "chinese": null
        },
        {
          "original": "Hardik Sharma",
          "chinese": null
        },
        {
          "original": "Evan O'Toole",
          "chinese": null
        },
        {
          "original": "Jaya Prakash Champati",
          "chinese": null
        },
        {
          "original": "Kui Wu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:52:54Z",
      "categories": [
        "cs.LG"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22132v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22132v1",
      "keyInfo": {
        "contributions": [
          "We introduce LLM Shepherding, a 提供结构的基础代码库（undefined） that requests only a short prefix (a hint) from the LLM and provides it to SLM",
          "We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22131v1",
      "title": "SMOG: 能够处理更大规模数据（undefined） Meta-Learning for Multi-Objective Bayesian 寻找最佳参数或解决方案的过程（undefined）",
      "originalTitle": "SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization",
      "summary": "Multi-objective 寻找最佳参数或解决方案的过程（undefined） aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related 寻找最佳参数或解决方案的过程（undefined） tasks is available, creating an opportunity for meta-learning to accelerate the 寻找最佳参数或解决方案的过程（undefined）. Bayesian 寻找最佳参数或解决方案的过程（undefined）, as a...",
      "plainSummary": "Multi-objective 寻找最佳参数或解决方案的过程（undefined） aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related 寻找最佳参数或解决方案的过程（undefined） tasks is available, creating an opportunity for meta-learning to accelerate the 寻找最佳参数或解决方案的过程（undefined）. Bayesian 寻找最佳参数或解决方案的过程（undefined）, as a promising technique for black-box 寻找最佳参数或解决方案的过程（undefined）, has been extended to meta-learning and multi-objective 寻找最佳参数或解决方案的过程（undefined） independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian 寻找最佳参数或解决方案的过程（undefined） - remain largely unexplored. 我们提出 SMOG, a 能够处理更大规模数据（undefined） and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process 观察到数据前的概率（undefined） across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task 观察到数据前的概率（undefined） augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian 寻找最佳参数或解决方案的过程（undefined） acquisition functions.",
      "oneSentenceSummary": "【cs.LG】Leonard Papenmeier等SMOG，使用We propose SMOG, a 能够处理更大规模数据（...，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Leonard Papenmeier",
          "chinese": null
        },
        {
          "original": "Petru Tighineanu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:51:58Z",
      "categories": [
        "cs.LG"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22131v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22131v1",
      "keyInfo": {
        "contributions": [
          "We propose SMOG, a 能够处理更大规模数据（undefined） and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives"
        ],
        "methods": [
          "We propose SMOG, a 能够处理更大规模数据（undefined） and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22130v1",
      "title": "World of Workflows: a 用于比较性能的标准数据集或方法（undefined） for Bringing World Models to Enterprise Systems",
      "originalTitle": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems",
      "summary": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observa...",
      "plainSummary": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a 用于比较性能的标准数据集或方法（undefined） of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.",
      "oneSentenceSummary": "【cs.AI】Lakshya Gupta等World of Workflows，在cs.AI取得新进展。",
      "authors": [
        {
          "original": "Lakshya Gupta",
          "chinese": null
        },
        {
          "original": "Litao Li",
          "chinese": null
        },
        {
          "original": "Yizhe Liu",
          "chinese": null
        },
        {
          "original": "Sriram Ganapathi Subramanian",
          "chinese": null
        },
        {
          "original": "Kaheer Suleman",
          "chinese": null
        },
        {
          "original": "Zichen Zhang",
          "chinese": null
        },
        {
          "original": "Haoye Lu",
          "chinese": null
        },
        {
          "original": "Sumit Pasupalak",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:51:54Z",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2601.22130v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22130v1",
      "keyInfo": {
        "contributions": [
          "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases",
          "We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a 用于比较性能的标准数据集或方法（undefined） of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22129v1",
      "title": "SWE-Replay: 速度快、资源消耗少（undefined） Test-Time Scaling for Software Engineering Agents",
      "originalTitle": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents",
      "summary": "Test-time scaling has been widely adopted to enhance the capabilities of 基于海量文本训练的语言模型，如GPT（undefined） (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail ...",
      "plainSummary": "Test-time scaling has been widely adopted to enhance the capabilities of 基于海量文本训练的语言模型，如GPT（undefined） (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first 速度快、资源消耗少（undefined） and 能够适用于新场景（undefined） test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from 观察到数据前的概率（undefined） trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a 对噪声和扰动不敏感（undefined） foundation for 速度快、资源消耗少（undefined） test-time scaling of software engineering agents.",
      "oneSentenceSummary": "【cs.SE】Yifeng Ding等SWE-Replay，使用While recent methods have atte...，在cs.SE取得新进展。",
      "authors": [
        {
          "original": "Yifeng Ding",
          "chinese": null
        },
        {
          "original": "Lingming Zhang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:50:29Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "cs.SE",
      "pdfUrl": "https://arxiv.org/pdf/2601.22129v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22129v1",
      "keyInfo": {
        "contributions": [
          "In this paper, we introduce SWE-Replay, the first 速度快、资源消耗少（undefined） and 能够适用于新场景（undefined） test-time scaling technique for modern agents without reliance on potentially noisy value estimates"
        ],
        "methods": [
          "While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22128v1",
      "title": "The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR",
      "originalTitle": "The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR",
      "summary": "Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical sys...",
      "plainSummary": "Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-将离散数据转换为连续向量表示（undefined） prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in 数据的压缩表示空间（undefined） from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.",
      "oneSentenceSummary": "【cs.AI】Irsyad Adam等The Patient is not a Moving Document，使用Using a linear probe evaluated...，在cs.AI取得新进展。",
      "authors": [
        {
          "original": "Irsyad Adam",
          "chinese": null
        },
        {
          "original": "Zekai Chen",
          "chinese": null
        },
        {
          "original": "David Laprade",
          "chinese": null
        },
        {
          "original": "Shaun Porwal",
          "chinese": null
        },
        {
          "original": "David Laub",
          "chinese": null
        },
        {
          "original": "Erik Reinertsen",
          "chinese": null
        },
        {
          "original": "Arda Pekis",
          "chinese": null
        },
        {
          "original": "Kevin Brown",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:49:37Z",
      "categories": [
        "cs.AI",
        "cs.CE",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2601.22128v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22128v1",
      "keyInfo": {
        "contributions": [
          "Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale",
          "To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-将离散数据转换为连续向量表示（undefined） prediction architecture (JEPA) with next-token prediction (SFT)"
        ],
        "methods": [
          "Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22127v1",
      "title": "EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers",
      "originalTitle": "EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers",
      "summary": "Current generative video models excel at producing 创新的、前人未做过的（undefined） content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based 提供结构的基础代码库（undefined） for audio-driven vi...",
      "plainSummary": "Current generative video models excel at producing 创新的、前人未做过的（undefined） content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based 提供结构的基础代码库（undefined） for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.",
      "oneSentenceSummary": "【cs.CV】John Flynn等EditYourself，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "John Flynn",
          "chinese": null
        },
        {
          "original": "Wolfgang Paier",
          "chinese": null
        },
        {
          "original": "Dimitar Dinev",
          "chinese": null
        },
        {
          "original": "Sam Nhut Nguyen",
          "chinese": null
        },
        {
          "original": "Hayk Poghosyan",
          "chinese": null
        },
        {
          "original": "Manuel Toribio",
          "chinese": null
        },
        {
          "original": "Sandipan Banerjee",
          "chinese": null
        },
        {
          "original": "Guy Gafni",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:49:27Z",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG",
        "cs.MM"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22127v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22127v1",
      "keyInfo": {
        "contributions": [
          "We introduce EditYourself, a DiT-based 提供结构的基础代码库（undefined） for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content",
          "This work represents a foundational step toward generative video models as practical tools for professional video post-production"
        ],
        "methods": [],
        "applications": [
          "We introduce EditYourself, a DiT-based 提供结构的基础代码库（undefined） for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content",
          "This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22125v1",
      "title": "Creative Image Generation with Diffusion Model",
      "originalTitle": "Creative Image Generation with Diffusion Model",
      "summary": "Creative image generation has emerged as a compelling area of research, driven by the need to produce 创新的、前人未做过的（undefined） and high-quality images that expand the boundaries of imagination. In this work, we propose a 创新的、前人未做过的（undefined） 提供结构的基础代码库（undefined） for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the...",
      "plainSummary": "Creative image generation has emerged as a compelling area of research, driven by the need to produce 创新的、前人未做过的（undefined） and high-quality images that expand the boundaries of imagination. In this work, 我们提出 a 创新的、前人未做过的（undefined） 提供结构的基础代码库（undefined） for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP 将离散数据转换为连续向量表示（undefined） space. Unlike 观察到数据前的概率（undefined） approaches that rely on a manual blending of concepts or exclusion of subcategories, 我们的方法 calculates the 描述随机变量取值的概率（undefined） of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. 大量实验 on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation 提供结构的基础代码库（undefined）, showcasing its ability to produce unique, 创新的、前人未做过的（undefined）, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.",
      "oneSentenceSummary": "【cs.CV】Kunpeng Song等Creative Image Generation with Diffusion Model，使用In this work, we propose a 创新的...，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Kunpeng Song",
          "chinese": null
        },
        {
          "original": "Ahmed Elgammal",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:48:48Z",
      "categories": [
        "cs.CV"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22125v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22125v1",
      "keyInfo": {
        "contributions": [
          "In this work, we propose a 创新的、前人未做过的（undefined） 提供结构的基础代码库（undefined） for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP 将离散数据转换为连续向量表示（undefined） space",
          "We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity"
        ],
        "methods": [
          "In this work, we propose a 创新的、前人未做过的（undefined） 提供结构的基础代码库（undefined） for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP 将离散数据转换为连续向量表示（undefined） space"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22124v1",
      "title": "A Federated and Parameter-速度快、资源消耗少（undefined） 提供结构的基础代码库（undefined） for 基于海量文本训练的语言模型，如GPT（undefined） Training in Medicine",
      "originalTitle": "A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine",
      "summary": "Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including 理解问题并给出答案的AI系统（undefined） and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizabili...",
      "plainSummary": "Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including 理解问题并给出答案的AI系统（undefined） and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-速度快、资源消耗少（undefined） federated learning 提供结构的基础代码库（undefined） for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the 提供结构的基础代码库（undefined） to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. 正确预测占总预测的比例（undefined） was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.",
      "oneSentenceSummary": "【cs.CL】Anran Li等A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine，使用To enable their use in clinica...，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Anran Li",
          "chinese": null
        },
        {
          "original": "Yuanyuan Chen",
          "chinese": null
        },
        {
          "original": "Wenjun Long",
          "chinese": null
        },
        {
          "original": "Yu Yin",
          "chinese": null
        },
        {
          "original": "Yan Hu",
          "chinese": null
        },
        {
          "original": "Hyunjae Kim",
          "chinese": null
        },
        {
          "original": "Weipeng Zhou",
          "chinese": null
        },
        {
          "original": "Yujia Zhou",
          "chinese": null
        },
        {
          "original": "Hongyi Peng",
          "chinese": null
        },
        {
          "original": "Yang Ren",
          "chinese": null
        },
        {
          "original": "Xuguang Ai",
          "chinese": null
        },
        {
          "original": "Zhenyue Qin",
          "chinese": null
        },
        {
          "original": "Ming Hu",
          "chinese": null
        },
        {
          "original": "Xiaoxiao Li",
          "chinese": null
        },
        {
          "original": "Han Yu",
          "chinese": null
        },
        {
          "original": "Yih-Chung Tham",
          "chinese": null
        },
        {
          "original": "Lucila Ohno-Machado",
          "chinese": null
        },
        {
          "original": "Hua Xu",
          "chinese": null
        },
        {
          "original": "Qingyu Chen",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:48:21Z",
      "categories": [
        "cs.CL",
        "cs.DC"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22124v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22124v1",
      "keyInfo": {
        "contributions": [
          "Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions",
          "We introduce the model-agnostic and parameter-速度快、资源消耗少（undefined） federated learning 提供结构的基础代码库（undefined） for adapting LLMs to medical applications"
        ],
        "methods": [
          "To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data",
          "Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22123v1",
      "title": "Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics",
      "originalTitle": "Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics",
      "summary": "Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a 提供结构的基础代码库（undefined） to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span , enabling stable large-timestep updates far beyond the stability limits of classical integrators. ...",
      "plainSummary": "Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a 提供结构的基础代码库（undefined） to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span , enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike 观察到数据前的概率（undefined） approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, 我们的方法 in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.",
      "oneSentenceSummary": "【cs.LG】Winfried Ripken等Learning Hamiltonian Flow Maps，使用Validated across diverse Hamil...，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Winfried Ripken",
          "chinese": null
        },
        {
          "original": "Michael Plainer",
          "chinese": null
        },
        {
          "original": "Gregor Lied",
          "chinese": null
        },
        {
          "original": "Thorben Frank",
          "chinese": null
        },
        {
          "original": "Oliver T. Unke",
          "chinese": null
        },
        {
          "original": "Stefan Chmiela",
          "chinese": null
        },
        {
          "original": "Frank Noé",
          "chinese": null
        },
        {
          "original": "Klaus Robert Müller",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:47:46Z",
      "categories": [
        "cs.LG"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22123v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22123v1",
      "keyInfo": {
        "contributions": [
          "To overcome this constraint, we introduce a 提供结构的基础代码库（undefined） to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators"
        ],
        "methods": [
          "Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF)"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22119v1",
      "title": "Alpha Discovery via Grammar-Guided Learning and Search",
      "originalTitle": "Alpha Discovery via Grammar-Guided Learning and Search",
      "summary": "Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based 提供结构的基础代码库（undefined） for defining and discovering alpha factors that are syntactically valid, financially 能够解释其决策过程（undefined）, and...",
      "plainSummary": "Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based 提供结构的基础代码库（undefined） for defining and discovering alpha factors that are syntactically valid, financially 能够解释其决策过程（undefined）, and computationally 速度快、资源消耗少（undefined）. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms 当前最好的、领先的方法（undefined） baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general 提供结构的基础代码库（undefined） for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.",
      "oneSentenceSummary": "【q-fin.CP】Han Yang等Alpha Discovery via Grammar-Guided Learning and Search，使用AlphaCFG uses an alpha-oriente...，在q-fin.CP取得新进展。",
      "authors": [
        {
          "original": "Han Yang",
          "chinese": null
        },
        {
          "original": "Dong Hao",
          "chinese": null
        },
        {
          "original": "Zhuohan Wang",
          "chinese": null
        },
        {
          "original": "Qi Shi",
          "chinese": null
        },
        {
          "original": "Xingtong Li",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:46:15Z",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "q-fin.CP",
      "pdfUrl": "https://arxiv.org/pdf/2601.22119v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22119v1",
      "keyInfo": {
        "contributions": [
          "We present AlphaCFG, a grammar-based 提供结构的基础代码库（undefined） for defining and discovering alpha factors that are syntactically valid, financially 能够解释其决策过程（undefined）, and computationally 速度快、资源消耗少（undefined）"
        ],
        "methods": [
          "AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22118v1",
      "title": "Defining Operational Conditions for Safety-Critical AI-Based Systems from Data",
      "originalTitle": "Defining Operational Conditions for Safety-Critical AI-Based Systems from Data",
      "summary": "让机器模拟人类智能的技术（undefined） (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, thi...",
      "plainSummary": "让机器模拟人类智能的技术（undefined） (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a 创新的、前人未做过的（undefined） Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the 创新的、前人未做过的（undefined）, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.",
      "oneSentenceSummary": "【cs.AI】Johann Christensen等Defining Operational Conditions for Safety-Critical AI-Based Systems from Data，使用This paper presents a 创新的、前人未做...，在cs.AI取得新进展。",
      "authors": [
        {
          "original": "Johann Christensen",
          "chinese": null
        },
        {
          "original": "Elena Hoemann",
          "chinese": null
        },
        {
          "original": "Frank Köster",
          "chinese": null
        },
        {
          "original": "Sven Hallerbach",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:46:02Z",
      "categories": [
        "cs.AI"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2601.22118v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22118v1",
      "keyInfo": {
        "contributions": [
          "Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems",
          "Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards"
        ],
        "methods": [
          "This paper presents a 创新的、前人未做过的（undefined） Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation"
        ],
        "applications": [
          "Utilizing the 创新的、前人未做过的（undefined）, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22114v1",
      "title": "SINA: A Circuit Schematic Image-to-Netlist Generator Using 让机器模拟人类智能的技术（undefined）",
      "originalTitle": "SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence",
      "summary": "Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates 使用多层神经网络来处理复杂模式的技术（undefined） for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity...",
      "plainSummary": "Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates 使用多层神经网络来处理复杂模式的技术（undefined） for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-学习语言模式和关系的AI模型（undefined） (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation 正确预测占总预测的比例（undefined）, which is 2.72x higher than 当前最好的、领先的方法（undefined） approaches.",
      "oneSentenceSummary": "【cs.CV】Saoud Aldowaish等SINA，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Saoud Aldowaish",
          "chinese": null
        },
        {
          "original": "Yashwanth Karumanchi",
          "chinese": null
        },
        {
          "original": "Kai-Chen Chiang",
          "chinese": null
        },
        {
          "original": "Soroosh Noorzad",
          "chinese": null
        },
        {
          "original": "Morteza Fayazi",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:41:52Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.SY"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22114v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22114v1",
      "keyInfo": {
        "contributions": [
          "In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator",
          "SINA integrates 使用多层神经网络来处理复杂模式的技术（undefined） for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-学习语言模式和关系的AI模型（undefined） (VLM) for reliable reference designator assignments"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22113v1",
      "title": "Diverse Approaches to Optimal Execution Schedule Generation",
      "originalTitle": "Diverse Approaches to Optimal Execution Schedule Generation",
      "summary": "We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution. Rather than searching for a single optimal policy, MAP-Elites generates a diverse portfolio of regime-specialist strategies indexed by liquidity and volatility conditions. Individual specialists achieve 8-10% performance improvements within their behavioural niches, while other cells show degradation...",
      "plainSummary": "We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution. Rather than searching for a single optimal policy, MAP-Elites generates a diverse portfolio of regime-specialist strategies indexed by liquidity and volatility conditions. Individual specialists achieve 8-10% performance improvements within their behavioural niches, while other cells show degradation, suggesting opportunities for ensemble approaches that combine improved specialists with the 用于对比的基准方法（undefined） PPO policy. Results indicate that quality-diversity methods offer promise for regime-adaptive execution, though substantial computational resources per behavioural cell may be required for 对噪声和扰动不敏感（undefined） specialist development across all market conditions. To ensure experimental integrity, we develop a calibrated Gymnasium environment focused on order scheduling rather than tactical placement decisions. The simulator features a transient impact model with exponential decay and square-root volume scaling, fit to 400+ U.S. equities with R^2>0.02 out-of-sample. Within this environment, two Proximal Policy 寻找最佳参数或解决方案的过程（undefined） architectures - both MLP and CNN feature extractors - demonstrate substantial improvements over industry baselines, with the CNN variant achieving 2.13 bps arrival slippage versus 5.23 bps for VWAP on 4,900 out-of-sample orders ($21B notional). These results validate both the simulation realism and provide strong single-policy baselines for quality-diversity methods.",
      "oneSentenceSummary": "【q-fin.TR】Robert de Witt等Diverse Approaches to Optimal Execution Schedule Generation，在q-fin.TR取得新进展。",
      "authors": [
        {
          "original": "Robert de Witt",
          "chinese": null
        },
        {
          "original": "Mikko S. Pakkanen",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:41:52Z",
      "categories": [
        "q-fin.TR",
        "cs.LG"
      ],
      "primaryCategory": "q-fin.TR",
      "pdfUrl": "https://arxiv.org/pdf/2601.22113v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22113v1",
      "keyInfo": {
        "contributions": [
          "We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution",
          "Results indicate that quality-diversity methods offer promise for regime-adaptive execution, though substantial computational resources per behavioural cell may be required for 对噪声和扰动不敏感（undefined） specialist development across all market conditions"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22111v1",
      "title": "Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment",
      "originalTitle": "Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment",
      "summary": "Accurate reconstruction of atmospheric wind fields is essential for applications such as weather forecasting, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems (UAS) provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectori...",
      "plainSummary": "Accurate reconstruction of atmospheric wind fields is essential for applications such as weather forecasting, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems (UAS) provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectories, limiting full wind-field recovery. This study presents a 提供结构的基础代码库（undefined） for reconstructing four-dimensional atmospheric wind fields using measurements obtained from a coordinated UAS swarm. A synthetic turbulence environment and high-fidelity multirotor simulation are used to generate training and evaluation data. Local wind components are estimated from UAS dynamics using a bidirectional long short-term memory network (Bi-LSTM) and assimilated into a physics-informed 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） (PINN) to reconstruct a continuous wind field in space and time. For local wind estimation, the bidirectional LSTM achieves root-mean-square errors (RMSE) of 0.064 and 0.062 m/s for the north and east components in low-wind conditions, increasing to 0.122 to 0.129 m/s under moderate winds and 0.271 to 0.273 m/s in high-wind conditions, while the vertical component exhibits higher error, with RMSE values of 0.029 to 0.091 m/s. The physics-informed reconstruction recovers the dominant spatial and temporal structure of the wind field up to 1000 m altitude while preserving mean flow direction and vertical shear. Under moderate wind conditions, the reconstructed mean wind field achieves an overall RMSE between 0.118 and 0.154 m/s across evaluated UAS configurations, with the lowest error obtained using a five-UAS swarm. These results demonstrate that coordinated UAS measurements enable accurate and 能够处理更大规模数据（undefined） four-dimensional wind-field reconstruction without dedicated wind sensors or fixed infrastructure.",
      "oneSentenceSummary": "【cs.LG】Abdullah Tasim等Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment，使用This study presents a 提供结构的基础代...，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Abdullah Tasim",
          "chinese": null
        },
        {
          "original": "Wei Sun",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:40:32Z",
      "categories": [
        "cs.LG",
        "eess.SY",
        "physics.ao-ph"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22111v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22111v1",
      "keyInfo": {
        "contributions": [
          "This study presents a 提供结构的基础代码库（undefined） for reconstructing four-dimensional atmospheric wind fields using measurements obtained from a coordinated UAS swarm"
        ],
        "methods": [
          "This study presents a 提供结构的基础代码库（undefined） for reconstructing four-dimensional atmospheric wind fields using measurements obtained from a coordinated UAS swarm",
          "Local wind components are estimated from UAS dynamics using a bidirectional long short-term memory network (Bi-LSTM) and assimilated into a physics-informed 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） (PINN) to reconstruct a continuous wind field in space and time"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22108v1",
      "title": "Value-Based Pre-Training with Downstream Feedback",
      "originalTitle": "Value-Based Pre-Training with Downstream Feedback",
      "summary": "Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lig...",
      "plainSummary": "Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-使用标注数据训练模型（undefined） (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., 将图像划分为不同区域或对象（undefined）). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the 当前最好的、领先的方法（undefined） results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear 正确预测占总预测的比例（undefined）, and we provide pilot evidence of improved token efficiency in continued pretraining.",
      "oneSentenceSummary": "【cs.LG】Shuqi Ke等Value-Based Pre-Training with Downstream Feedback，使用5B--7B language models improve...，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Shuqi Ke",
          "chinese": null
        },
        {
          "original": "Giulia Fanti",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:38:09Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22108v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22108v1",
      "keyInfo": {
        "contributions": [
          "We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step",
          "The V-Pretraining task designer selects pretraining tasks (e"
        ],
        "methods": [
          "5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22107v1",
      "title": "观察到数据前的概率（undefined）-Informed Flow Matching for Graph Reconstruction",
      "originalTitle": "Prior-Informed Flow Matching for Graph Reconstruction",
      "summary": "We introduce 观察到数据前的概率（undefined）-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical 将离散数据转换为连续向量表示（undefined） methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating 将离散数据转换为连续向量表示（undefined）...",
      "plainSummary": "We introduce 观察到数据前的概率（undefined）-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical 将离散数据转换为连续向量表示（undefined） methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating 将离散数据转换为连续向量表示（undefined）-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, 我们的方法 first uses a 观察到数据前的概率（undefined）, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and 当前最好的、领先的方法（undefined） generative baselines in reconstruction 正确预测占总预测的比例（undefined）.",
      "oneSentenceSummary": "【cs.LG】Harvey Chen等Prior-Informed Flow Matching for Graph Reconstruction，使用Grounded in a permutation equi...，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Harvey Chen",
          "chinese": null
        },
        {
          "original": "Nicolas Zilberstein",
          "chinese": null
        },
        {
          "original": "Santiago Segarra",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:38:02Z",
      "categories": [
        "cs.LG"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22107v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22107v1",
      "keyInfo": {
        "contributions": [
          "We introduce 观察到数据前的概率（undefined）-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction"
        ],
        "methods": [
          "Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a 观察到数据前的概率（undefined）, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information",
          "It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22101v1",
      "title": "ECO: Quantized Training without Full-预测为正例中真正正例的比例（undefined） Master Weights",
      "originalTitle": "ECO: Quantized Training without Full-Precision Master Weights",
      "summary": "Quantization has significantly improved the compute and memory efficiency of 基于海量文本训练的语言模型，如GPT（undefined） (LLM) training. However, existing approaches still rely on accumulating their updates in high-预测为正例中真正正例的比例（undefined）: concretely, gradient updates must be applied to a high-预测为正例中真正正例的比例（undefined） weight buffer, known as . This buffer introduces substantial memory overhead, particularly fo...",
      "plainSummary": "Quantization has significantly improved the compute and memory efficiency of 基于海量文本训练的语言模型，如GPT（undefined） (LLM) training. However, existing approaches still rely on accumulating their updates in high-预测为正例中真正正例的比例（undefined）: concretely, gradient updates must be applied to a high-预测为正例中真正正例的比例（undefined） weight buffer, known as . This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show 基于实验和观察的（undefined） results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and 在预训练模型基础上进行小幅调整（undefined） DeepSeek-MoE-16B in INT4 预测为正例中真正正例的比例（undefined）. Throughout, ECO matches baselines with master weights up to near-lossless 正确预测占总预测的比例（undefined）, significantly shifting the static memory vs validation loss Pareto frontier.",
      "oneSentenceSummary": "【cs.CL】Mahdi Nikdan等ECO，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Mahdi Nikdan",
          "chinese": null
        },
        {
          "original": "Amir Zandieh",
          "chinese": null
        },
        {
          "original": "Dan Alistarh",
          "chinese": null
        },
        {
          "original": "Vahab Mirrokni",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:35:01Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22101v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22101v1",
      "keyInfo": {
        "contributions": [
          "This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage",
          "To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters"
        ],
        "methods": [],
        "applications": [
          "However, existing approaches still rely on accumulating their updates in high-预测为正例中真正正例的比例（undefined）: concretely, gradient updates must be applied to a high-预测为正例中真正正例的比例（undefined） weight buffer, known as $\\textit{master weights}$"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22100v1",
      "title": "Boosting CVaR Policy 寻找最佳参数或解决方案的过程（undefined） with Quantile Gradients",
      "originalTitle": "Boosting CVaR Policy Optimization with Quantile Gradients",
      "summary": "Optimizing Conditional Value-at-risk (CVaR) using 优化策略网络参数的梯度下降方法（undefined） (a.k.a CVaR-PG) faces significant challenges of sample inefficiency. This inefficiency stems from the fact that it focuses on tail-end performance and overlooks many sampled trajectories. We address this problem by augmenting CVaR with an expected quantile term. Quantile 寻找最佳参数或解决方案的过程（undefined） admits a dynamic programm...",
      "plainSummary": "Optimizing Conditional Value-at-risk (CVaR) using 优化策略网络参数的梯度下降方法（undefined） (a.k.a CVaR-PG) faces significant challenges of sample inefficiency. This inefficiency stems from the fact that it focuses on tail-end performance and overlooks many sampled trajectories. We address this problem by augmenting CVaR with an expected quantile term. Quantile 寻找最佳参数或解决方案的过程（undefined） admits a dynamic programming formulation that leverages all sampled data, thus improves sample efficiency. This does not alter the CVaR objective since CVaR corresponds to the expectation of quantile over the tail. 基于实验和观察的（undefined） results in domains with verifiable risk-averse behavior show that our algorithm within the Markovian policy class substantially improves upon CVaR-PG and consistently outperforms other existing methods.",
      "oneSentenceSummary": "【cs.LG】Yudong Luo等Boosting CVaR Policy Optimization with Quantile Gradients，使用Optimizing Conditional Value-a...，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Yudong Luo",
          "chinese": null
        },
        {
          "original": "Erick Delage",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:33:46Z",
      "categories": [
        "cs.LG"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22100v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22100v1",
      "keyInfo": {
        "contributions": [],
        "methods": [
          "Optimizing Conditional Value-at-risk (CVaR) using 优化策略网络参数的梯度下降方法（undefined） (a",
          "Quantile 寻找最佳参数或解决方案的过程（undefined） admits a dynamic programming formulation that leverages all sampled data, thus improves sample efficiency"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22095v1",
      "title": "GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic 寻找最佳参数或解决方案的过程（undefined）",
      "originalTitle": "GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization",
      "summary": "The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） architecture design. In this work, we rethink these approaches through the lens of manifold 寻找最佳参数或解决方案的过程（undefined）, interpreting the outputs of the Feed-Forward Network (FFN) and attention layers as update directions in 寻找最佳参数或解决方案的过程（undefined）. Building o...",
      "plainSummary": "The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） architecture design. In this work, we rethink these approaches through the lens of manifold 寻找最佳参数或解决方案的过程（undefined）, interpreting the outputs of the Feed-Forward Network (FFN) and attention layers as update directions in 寻找最佳参数或解决方案的过程（undefined）. Building on this perspective, we introduce GeoNorm, a 创新的、前人未做过的（undefined） method that replaces standard normalization with geodesic updates on the manifold. Furthermore, analogous to learning rate schedules, 我们提出 a layer-wise update decay for the FFN and attention components. 覆盖广泛的、详细的（undefined） experiments demonstrate that GeoNorm consistently outperforms existing normalization methods in 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） models. Crucially, GeoNorm can be seamlessly integrated into standard 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） architectures, achieving performance improvements with negligible additional computational cost.",
      "oneSentenceSummary": "【cs.LG】Chuanyang Zheng等GeoNorm，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Chuanyang Zheng",
          "chinese": null
        },
        {
          "original": "Jiankai Sun",
          "chinese": null
        },
        {
          "original": "Yihang Gao",
          "chinese": null
        },
        {
          "original": "Chi Wang",
          "chinese": null
        },
        {
          "original": "Yuehao Wang",
          "chinese": null
        },
        {
          "original": "Jing Xiong",
          "chinese": null
        },
        {
          "original": "Liliang Ren",
          "chinese": null
        },
        {
          "original": "Bo Peng",
          "chinese": null
        },
        {
          "original": "Qingmei Wang",
          "chinese": null
        },
        {
          "original": "Xiaoran Shang",
          "chinese": null
        },
        {
          "original": "Mac Schwager",
          "chinese": null
        },
        {
          "original": "Anderson Schneider",
          "chinese": null
        },
        {
          "original": "Yuriy Nevmyvaka",
          "chinese": null
        },
        {
          "original": "Xiaodong Liu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:31:31Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22095v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22095v1",
      "keyInfo": {
        "contributions": [
          "The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） architecture design",
          "Building on this perspective, we introduce GeoNorm, a 创新的、前人未做过的（undefined） method that replaces standard normalization with geodesic updates on the manifold"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22094v1",
      "title": "RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation",
      "originalTitle": "RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation",
      "summary": "In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-...",
      "plainSummary": "In this paper, 我们提出 a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-image references and cannot leverage 3D assets, constraining their practical versatility. To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references. Our spatially aligned dual-branch generation architecture and domain-decoupled generation mechanism ensure the simultaneous generation of two spatially aligned but content-disentangled outputs, RGB images and point maps, linking 2D image attributes with 3D asset attributes. Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.",
      "oneSentenceSummary": "【cs.CV】Hanzhuo Huang等RefAny3D，使用To address this gap, we presen...，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Hanzhuo Huang",
          "chinese": null
        },
        {
          "original": "Qingyang Bao",
          "chinese": null
        },
        {
          "original": "Zekai Gu",
          "chinese": null
        },
        {
          "original": "Zhongshuo Du",
          "chinese": null
        },
        {
          "original": "Cheng Lin",
          "chinese": null
        },
        {
          "original": "Yuan Liu",
          "chinese": null
        },
        {
          "original": "Sibei Yang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:30:10Z",
      "categories": [
        "cs.CV"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22094v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22094v1",
      "keyInfo": {
        "contributions": [
          "In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models",
          "To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references"
        ],
        "methods": [
          "To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22093v1",
      "title": "Investigating Associational Biases in Inter-Model Communication of Large Generative Models",
      "originalTitle": "Investigating Associational Biases in Inter-Model Communication of Large Generative Models",
      "summary": "Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges ...",
      "plainSummary": "Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one 能够创建新数据的AI模型（undefined）'s output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication 数据处理或模型训练的完整流程（undefined） that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability 数据处理或模型训练的完整流程（undefined）. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.",
      "oneSentenceSummary": "【cs.CY】Fethiye Irmak Dogan等Investigating Associational Biases in Inter-Model Communication of Large Generative Models，使用In this work, focusing on huma...，在cs.CY取得新进展。",
      "authors": [
        {
          "original": "Fethiye Irmak Dogan",
          "chinese": null
        },
        {
          "original": "Yuval Weiss",
          "chinese": null
        },
        {
          "original": "Kajal Patel",
          "chinese": null
        },
        {
          "original": "Jiaee Cheong",
          "chinese": null
        },
        {
          "original": "Hatice Gunes",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:29:55Z",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primaryCategory": "cs.CY",
      "pdfUrl": "https://arxiv.org/pdf/2601.22093v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22093v1",
      "keyInfo": {
        "contributions": [
          "Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions"
        ],
        "methods": [
          "In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication 数据处理或模型训练的完整流程（undefined） that alternates between image generation and image description",
          "Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability 数据处理或模型训练的完整流程（undefined）"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22090v1",
      "title": "ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection",
      "originalTitle": "ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection",
      "summary": "Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation 数据处理或模型训练的完整流程（undefined） that initializes an intent detector from a model pretrained on large-scale able-bodi...",
      "plainSummary": "Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. 我们提出 a healthy-to-stroke adaptation 数据处理或模型训练的完整流程（undefined） that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-速度快、资源消耗少（undefined） LoRA adapters, and full end-to-end 在预训练模型基础上进行小幅调整（undefined）) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition 正确预测占总预测的比例（undefined） from 0.42 to 0.61 and raw 正确预测占总预测的比例（undefined） from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.",
      "oneSentenceSummary": "【cs.RO】Runsheng Wang等ReactEMG Stroke，使用We propose a healthy-to-stroke...，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Runsheng Wang",
          "chinese": null
        },
        {
          "original": "Katelyn Lee",
          "chinese": null
        },
        {
          "original": "Xinyue Zhu",
          "chinese": null
        },
        {
          "original": "Lauren Winterbottom",
          "chinese": null
        },
        {
          "original": "Dawn M. Nilsen",
          "chinese": null
        },
        {
          "original": "Joel Stein",
          "chinese": null
        },
        {
          "original": "Matei Ciocarlie",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:26:51Z",
      "categories": [
        "cs.RO"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.22090v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22090v1",
      "keyInfo": {
        "contributions": [
          "We propose a healthy-to-stroke adaptation 数据处理或模型训练的完整流程（undefined） that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data",
          "These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection"
        ],
        "methods": [
          "We propose a healthy-to-stroke adaptation 数据处理或模型训练的完整流程（undefined） that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data",
          "Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-速度快、资源消耗少（undefined） LoRA adapters, and full end-to-end 在预训练模型基础上进行小幅调整（undefined）) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22086v1",
      "title": "Learning Transient Convective Heat Transfer with Geometry Aware World Models",
      "originalTitle": "Learning Transient Convective Heat Transfer with Geometry Aware World Models",
      "summary": "Partial differential equation (PDE) simulations are fundamental to engineering and physics but are often computationally prohibitive for real-time applications. While generative AI offers a promising avenue for surrogate modeling, standard video generation architectures lack the specific control and data compatibility required for physical simulations. This paper introduces a geometry aware world ...",
      "plainSummary": "Partial differential equation (PDE) simulations are fundamental to engineering and physics but are often computationally prohibitive for real-time applications. While generative AI offers a promising avenue for surrogate modeling, standard video generation architectures lack the specific control and data compatibility required for physical simulations. This paper introduces a geometry aware world model architecture, derived from a video generation architecture (LongVideoGAN), designed to learn transient physics. We introduce two key architecture elements: (1) a twofold conditioning mechanism incorporating global physical parameters and local geometric masks, and (2) an architectural adaptation to support arbitrary channel dimensions, moving beyond standard RGB constraints. We evaluate this approach on a 2D transient computational fluid dynamics (CFD) problem involving convective heat transfer from buoyancy-driven flow coupled to a heat flow in a solid structure. We demonstrate that the conditioned model successfully reproduces complex temporal dynamics and spatial correlations of the training data. Furthermore, we assess the model's generalization capabilities on unseen geometric configurations, highlighting both its potential for controlled simulation synthesis and current limitations in spatial 预测为正例中真正正例的比例（undefined） for out-of-distribution samples.",
      "oneSentenceSummary": "【physics.flu-dyn】Onur T. Doganay等Learning Transient Convective Heat Transfer with Geometry Aware World Models，在physics.flu-dyn取得新进展。",
      "authors": [
        {
          "original": "Onur T. Doganay",
          "chinese": null
        },
        {
          "original": "Alexander Klawonn",
          "chinese": null
        },
        {
          "original": "Martin Eigel",
          "chinese": null
        },
        {
          "original": "Hanno Gottschalk",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:24:24Z",
      "categories": [
        "physics.flu-dyn",
        "cs.CV"
      ],
      "primaryCategory": "physics.flu-dyn",
      "pdfUrl": "https://arxiv.org/pdf/2601.22086v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22086v1",
      "keyInfo": {
        "contributions": [
          "This paper introduces a geometry aware world model architecture, derived from a video generation architecture (LongVideoGAN), designed to learn transient physics",
          "We introduce two key architecture elements: (1) a twofold conditioning mechanism incorporating global physical parameters and local geometric masks, and (2) an architectural adaptation to support arbitrary channel dimensions, moving beyond standard RGB constraints"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22083v1",
      "title": "Latent Adversarial 防止过拟合的技术（undefined） for Offline Preference 寻找最佳参数或解决方案的过程（undefined）",
      "originalTitle": "Latent Adversarial Regularization for Offline Preference Optimization",
      "summary": "Learning from human feedback typically relies on preference 寻找最佳参数或解决方案的过程（undefined） that constrains policy updates through token-level 防止过拟合的技术（undefined）. However, preference 寻找最佳参数或解决方案的过程（undefined） for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space 防止过拟合的技术（undefi...",
      "plainSummary": "Learning from human feedback typically relies on preference 寻找最佳参数或解决方案的过程（undefined） that constrains policy updates through token-level 防止过拟合的技术（undefined）. However, preference 寻找最佳参数或解决方案的过程（undefined） for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space 防止过拟合的技术（undefined） for 学习语言模式和关系的AI模型（undefined） preference 寻找最佳参数或解决方案的过程（undefined）. We introduce GANPO, which achieves latent-space 防止过拟合的技术（undefined） by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference 寻找最佳参数或解决方案的过程（undefined） objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space 防止过拟合的技术（undefined）. Further, by comparing GANPO-induced inferential biases with those from token-level 防止过拟合的技术（undefined）, we find that GANPO provides more 对噪声和扰动不敏感（undefined） structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.",
      "oneSentenceSummary": "【cs.LG】Enyi Jiang等Latent Adversarial Regularization for Offline Preference Optimization，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Enyi Jiang",
          "chinese": null
        },
        {
          "original": "Yibo Jacky Zhang",
          "chinese": null
        },
        {
          "original": "Yinglun Xu",
          "chinese": null
        },
        {
          "original": "Andreas Haupt",
          "chinese": null
        },
        {
          "original": "Nancy Amato",
          "chinese": null
        },
        {
          "original": "Sanmi Koyejo",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:21:57Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.22083v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22083v1",
      "keyInfo": {
        "contributions": [
          "We introduce GANPO, which achieves latent-space 防止过拟合的技术（undefined） by penalizing divergence between the internal representations of a policy model and a reference model",
          "Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22075v1",
      "title": "Lens-descriptor guided evolutionary algorithm for 寻找最佳参数或解决方案的过程（undefined） of complex optical systems with glass choice",
      "originalTitle": "Lens-descriptor guided evolutionary algorithm for optimization of complex optical systems with glass choice",
      "summary": "Designing high-performance optical lenses entails exploring a high-dimensional, tightly constrained space of surface curvatures, glass choices, element thicknesses, and spacings. In practice, standard optimizers (e.g., gradient-based local search and evolutionary strategies) often converge to a single local optimum, overlooking many comparably good alternatives that matter for downstream engineeri...",
      "plainSummary": "Designing high-performance optical lenses entails exploring a high-dimensional, tightly constrained space of surface curvatures, glass choices, element thicknesses, and spacings. In practice, standard optimizers (e.g., gradient-based local search and evolutionary strategies) often converge to a single local optimum, overlooking many comparably good alternatives that matter for downstream engineering decisions. 我们提出 the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage 提供结构的基础代码库（undefined） for multimodal lens 寻找最佳参数或解决方案的过程（undefined）. LDG-EA first partitions the design space into behavior descriptors defined by curvature-sign patterns and material indices, then learns a probabilistic model over descriptors to allocate evaluations toward promising regions. Within each descriptor, LDG-EA applies the Hill-Valley Evolutionary Algorithm with covariance-matrix self-adaptation to recover multiple distinct local minima, optionally followed by gradient-based refinement. On a 24-variable (18 continuous and 6 integer), six-element Double-Gauss topology, LDG-EA generates on average around 14500 candidate minima spanning 636 unique descriptors, an order of magnitude more than a CMA-ES 用于对比的基准方法（undefined）, while keeping wall-clock time at one hour scale. Although the best LDG-EA design is slightly worse than a fine-tuned reference lens, it remains in the same performance range. Overall, the proposed LDG-EA produces a diverse set of solutions while maintaining competitive quality within practical computational budgets and wall-clock time.",
      "oneSentenceSummary": "【cs.NE】Kirill Antonov等Lens-descriptor guided evolutionary algorithm for optimization of complex optical systems with glass choice，使用Within each descriptor, LDG-EA...，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Kirill Antonov",
          "chinese": null
        },
        {
          "original": "Teus Tukker",
          "chinese": null
        },
        {
          "original": "Tiago Botari",
          "chinese": null
        },
        {
          "original": "Thomas H. W. Bäck",
          "chinese": null
        },
        {
          "original": "Anna V. Kononova",
          "chinese": null
        },
        {
          "original": "Niki van Stein",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:13:24Z",
      "categories": [
        "cs.NE"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.22075v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22075v1",
      "keyInfo": {
        "contributions": [
          "Designing high-performance optical lenses entails exploring a high-dimensional, tightly constrained space of surface curvatures, glass choices, element thicknesses, and spacings",
          "We propose the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage 提供结构的基础代码库（undefined） for multimodal lens 寻找最佳参数或解决方案的过程（undefined）"
        ],
        "methods": [
          "Within each descriptor, LDG-EA applies the Hill-Valley Evolutionary Algorithm with covariance-matrix self-adaptation to recover multiple distinct local minima, optionally followed by gradient-based refinement"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22074v1",
      "title": "mjlab: A Lightweight 提供结构的基础代码库（undefined） for GPU-Accelerated Robot Learning",
      "originalTitle": "mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning",
      "summary": "We present mjlab, a lightweight, open-source 提供结构的基础代码库（undefined） for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is ...",
      "plainSummary": "We present mjlab, a lightweight, open-source 提供结构的基础代码库（undefined） for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a 提供结构的基础代码库（undefined） installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.",
      "oneSentenceSummary": "【cs.RO】Kevin Zakka等mjlab，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Kevin Zakka",
          "chinese": null
        },
        {
          "original": "Qiayuan Liao",
          "chinese": null
        },
        {
          "original": "Brent Yi",
          "chinese": null
        },
        {
          "original": "Louis Le Lay",
          "chinese": null
        },
        {
          "original": "Koushil Sreenath",
          "chinese": null
        },
        {
          "original": "Pieter Abbeel",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:11:26Z",
      "categories": [
        "cs.RO"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.22074v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22074v1",
      "keyInfo": {
        "contributions": [
          "We present mjlab, a lightweight, open-source 提供结构的基础代码库（undefined） for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction",
          "mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22069v1",
      "title": "VTC-R1: Vision-Text Compression for 速度快、资源消耗少（undefined） Long-Context Reasoning",
      "originalTitle": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning",
      "summary": "Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing 速度快、资源消耗少（undefined） approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we ...",
      "plainSummary": "Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing 速度快、资源消耗少（undefined） approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, 我们提出 VTC-R1, a new 速度快、资源消耗少（undefined） reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as \"optical memory.\" We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. 大量实验 on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a 能够处理更大规模数据（undefined） solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.",
      "oneSentenceSummary": "【cs.CL】Yibo Wang等VTC-R1，使用\" We construct a training data...，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Yibo Wang",
          "chinese": null
        },
        {
          "original": "Yongcheng Jing",
          "chinese": null
        },
        {
          "original": "Shunyu Liu",
          "chinese": null
        },
        {
          "original": "Hao Guan",
          "chinese": null
        },
        {
          "original": "Rong-cheng Tu",
          "chinese": null
        },
        {
          "original": "Chengyu Wang",
          "chinese": null
        },
        {
          "original": "Jun Huang",
          "chinese": null
        },
        {
          "original": "Dacheng Tao",
          "chinese": null
        }
      ],
      "published": "2026-01-29T18:07:39Z",
      "categories": [
        "cs.CL"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22069v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22069v1",
      "keyInfo": {
        "contributions": [
          "Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity",
          "In this paper, we propose VTC-R1, a new 速度快、资源消耗少（undefined） reasoning paradigm that integrates vision-text compression into the reasoning process"
        ],
        "methods": [
          "\" We construct a training dataset based on OpenR1-Math-220K achieving 3"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22061v1",
      "title": "BLO-Inst: Bi-Level 寻找最佳参数或解决方案的过程（undefined） Based Alignment of YOLO and SAM for 对噪声和扰动不敏感（undefined） Instance Segmentation",
      "originalTitle": "BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation",
      "summary": "The Segment Anything Model has revolutionized 将图像划分为不同区域或对象（undefined） with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do n...",
      "plainSummary": "The Segment Anything Model has revolutionized 将图像划分为不同区域或对象（undefined） with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment 模型在训练数据上表现好但在新数据上表现差（undefined） in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a 能够适用于新场景（undefined） policy. To bridge this gap, we introduce BLO-Inst, a unified 提供结构的基础代码库（undefined） that aligns detection and segmentation objectives by bi-level 寻找最佳参数或解决方案的过程（undefined）. We formulate the alignment as a nested 寻找最佳参数或解决方案的过程（undefined） problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset (). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset (). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization 正确预测占总预测的比例（undefined）, but for downstream mask quality. 大量实验 demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.",
      "oneSentenceSummary": "【cs.CV】Li Zhang等BLO-Inst，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Li Zhang",
          "chinese": null
        },
        {
          "original": "Pengtao Xie",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:58:55Z",
      "categories": [
        "cs.CV"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22061v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22061v1",
      "keyInfo": {
        "contributions": [
          "To bridge this gap, we introduce BLO-Inst, a unified 提供结构的基础代码库（undefined） that aligns detection and segmentation objectives by bi-level 寻找最佳参数或解决方案的过程（undefined）"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22060v1",
      "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
      "originalTitle": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
      "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, 观察到数据前的概率（undefined） work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, the...",
      "plainSummary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, 观察到数据前的概率（undefined） work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, 我们提出 Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
      "oneSentenceSummary": "【cs.CV】Wenxuan Huang等Vision-DeepResearch，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Wenxuan Huang",
          "chinese": null
        },
        {
          "original": "Yu Zeng",
          "chinese": null
        },
        {
          "original": "Qiuchen Wang",
          "chinese": null
        },
        {
          "original": "Zhen Fang",
          "chinese": null
        },
        {
          "original": "Shaosheng Cao",
          "chinese": null
        },
        {
          "original": "Zheng Chu",
          "chinese": null
        },
        {
          "original": "Qingyu Yin",
          "chinese": null
        },
        {
          "original": "Shuang Chen",
          "chinese": null
        },
        {
          "original": "Zhenfei Yin",
          "chinese": null
        },
        {
          "original": "Lin Chen",
          "chinese": null
        },
        {
          "original": "Zehui Chen",
          "chinese": null
        },
        {
          "original": "Yao Hu",
          "chinese": null
        },
        {
          "original": "Philip Torr",
          "chinese": null
        },
        {
          "original": "Feng Zhao",
          "chinese": null
        },
        {
          "original": "Wanli Ouyang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:58:40Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22060v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22060v1",
      "keyInfo": {
        "contributions": [
          "However, constrained by the capacity of their internal world knowledge, 观察到数据前的概率（undefined） work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information",
          "Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22057v1",
      "title": "Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models",
      "originalTitle": "Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models",
      "summary": "Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture ...",
      "plainSummary": "Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture reusable motion components. To improve both latent factor discovery and quality of compositional generation, we introduce an adversarial training signal via a discriminator trained to distinguish between single-source samples and those generated by recombining factors across sources. By optimizing the generator to fool this discriminator, we encourage physical and semantic consistency in the resulting recombinations. 我们的方法 outperforms implementations of 观察到数据前的概率（undefined） baselines on CelebA-HQ, Virtual KITTI, CLEVR, and Falcor3D, achieving lower FID scores and better disentanglement as measured by MIG and MCC. Furthermore, we demonstrate a 创新的、前人未做过的（undefined） application to robotic video trajectories: by recombining learned action components, we generate diverse sequences that significantly increase state-space coverage for exploration on the LIBERO 用于比较性能的标准数据集或方法（undefined）.",
      "oneSentenceSummary": "【cs.CV】Archer Wang等Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Archer Wang",
          "chinese": null
        },
        {
          "original": "Emile Anand",
          "chinese": null
        },
        {
          "original": "Yilun Du",
          "chinese": null
        },
        {
          "original": "Marin Soljačić",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:57:06Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22057v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22057v1",
      "keyInfo": {
        "contributions": [
          "Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination",
          "To improve both latent factor discovery and quality of compositional generation, we introduce an adversarial training signal via a discriminator trained to distinguish between single-source samples and those generated by recombining factors across sources"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22055v1",
      "title": "$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA",
      "originalTitle": "$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA",
      "summary": "Retrieval-augmented generation is a practical paradigm for 理解问题并给出答案的AI系统（undefined） over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieva...",
      "plainSummary": "Retrieval-augmented generation is a practical paradigm for 理解问题并给出答案的AI系统（undefined） over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce -Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, -Reader with Qwen3-VL-32B-Instruct reaches 66.21\\% average 正确预测占总预测的比例（undefined）, outperforming strong baselines and a standalone GPT-5 (53.08\\%).",
      "oneSentenceSummary": "【cs.CL】Yaxin Du等$G^2$-Reader，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Yaxin Du",
          "chinese": null
        },
        {
          "original": "Junru Song",
          "chinese": null
        },
        {
          "original": "Yifan Zhou",
          "chinese": null
        },
        {
          "original": "Cheng Wang",
          "chinese": null
        },
        {
          "original": "Jiahao Gu",
          "chinese": null
        },
        {
          "original": "Zimeng Chen",
          "chinese": null
        },
        {
          "original": "Menglan Chen",
          "chinese": null
        },
        {
          "original": "Wen Yao",
          "chinese": null
        },
        {
          "original": "Yang Yang",
          "chinese": null
        },
        {
          "original": "Ying Wen",
          "chinese": null
        },
        {
          "original": "Siheng Chen",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:52:54Z",
      "categories": [
        "cs.CL"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22055v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22055v1",
      "keyInfo": {
        "contributions": [
          "We introduce $G^2$-Reader, a dual-graph system, to address both issues"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22054v1",
      "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
      "originalTitle": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
      "summary": "Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and 能够处理更大规模数据（undefined） pretraining 提供结构的基础代码库（undefined） that learns metric depth from noisy, diverse 3D s...",
      "plainSummary": "Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and 能够处理更大规模数据（undefined） pretraining 提供结构的基础代码库（undefined） that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves 当前最好的、领先的方法（undefined） results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal 基于海量文本训练的语言模型，如GPT（undefined） capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward 能够处理更大规模数据（undefined） and 速度快、资源消耗少（undefined） real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.",
      "oneSentenceSummary": "【cs.CV】Baorui Ma等MetricAnything，使用Using about 20M image-depth pa...，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Baorui Ma",
          "chinese": null
        },
        {
          "original": "Jiahui Yang",
          "chinese": null
        },
        {
          "original": "Donglin Di",
          "chinese": null
        },
        {
          "original": "Xuancheng Zhang",
          "chinese": null
        },
        {
          "original": "Jianxun Cui",
          "chinese": null
        },
        {
          "original": "Hao Li",
          "chinese": null
        },
        {
          "original": "Yan Xie",
          "chinese": null
        },
        {
          "original": "Wei Chen",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:52:41Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22054v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22054v1",
      "keyInfo": {
        "contributions": [
          "We introduce Metric Anything, a simple and 能够处理更大规模数据（undefined） pretraining 提供结构的基础代码库（undefined） that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures",
          "Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases"
        ],
        "methods": [
          "Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track",
          "We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal 基于海量文本训练的语言模型，如GPT（undefined） capabilities in spatial intelligence"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22050v1",
      "title": "MasalBench: A 用于比较性能的标准数据集或方法（undefined） for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs",
      "originalTitle": "MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs",
      "summary": "In recent years, multilingual Large Language Models (LLMs) have become an inseparable part of daily life, making it crucial for them to master the rules of conversational language in order to communicate effectively with users. While previous work has evaluated LLMs' understanding of figurative language in high-resource languages, their performance in low-resource languages remains underexplored. ...",
      "plainSummary": "In recent years, multilingual Large Language Models (LLMs) have become an inseparable part of daily life, making it crucial for them to master the rules of conversational language in order to communicate effectively with users. While previous work has evaluated LLMs' understanding of figurative language in high-resource languages, their performance in low-resource languages remains underexplored. In this paper, we introduce MasalBench, a 覆盖广泛的、详细的（undefined） 用于比较性能的标准数据集或方法（undefined） for assessing LLMs' contextual and cross-cultural understanding of Persian proverbs, which are a key component of conversation in this low-resource language. We evaluate eight 当前最好的、领先的方法（undefined） LLMs on MasalBench and find that they perform well in identifying Persian proverbs in context, achieving accuracies above 0.90. However, their performance drops considerably when tasked with identifying equivalent English proverbs, with the best model achieving 0.79 正确预测占总预测的比例（undefined）. Our findings highlight the limitations of current LLMs in cultural knowledge and analogical reasoning, and they provide a 提供结构的基础代码库（undefined） for assessing cross-cultural understanding in other low-resource languages. MasalBench is available at https://github.com/kalhorghazal/MasalBench.",
      "oneSentenceSummary": "【cs.CL】Ghazal Kalhor等MasalBench，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Ghazal Kalhor",
          "chinese": null
        },
        {
          "original": "Behnam Bahrak",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:49:44Z",
      "categories": [
        "cs.CL"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22050v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22050v1",
      "keyInfo": {
        "contributions": [
          "In this paper, we introduce MasalBench, a 覆盖广泛的、详细的（undefined） 用于比较性能的标准数据集或方法（undefined） for assessing LLMs' contextual and cross-cultural understanding of Persian proverbs, which are a key component of conversation in this low-resource language"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22047v1",
      "title": "On the Paradoxical Interference between Instruction-Following and Task Solving",
      "originalTitle": "On the Paradoxical Interference between Instruction-Following and Task Solving",
      "summary": "Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It m...",
      "plainSummary": "Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. 我们提出 a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It measures task performance drop after inserting into the instruction a self-evident constraint, which is naturally met by the original successful model output and extracted from it. Experiments on current LLMs in mathematics, multi-hop QA, and code generation show that adding the self-evident constraints leads to substantial performance drops, even for advanced models such as Claude-Sonnet-4.5. We validate the generality of the interference across constraint types and scales. Furthermore, we identify common failure patterns, and by investigating the mechanisms of interference, we observe that failed cases allocate significantly more attention to constraints compared to successful ones. Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting 基于实验和观察的（undefined） observations on current alignment strategies. We will release our code and data to facilitate further research",
      "oneSentenceSummary": "【cs.CL】Yunjia Qi等On the Paradoxical Interference between Instruction-Following and Task Solving，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Yunjia Qi",
          "chinese": null
        },
        {
          "original": "Hao Peng",
          "chinese": null
        },
        {
          "original": "Xintong Shi",
          "chinese": null
        },
        {
          "original": "Amy Xin",
          "chinese": null
        },
        {
          "original": "Xiaozhi Wang",
          "chinese": null
        },
        {
          "original": "Bin Xu",
          "chinese": null
        },
        {
          "original": "Lei Hou",
          "chinese": null
        },
        {
          "original": "Juanzi Li",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:48:56Z",
      "categories": [
        "cs.CL"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22047v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22047v1",
      "keyInfo": {
        "contributions": [
          "We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving",
          "Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting 基于实验和观察的（undefined） observations on current alignment strategies"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22046v1",
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian 提供结构的基础代码库（undefined） for Streaming 3D Reconstruction",
      "originalTitle": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "summary": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an 速度快、资源消耗少（undefined） on-the-fly reconstruction 提供结构的基础代码库（undefined） built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry a...",
      "plainSummary": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an 速度快、资源消耗少（undefined） on-the-fly reconstruction 提供结构的基础代码库（undefined） built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and 寻找最佳参数或解决方案的过程（undefined） strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene 寻找最佳参数或解决方案的过程（undefined）. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .",
      "oneSentenceSummary": "【cs.CV】Changjian Jiang等PLANING，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Changjian Jiang",
          "chinese": null
        },
        {
          "original": "Kerui Ren",
          "chinese": null
        },
        {
          "original": "Xudong Li",
          "chinese": null
        },
        {
          "original": "Kaiwen Song",
          "chinese": null
        },
        {
          "original": "Linning Xu",
          "chinese": null
        },
        {
          "original": "Tao Lu",
          "chinese": null
        },
        {
          "original": "Junting Dong",
          "chinese": null
        },
        {
          "original": "Yu Zhang",
          "chinese": null
        },
        {
          "original": "Bo Dai",
          "chinese": null
        },
        {
          "original": "Mulin Yu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:47:26Z",
      "categories": [
        "cs.CV"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22046v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22046v1",
      "keyInfo": {
        "contributions": [
          "We present PLANING, an 速度快、资源消耗少（undefined） on-the-fly reconstruction 提供结构的基础代码库（undefined） built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22045v1",
      "title": "Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion",
      "originalTitle": "Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion",
      "summary": "Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To...",
      "plainSummary": "Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR 提供结构的基础代码库（undefined） that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing 对噪声和扰动不敏感（undefined） priors that complement photometric cues from images. Our 提供结构的基础代码库（undefined） integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and 速度快、资源消耗少（undefined） 寻找最佳参数或解决方案的过程（undefined）. We also construct the first 用于比较性能的标准数据集或方法（undefined） dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. 大量实验 show that incorporating 3D SAR markedly enhances reconstruction 正确预测占总预测的比例（undefined）, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward 能够处理更大规模数据（undefined） high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.",
      "oneSentenceSummary": "【cs.CV】Da Li等Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Da Li",
          "chinese": null
        },
        {
          "original": "Chen Yao",
          "chinese": null
        },
        {
          "original": "Tong Mao",
          "chinese": null
        },
        {
          "original": "Jiacheng Bao",
          "chinese": null
        },
        {
          "original": "Houjun Sun",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:47:07Z",
      "categories": [
        "cs.CV"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.22045v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22045v1",
      "keyInfo": {
        "contributions": [
          "To address this challenge, we present the first urban NSR 提供结构的基础代码库（undefined） that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22041v1",
      "title": "Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems",
      "originalTitle": "Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems",
      "summary": "Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignm...",
      "plainSummary": "Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification 信息不确定性的度量（undefined）, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited 在预训练模型基础上进行小幅调整（undefined） enables successful cross-system communication. This work positions emergent communication as a 提供结构的基础代码库（undefined） for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.",
      "oneSentenceSummary": "【cs.MA】Naomi Pitzer等Learning to Communicate Across Modalities，使用Unimodal systems communicate m...，在cs.MA取得新进展。",
      "authors": [
        {
          "original": "Naomi Pitzer",
          "chinese": null
        },
        {
          "original": "Daniela Mihai",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:45:41Z",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primaryCategory": "cs.MA",
      "pdfUrl": "https://arxiv.org/pdf/2601.22041v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22041v1",
      "keyInfo": {
        "contributions": [
          "Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings",
          "This work positions emergent communication as a 提供结构的基础代码库（undefined） for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation"
        ],
        "methods": [
          "Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification 信息不确定性的度量（undefined）, while multimodal agents require greater information exchange and exhibit higher uncertainty"
        ],
        "applications": [
          "Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited 在预训练模型基础上进行小幅调整（undefined） enables successful cross-system communication"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22040v1",
      "title": "A Separable Architecture for Continuous Token Representation in Language Models",
      "originalTitle": "A Separable Architecture for Continuous Token Representation in Language Models",
      "summary": "一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), 将离散数据转换为连续向量表示（undefined） matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an arch...",
      "plainSummary": "一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), 将离散数据转换为连续向量表示（undefined） matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous 将离散数据转换为连续向量表示（undefined） generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an 基于实验和观察的（undefined） power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with to more parameters.",
      "oneSentenceSummary": "【cs.CL】Reza T. Batley等A Separable Architecture for Continuous Token Representation in Language Models，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Reza T. Batley",
          "chinese": null
        },
        {
          "original": "Sourav Saha",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:44:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22040v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22040v1",
      "keyInfo": {
        "contributions": [],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22035v1",
      "title": "Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models",
      "originalTitle": "Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models",
      "summary": "Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces pr...",
      "plainSummary": "Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces premature commitment. Masked diffusion language models (MDLMs), which iteratively refine all tokens in parallel, offer a way to decouple computation order from output structure. We validate this capability on GSM8K, Math500, and ReasonOrderQA, a 用于比较性能的标准数据集或方法（undefined） we introduce with controlled difficulty and order-level evaluation. When prompts request answers before reasoning, AR models exhibit large 正确预测占总预测的比例（undefined） gaps compared to standard chain-of-thought ordering (up to 67% relative drop), while MDLMs remain stable (14% relative drop), a property we term \"order robustness\". Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e.g., reasoning steps) earlier in the diffusion process than complex ones (e.g., final answers), enabling reasoning tokens to stabilize before answer commitment. Finally, we identify failure conditions where this advantage weakens, outlining the limits required for order robustness.",
      "oneSentenceSummary": "【cs.CL】Longxuan Yu等Thinking Out of Order，使用Using ReasonOrderQA, we presen...，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Longxuan Yu",
          "chinese": null
        },
        {
          "original": "Yu Fu",
          "chinese": null
        },
        {
          "original": "Shaorong Zhang",
          "chinese": null
        },
        {
          "original": "Hui Liu",
          "chinese": null
        },
        {
          "original": "Mukund Varma T",
          "chinese": null
        },
        {
          "original": "Greg Ver Steeg",
          "chinese": null
        },
        {
          "original": "Yue Dong",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:40:58Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22035v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22035v1",
      "keyInfo": {
        "contributions": [
          ", producing answers before explanations due to presentation or schema constraints)",
          "We validate this capability on GSM8K, Math500, and ReasonOrderQA, a 用于比较性能的标准数据集或方法（undefined） we introduce with controlled difficulty and order-level evaluation"
        ],
        "methods": [
          "Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22031v1",
      "title": "Causal Autoregressive Diffusion 学习语言模式和关系的AI模型（undefined）",
      "originalTitle": "Causal Autoregressive Diffusion Language Model",
      "summary": "In this work, we propose Causal Autoregressive Diffusion (CARD), a 创新的、前人未做过的（undefined） 提供结构的基础代码库（undefined） that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the 寻找最佳参数或解决方案的过程（undefined） in...",
      "plainSummary": "In this work, 我们提出 Causal Autoregressive Diffusion (CARD), a 创新的、前人未做过的（undefined） 提供结构的基础代码库（undefined） that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the 寻找最佳参数或解决方案的过程（undefined） instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a 对噪声和扰动不敏感（undefined） paradigm for next-generation 速度快、资源消耗少（undefined） LLMs.",
      "oneSentenceSummary": "【cs.CL】Junhao Ruan等Causal Autoregressive Diffusion Language Model，使用This design enables dynamic pa...，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Junhao Ruan",
          "chinese": null
        },
        {
          "original": "Bei Li",
          "chinese": null
        },
        {
          "original": "Yongjing Yin",
          "chinese": null
        },
        {
          "original": "Pengcheng Huang",
          "chinese": null
        },
        {
          "original": "Xin Chen",
          "chinese": null
        },
        {
          "original": "Jingang Wang",
          "chinese": null
        },
        {
          "original": "Xunliang Cai",
          "chinese": null
        },
        {
          "original": "Tong Xiao",
          "chinese": null
        },
        {
          "original": "JingBo Zhu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:38:29Z",
      "categories": [
        "cs.CL"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22031v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22031v1",
      "keyInfo": {
        "contributions": [
          "In this work, we propose Causal Autoregressive Diffusion (CARD), a 创新的、前人未做过的（undefined） 提供结构的基础代码库（undefined） that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models",
          "To address the 寻找最佳参数或解决方案的过程（undefined） instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles"
        ],
        "methods": [
          "This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence"
        ],
        "applications": [
          "This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence",
          "Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a 对噪声和扰动不敏感（undefined） paradigm for next-generation 速度快、资源消耗少（undefined） LLMs"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22025v1",
      "title": "When \"Better\" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications",
      "originalTitle": "When \"Better\" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications",
      "summary": "Evaluating 基于海量文本训练的语言模型，如GPT（undefined） (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop. We introduce the Minimum Viable Evaluation Suite (MVES), a tiered se...",
      "plainSummary": "Evaluating 基于海量文本训练的语言模型，如GPT（undefined） (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop. We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes. In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic \"improved\" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes. All test suites, harnesses, and results are included for reproducibility.",
      "oneSentenceSummary": "【cs.CL】Daniel Commey等When \"Better\" Prompts Hurt，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Daniel Commey",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:32:34Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.SE"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.22025v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22025v1",
      "keyInfo": {
        "contributions": [
          "We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop",
          "We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22018v1",
      "title": "PocketDP3: 速度快、资源消耗少（undefined） Pocket-Scale 3D Visuomotor Policy",
      "originalTitle": "PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy",
      "summary": "Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet 速度快、资源消耗少（undefined） point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivat...",
      "plainSummary": "Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet 速度快、资源消耗少（undefined） point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, 我们提出 PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in 观察到数据前的概率（undefined） methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables 速度快、资源消耗少（undefined） fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, 我们的方法 supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves 当前最好的、领先的方法（undefined） performance with fewer than 1% of the parameters of 观察到数据前的概率（undefined） methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of 我们的方法 in real-world settings. Code will be released.",
      "oneSentenceSummary": "【cs.RO】Jinhao Zhang等PocketDP3，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Jinhao Zhang",
          "chinese": null
        },
        {
          "original": "Zhexuan Zhou",
          "chinese": null
        },
        {
          "original": "Huizhe Li",
          "chinese": null
        },
        {
          "original": "Yichen Lai",
          "chinese": null
        },
        {
          "original": "Wenlong Xia",
          "chinese": null
        },
        {
          "original": "Haoming Song",
          "chinese": null
        },
        {
          "original": "Youmin Gong",
          "chinese": null
        },
        {
          "original": "Jie Me",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:23:25Z",
      "categories": [
        "cs.RO"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.22018v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22018v1",
      "keyInfo": {
        "contributions": [
          "Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder",
          "Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in 观察到数据前的概率（undefined） methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks"
        ],
        "methods": [],
        "applications": [
          "This architecture enables 速度快、资源消耗少（undefined） fusion across temporal and channel dimensions, significantly reducing model size"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22005v1",
      "title": "Hierarchy of discriminative power and complexity in learning quantum ensembles",
      "originalTitle": "Hierarchy of discriminative power and complexity in learning quantum ensembles",
      "summary": "Distance metrics are central to 让计算机通过数据自动学习和改进的技术（undefined）, yet distances between ensembles of quantum states remain poorly understood due to fundamental quantum measurement constraints. We introduce a hierarchy of integral probability metrics, termed MMD-, which generalizes the maximum mean discrepancy to quantum ensembles and exhibit a strict trade-off between discriminative power and statist...",
      "plainSummary": "Distance metrics are central to 让计算机通过数据自动学习和改进的技术（undefined）, yet distances between ensembles of quantum states remain poorly understood due to fundamental quantum measurement constraints. We introduce a hierarchy of integral probability metrics, termed MMD-, which generalizes the maximum mean discrepancy to quantum ensembles and exhibit a strict trade-off between discriminative power and statistical efficiency as the moment order increases. For pure-state ensembles of size , estimating MMD- using experimentally feasible SWAP-test-based estimators requires samples for constant , and samples to achieve full discriminative power at . In contrast, the quantum Wasserstein distance attains full discriminative power with samples. These results provide principled guidance for the design of loss functions in quantum 让计算机通过数据自动学习和改进的技术（undefined）, which we illustrate in the training quantum denoising diffusion probabilistic models.",
      "oneSentenceSummary": "【quant-ph】Jian Yao等Hierarchy of discriminative power and complexity in learning quantum ensembles，使用For pure-state ensembles of si...，在quant-ph取得新进展。",
      "authors": [
        {
          "original": "Jian Yao",
          "chinese": null
        },
        {
          "original": "Pengtao Li",
          "chinese": null
        },
        {
          "original": "Xiaohui Chen",
          "chinese": null
        },
        {
          "original": "Quntao Zhuang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:13:48Z",
      "categories": [
        "quant-ph",
        "math.ST",
        "stat.ML"
      ],
      "primaryCategory": "quant-ph",
      "pdfUrl": "https://arxiv.org/pdf/2601.22005v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22005v1",
      "keyInfo": {
        "contributions": [
          "We introduce a hierarchy of integral probability metrics, termed MMD-$k$, which generalizes the maximum mean discrepancy to quantum ensembles and exhibit a strict trade-off between discriminative power and statistical efficiency as the moment order $k$ increases",
          "These results provide principled guidance for the design of loss functions in quantum 让计算机通过数据自动学习和改进的技术（undefined）, which we illustrate in the training quantum denoising diffusion probabilistic models"
        ],
        "methods": [
          "For pure-state ensembles of size $N$, estimating MMD-$k$ using experimentally feasible SWAP-test-based estimators requires $Θ(N^{2-2/k})$ samples for constant $k$, and $Θ(N^3)$ samples to achieve full discriminative power at $k = N$"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.22003v1",
      "title": "速度快、资源消耗少（undefined） Stochastic Optimisation via Sequential Monte Carlo",
      "originalTitle": "Efficient Stochastic Optimisation via Sequential Monte Carlo",
      "summary": "The problem of optimising functions with intractable gradients frequently arise in 让计算机通过数据自动学习和改进的技术（undefined） and statistics, ranging from maximum marginal 给定参数下观察到数据的概率（undefined） estimation procedures to 在预训练模型基础上进行小幅调整（undefined） of generative models. Stochastic approximation methods for this class of problems typically require inner sampling loops to obtain (biased) stochastic gradient esti...",
      "plainSummary": "The problem of optimising functions with intractable gradients frequently arise in 让计算机通过数据自动学习和改进的技术（undefined） and statistics, ranging from maximum marginal 给定参数下观察到数据的概率（undefined） estimation procedures to 在预训练模型基础上进行小幅调整（undefined） of generative models. Stochastic approximation methods for this class of problems typically require inner sampling loops to obtain (biased) stochastic gradient estimates, which rapidly becomes computationally expensive. In this work, we develop sequential Monte Carlo (SMC) samplers for optimisation of functions with intractable gradients. Our approach replaces expensive inner sampling methods with 速度快、资源消耗少（undefined） SMC approximations, which can result in significant computational gains. We establish convergence results for the basic recursions defined by 我们的方法ology which SMC samplers approximate. We demonstrate the effectiveness of our approach on the reward-tuning of energy-based models within various settings.",
      "oneSentenceSummary": "【stat.ML】James Cuin等Efficient Stochastic Optimisation via Sequential Monte Carlo，在stat.ML取得新进展。",
      "authors": [
        {
          "original": "James Cuin",
          "chinese": null
        },
        {
          "original": "Davide Carbone",
          "chinese": null
        },
        {
          "original": "Yanbo Tang",
          "chinese": null
        },
        {
          "original": "O. Deniz Akyildiz",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:13:25Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.CO"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2601.22003v1",
      "abstractUrl": "https://arxiv.org/abs/2601.22003v1",
      "keyInfo": {
        "contributions": [
          "In this work, we develop sequential Monte Carlo (SMC) samplers for optimisation of functions with intractable gradients"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21998v1",
      "title": "Causal World Modeling for Robot Control",
      "originalTitle": "Causal World Modeling for Robot Control",
      "summary": "This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion 提供结构的基础代码库（undefined） that ...",
      "plainSummary": "This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion 提供结构的基础代码库（undefined） that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared 数据的压缩表示空间（undefined）, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference 数据处理或模型训练的完整流程（undefined）, parallelizing action prediction and motor execution to support 速度快、资源消耗少（undefined） control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to 创新的、前人未做过的（undefined） configurations. The code and model are made publicly available to facilitate the community.",
      "oneSentenceSummary": "【cs.CV】Lin Li等Causal World Modeling for Robot Control，在cs.CV取得新进展。",
      "authors": [
        {
          "original": "Lin Li",
          "chinese": null
        },
        {
          "original": "Qihang Zhang",
          "chinese": null
        },
        {
          "original": "Yiming Luo",
          "chinese": null
        },
        {
          "original": "Shuai Yang",
          "chinese": null
        },
        {
          "original": "Ruilin Wang",
          "chinese": null
        },
        {
          "original": "Fei Han",
          "chinese": null
        },
        {
          "original": "Mingrui Yu",
          "chinese": null
        },
        {
          "original": "Zelin Gao",
          "chinese": null
        },
        {
          "original": "Nan Xue",
          "chinese": null
        },
        {
          "original": "Xing Zhu",
          "chinese": null
        },
        {
          "original": "Yujun Shen",
          "chinese": null
        },
        {
          "original": "Yinghao Xu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:07:43Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2601.21998v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21998v1",
      "keyInfo": {
        "contributions": [
          "Inspired by this, we introduce LingBot-VA, an autoregressive diffusion 提供结构的基础代码库（undefined） that learns frame prediction and policy execution simultaneously",
          "Our model features three carefully crafted designs: (1) a shared 数据的压缩表示空间（undefined）, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference 数据处理或模型训练的完整流程（undefined）, parallelizing action prediction and motor execution to support 速度快、资源消耗少（undefined） control"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21996v1",
      "title": "Mechanistic Data Attribution: Tracing the Training Origins of 能够解释其决策过程（undefined） LLM Units",
      "originalTitle": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
      "summary": "While Mechanistic Interpretability has identified 能够解释其决策过程（undefined） circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a 能够处理更大规模数据（undefined） 提供结构的基础代码库（undefined） that employs Influence Functions to trace 能够解释其决策过程（undefined） units back to specific training samples. Through extensive experiments on the Pythia family, we cau...",
      "plainSummary": "While Mechanistic Interpretability has identified 能够解释其决策过程（undefined） circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a 能够处理更大规模数据（undefined） 提供结构的基础代码库（undefined） that employs Influence Functions to trace 能够解释其决策过程（undefined） units back to specific training samples. Through 大量实验 on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of 能够解释其决策过程（undefined） heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's 通过提示中的示例来学习新任务（undefined） (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, 我们提出 a mechanistic data augmentation 数据处理或模型训练的完整流程（undefined） that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.",
      "oneSentenceSummary": "【cs.CL】Jianhui Chen等Mechanistic Data Attribution，使用We introduce Mechanistic Data ...，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Jianhui Chen",
          "chinese": null
        },
        {
          "original": "Yuzhang Luo",
          "chinese": null
        },
        {
          "original": "Liangming Pan",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:06:54Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.21996v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21996v1",
      "keyInfo": {
        "contributions": [
          "We introduce Mechanistic Data Attribution (MDA), a 能够处理更大规模数据（undefined） 提供结构的基础代码库（undefined） that employs Influence Functions to trace 能够解释其决策过程（undefined） units back to specific training samples",
          "Finally, we propose a mechanistic data augmentation 数据处理或模型训练的完整流程（undefined） that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs"
        ],
        "methods": [
          "We introduce Mechanistic Data Attribution (MDA), a 能够处理更大规模数据（undefined） 提供结构的基础代码库（undefined） that employs Influence Functions to trace 能够解释其决策过程（undefined） units back to specific training samples"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21988v1",
      "title": "Generalized Information Gathering Under Dynamics Uncertainty",
      "originalTitle": "Generalized Information Gathering Under Dynamics Uncertainty",
      "summary": "An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying 提供结构的基础代码库（undefined） that decouples these choices from the information-gatheri...",
      "plainSummary": "An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying 提供结构的基础代码库（undefined） that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this 提供结构的基础代码库（undefined）, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our 提供结构的基础代码库（undefined） to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing 基于数学推导的（undefined） justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our 提供结构的基础代码库（undefined） through experiments spanning linear, nonlinear, and multi-agent systems.",
      "oneSentenceSummary": "【cs.LG】Fernando Palafox等Generalized Information Gathering Under Dynamics Uncertainty，使用Using this 提供结构的基础代码库（undefine...，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Fernando Palafox",
          "chinese": null
        },
        {
          "original": "Jingqi Li",
          "chinese": null
        },
        {
          "original": "Jesse Milzman",
          "chinese": null
        },
        {
          "original": "David Fridovich-Keil",
          "chinese": null
        }
      ],
      "published": "2026-01-29T17:00:35Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.RO",
        "eess.SY"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.21988v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21988v1",
      "keyInfo": {
        "contributions": [
          "We present a unifying 提供结构的基础代码库（undefined） that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls"
        ],
        "methods": [
          "Using this 提供结构的基础代码库（undefined）, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21976v1",
      "title": "Macro-Scale Electrostatic Origami Motor",
      "originalTitle": "Macro-Scale Electrostatic Origami Motor",
      "summary": "Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to l...",
      "plainSummary": "Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.",
      "oneSentenceSummary": "【cs.RO】Alex S. Miller等Macro-Scale Electrostatic Origami Motor，使用Using corona discharge for tor...，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Alex S. Miller",
          "chinese": null
        },
        {
          "original": "Leo McElroy",
          "chinese": null
        },
        {
          "original": "Jeffrey H. Lang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T16:53:24Z",
      "categories": [
        "cs.RO",
        "physics.app-ph"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21976v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21976v1",
      "keyInfo": {
        "contributions": [
          "For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure",
          "This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate"
        ],
        "methods": [
          "Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21971v1",
      "title": "MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts",
      "originalTitle": "MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts",
      "summary": "Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of...",
      "plainSummary": "Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike 观察到数据前的概率（undefined） surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We 用于比较性能的标准数据集或方法（undefined） 我们的方法 against 当前最好的、领先的方法（undefined） Vision-Language-Action (VLA) models and the standard ACT 用于对比的基准方法（undefined）. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including 创新的、前人未做过的（undefined） grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.",
      "oneSentenceSummary": "【cs.RO】Lorenzo Mazza等MoE-ACT，使用Unlike 观察到数据前的概率（undefined） su...，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Lorenzo Mazza",
          "chinese": null
        },
        {
          "original": "Ariel Rodriguez",
          "chinese": null
        },
        {
          "original": "Rayan Younis",
          "chinese": null
        },
        {
          "original": "Martin Lelis",
          "chinese": null
        },
        {
          "original": "Ortrun Hellig",
          "chinese": null
        },
        {
          "original": "Chenpan Li",
          "chinese": null
        },
        {
          "original": "Sebastian Bodenstedt",
          "chinese": null
        },
        {
          "original": "Martin Wagner",
          "chinese": null
        },
        {
          "original": "Stefanie Speidel",
          "chinese": null
        }
      ],
      "published": "2026-01-29T16:50:14Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21971v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21971v1",
      "keyInfo": {
        "contributions": [
          "We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy",
          "To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery"
        ],
        "methods": [
          "Unlike 观察到数据前的概率（undefined） surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21959v1",
      "title": "Near-Optimal Private Tests for Simple and MLR Hypotheses",
      "originalTitle": "Near-Optimal Private Tests for Simple and MLR Hypotheses",
      "summary": "We develop a near-optimal testing procedure under the 提供结构的基础代码库（undefined） of Gaussian differential privacy for simple as well as one- and two-sided tests under monotone 给定参数下观察到数据的概率（undefined） ratio conditions. Our mechanism is based on a private mean estimator with data-driven clamping bounds, whose population risk matches the private minimax rate up to logarithmic factors. Using this estimato...",
      "plainSummary": "We develop a near-optimal testing procedure under the 提供结构的基础代码库（undefined） of Gaussian differential privacy for simple as well as one- and two-sided tests under monotone 给定参数下观察到数据的概率（undefined） ratio conditions. Our mechanism is based on a private mean estimator with data-driven clamping bounds, whose population risk matches the private minimax rate up to logarithmic factors. Using this estimator, we construct private test statistics that achieve the same asymptotic relative efficiency as the non-private, most powerful tests while maintaining conservative type I error control. In addition to our 基于数学推导的（undefined） results, our numerical experiments show that our private tests outperform competing DP methods and offer comparable power to the non-private most powerful tests, even at moderately small sample sizes and privacy loss budgets.",
      "oneSentenceSummary": "【stat.ML】Yu-Wei Chen等Near-Optimal Private Tests for Simple and MLR Hypotheses，使用Our mechanism is based on a pr...，在stat.ML取得新进展。",
      "authors": [
        {
          "original": "Yu-Wei Chen",
          "chinese": null
        },
        {
          "original": "Raghu Pasupathy",
          "chinese": null
        },
        {
          "original": "Jordan Awan",
          "chinese": null
        }
      ],
      "published": "2026-01-29T16:36:21Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2601.21959v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21959v1",
      "keyInfo": {
        "contributions": [
          "We develop a near-optimal testing procedure under the 提供结构的基础代码库（undefined） of Gaussian differential privacy for simple as well as one- and two-sided tests under monotone 给定参数下观察到数据的概率（undefined） ratio conditions"
        ],
        "methods": [
          "Our mechanism is based on a private mean estimator with data-driven clamping bounds, whose population risk matches the private minimax rate up to logarithmic factors",
          "Using this estimator, we construct private test statistics that achieve the same asymptotic relative efficiency as the non-private, most powerful tests while maintaining conservative type I error control"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21951v1",
      "title": "Diffusion Path Samplers via Sequential Monte Carlo",
      "originalTitle": "Diffusion Path Samplers via Sequential Monte Carlo",
      "summary": "We develop a diffusion-based sampler for target distributions known up to a normalising constant. To this end, we rely on the well-known diffusion path that smoothly interpolates between a (simple) base distribution and the target distribution, widely used in diffusion models. Our approach is based on a practical implementation of diffusion-annealed Langevin Monte Carlo, which approximates the dif...",
      "plainSummary": "We develop a diffusion-based sampler for target distributions known up to a normalising constant. To this end, we rely on the well-known diffusion path that smoothly interpolates between a (simple) base distribution and the target distribution, widely used in diffusion models. Our approach is based on a practical implementation of diffusion-annealed Langevin Monte Carlo, which approximates the diffusion path with convergence guarantees. We tackle the score estimation problem by developing an 速度快、资源消耗少（undefined） sequential Monte Carlo sampler that evolves auxiliary variables from conditional distributions along the path, which provides principled score estimates for time-varying distributions. We further develop 创新的、前人未做过的（undefined） control variate schedules that minimise the variance of these score estimates. Finally, we provide 基于数学推导的（undefined） guarantees and empirically demonstrate the effectiveness of 我们的方法 on several synthetic and real-world datasets.",
      "oneSentenceSummary": "【stat.ML】James Matthew Young等Diffusion Path Samplers via Sequential Monte Carlo，使用Our approach is based on a pra...，在stat.ML取得新进展。",
      "authors": [
        {
          "original": "James Matthew Young",
          "chinese": null
        },
        {
          "original": "Paula Cordero-Encinar",
          "chinese": null
        },
        {
          "original": "Sebastian Reich",
          "chinese": null
        },
        {
          "original": "Andrew Duncan",
          "chinese": null
        },
        {
          "original": "O. Deniz Akyildiz",
          "chinese": null
        }
      ],
      "published": "2026-01-29T16:32:12Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.CO"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2601.21951v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21951v1",
      "keyInfo": {
        "contributions": [
          "We develop a diffusion-based sampler for target distributions known up to a normalising constant",
          "We tackle the score estimation problem by developing an 速度快、资源消耗少（undefined） sequential Monte Carlo sampler that evolves auxiliary variables from conditional distributions along the path, which provides principled score estimates for time-varying distributions"
        ],
        "methods": [
          "Our approach is based on a practical implementation of diffusion-annealed Langevin Monte Carlo, which approximates the diffusion path with convergence guarantees"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21945v1",
      "title": "Dependence of Equilibrium Propagation Training Success on Network Architecture",
      "originalTitle": "Dependence of Equilibrium Propagation Training Success on Network Architecture",
      "summary": "The rapid rise of 让机器模拟人类智能的技术（undefined） has led to an unsustainable growth in energy consumption. This has motivated progress in neuromorphic computing and physics-based training of learning machines as alternatives to digital neural networks. Many 基于数学推导的（undefined） studies focus on simple architectures like all-to-all or densely connected layered networks. However, these may be challenging to ...",
      "plainSummary": "The rapid rise of 让机器模拟人类智能的技术（undefined） has led to an unsustainable growth in energy consumption. This has motivated progress in neuromorphic computing and physics-based training of learning machines as alternatives to digital neural networks. Many 基于数学推导的（undefined） studies focus on simple architectures like all-to-all or densely connected layered networks. However, these may be challenging to realize experimentally, e.g. due to connectivity constraints. In this work, we investigate the performance of the widespread physics-based training method of equilibrium propagation for more realistic architectural choices, specifically, locally connected lattices. We train an XY model and explore the influence of architecture on various 用于比较性能的标准数据集或方法（undefined） tasks, tracking the evolution of spatially distributed responses and couplings during training. Our results show that sparse networks with only local connections can achieve performance comparable to dense networks. Our findings provide guidelines for further scaling up architectures based on equilibrium propagation in realistic settings.",
      "oneSentenceSummary": "【cs.LG】Qingshan Wang等Dependence of Equilibrium Propagation Training Success on Network Architecture，使用Our findings provide guideline...，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Qingshan Wang",
          "chinese": null
        },
        {
          "original": "Clara C. Wanjura",
          "chinese": null
        },
        {
          "original": "Florian Marquardt",
          "chinese": null
        }
      ],
      "published": "2026-01-29T16:29:31Z",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.ET",
        "cs.NE"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.21945v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21945v1",
      "keyInfo": {
        "contributions": [],
        "methods": [
          "Our findings provide guidelines for further scaling up architectures based on equilibrium propagation in realistic settings"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21942v1",
      "title": "Clustering in Deep Stochastic Transformers",
      "originalTitle": "Clustering in Deep Stochastic Transformers",
      "summary": "Transformers have revolutionized 使用多层神经网络来处理复杂模式的技术（undefined） across various domains but understanding the precise token dynamics remains a 基于数学推导的（undefined） challenge. Existing theories of deep Transformers with layer normalization typically predict that tokens cluster to a single point; however, these results rely on deterministic weight assumptions, which fail to capture the standard initiali...",
      "plainSummary": "Transformers have revolutionized 使用多层神经网络来处理复杂模式的技术（undefined） across various domains but understanding the precise token dynamics remains a 基于数学推导的（undefined） challenge. Existing theories of deep Transformers with layer normalization typically predict that tokens cluster to a single point; however, these results rely on deterministic weight assumptions, which fail to capture the standard initialization scheme in Transformers. In this work, we show that accounting for the intrinsic stochasticity of random initialization alters this picture. More precisely, we analyze deep Transformers where noise arises from the random initialization of value matrices. Under diffusion scaling and token-wise RMS normalization, we prove that, as the number of 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） layers goes to infinity, the discrete token dynamics converge to an interacting-particle system on the sphere where tokens are driven by a \\emph{common} matrix-valued Brownian noise. In this limit, we show that initialization noise prevents the collapse to a single cluster predicted by deterministic models. For two tokens, we prove a phase transition governed by the interaction strength and the token dimension: unlike deterministic attention flows, antipodal configurations become attracting with positive probability. Numerical experiments confirm the predicted transition, reveal that antipodal formations persist for more than two tokens, and demonstrate that suppressing the intrinsic noise degrades 正确预测占总预测的比例（undefined）.",
      "oneSentenceSummary": "【stat.ML】Lev Fedorov等Clustering in Deep Stochastic Transformers，在stat.ML取得新进展。",
      "authors": [
        {
          "original": "Lev Fedorov",
          "chinese": null
        },
        {
          "original": "Michaël E. Sander",
          "chinese": null
        },
        {
          "original": "Romuald Elie",
          "chinese": null
        },
        {
          "original": "Pierre Marion",
          "chinese": null
        },
        {
          "original": "Mathieu Laurière",
          "chinese": null
        }
      ],
      "published": "2026-01-29T16:28:13Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2601.21942v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21942v1",
      "keyInfo": {
        "contributions": [],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21926v1",
      "title": "Information Filtering via Variational 防止过拟合的技术（undefined） for Robot Manipulation",
      "originalTitle": "Information Filtering via Variational Regularization for Robot Manipulation",
      "summary": "Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, 基于实验和观察的（undefined） evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find t...",
      "plainSummary": "Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, 基于实验和观察的（undefined） evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, 我们提出 Variational 防止过拟合的技术（undefined） (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. 大量实验 on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the 用于对比的基准方法（undefined） DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new 当前最好的、领先的方法（undefined） results. Real-world experiments further demonstrate that 我们的方法 performs well in practical deployments. Code will released.",
      "oneSentenceSummary": "【cs.RO】Jinhao Zhang等Information Filtering via Variational Regularization for Robot Manipulation，使用To this end, we propose Variat...，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Jinhao Zhang",
          "chinese": null
        },
        {
          "original": "Wenlong Xia",
          "chinese": null
        },
        {
          "original": "Yaojia Wang",
          "chinese": null
        },
        {
          "original": "Zhexuan Zhou",
          "chinese": null
        },
        {
          "original": "Huizhe Li",
          "chinese": null
        },
        {
          "original": "Yichen Lai",
          "chinese": null
        },
        {
          "original": "Haoming Song",
          "chinese": null
        },
        {
          "original": "Youmin Gong",
          "chinese": null
        },
        {
          "original": "Jie Me",
          "chinese": null
        }
      ],
      "published": "2026-01-29T16:17:42Z",
      "categories": [
        "cs.RO"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21926v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21926v1",
      "keyInfo": {
        "contributions": [
          "Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills",
          "While increasing model capacity can improve denoising, 基于实验和观察的（undefined） evidence suggests that it also introduces redundancy and noise in intermediate feature blocks"
        ],
        "methods": [
          "To this end, we propose Variational 防止过拟合的技术（undefined） (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21924v1",
      "title": "Optimistic Transfer under Task Shift via Bellman Alignment",
      "originalTitle": "Optimistic Transfer under Task Shift via Bellman Alignment",
      "summary": "We study online transfer 通过试错学习最佳策略的机器学习方法（undefined） (RL) in episodic Markov decision processes, where experience from related source tasks is available during learning on a target task. A fundamental difficulty is that task similarity is typically defined in terms of rewards or transitions, whereas online RL algorithms operate on Bellman regression targets. As a result, naively reusing source Be...",
      "plainSummary": "We study online transfer 通过试错学习最佳策略的机器学习方法（undefined） (RL) in episodic Markov decision processes, where experience from related source tasks is available during learning on a target task. A fundamental difficulty is that task similarity is typically defined in terms of rewards or transitions, whereas online RL algorithms operate on Bellman regression targets. As a result, naively reusing source Bellman updates introduces systematic bias and invalidates regret guarantees. We identify one-step Bellman alignment as the correct abstraction for transfer in online RL and propose re-weighted targeting (RWT), an operator-level correction that retargets continuation values and compensates for transition mismatch via a change of measure. RWT reduces task mismatch to a fixed one-step correction and enables statistically sound reuse of source data. This alignment yields a two-stage RWT -learning 提供结构的基础代码库（undefined） that separates variance reduction from bias correction. Under RKHS function approximation, we establish regret bounds that scale with the complexity of the task shift rather than the target MDP. 基于实验和观察的（undefined） results in both tabular and 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） settings demonstrate consistent improvements over single-task learning and naïve pooling, highlighting Bellman alignment as a model-agnostic transfer principle for online RL.",
      "oneSentenceSummary": "【cs.LG】Jinhang Chai等Optimistic Transfer under Task Shift via Bellman Alignment，使用As a result, naively reusing s...，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Jinhang Chai",
          "chinese": null
        },
        {
          "original": "Enpei Zhang",
          "chinese": null
        },
        {
          "original": "Elynn Chen",
          "chinese": null
        },
        {
          "original": "Yujun Yan",
          "chinese": null
        }
      ],
      "published": "2026-01-29T16:16:24Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.21924v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21924v1",
      "keyInfo": {
        "contributions": [
          "As a result, naively reusing source Bellman updates introduces systematic bias and invalidates regret guarantees",
          "We identify one-step Bellman alignment as the correct abstraction for transfer in online RL and propose re-weighted targeting (RWT), an operator-level correction that retargets continuation values and compensates for transition mismatch via a change of measure"
        ],
        "methods": [
          "As a result, naively reusing source Bellman updates introduces systematic bias and invalidates regret guarantees"
        ],
        "applications": [
          "RWT reduces task mismatch to a fixed one-step correction and enables statistically sound reuse of source data"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21917v1",
      "title": "On Approximate Computation of Critical Points",
      "originalTitle": "On Approximate Computation of Critical Points",
      "summary": "We show that computing even very coarse approximations of critical points is intractable for simple classes of nonconvex functions. More concretely, we prove that if there exists a polynomial-time algorithm that takes as input a polynomial in variables of constant degree (as low as three) and outputs a point whose gradient has Euclidean norm at most whenever the polynomial has a critical point, th...",
      "plainSummary": "We show that computing even very coarse approximations of critical points is intractable for simple classes of nonconvex functions. More concretely, we prove that if there exists a polynomial-time algorithm that takes as input a polynomial in variables of constant degree (as low as three) and outputs a point whose gradient has Euclidean norm at most whenever the polynomial has a critical point, then P=NP. The algorithm is permitted to return an arbitrary point when no critical point exists. We also prove hardness results for approximate computation of critical points under additional structural assumptions, including settings in which existence and uniqueness of a critical point are guaranteed, the function is lower bounded, and approximation is measured in terms of distance to a critical point. Overall, our results stand in contrast to the commonly-held belief that, in nonconvex 寻找最佳参数或解决方案的过程（undefined）, approximate computation of critical points is a tractable task.",
      "oneSentenceSummary": "【math.OC】Amir Ali Ahmadi等On Approximate Computation of Critical Points，在math.OC取得新进展。",
      "authors": [
        {
          "original": "Amir Ali Ahmadi",
          "chinese": null
        },
        {
          "original": "Georgina Hall",
          "chinese": null
        }
      ],
      "published": "2026-01-29T16:08:22Z",
      "categories": [
        "math.OC",
        "cs.CC",
        "cs.LG",
        "math.NA",
        "stat.ML"
      ],
      "primaryCategory": "math.OC",
      "pdfUrl": "https://arxiv.org/pdf/2601.21917v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21917v1",
      "keyInfo": {
        "contributions": [],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21895v1",
      "title": "Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text",
      "originalTitle": "Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text",
      "summary": "Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystif...",
      "plainSummary": "Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their generalization ability. Building on this insight, we introduce a 创新的、前人未做过的（undefined） rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text. Theoretically, we demonstrate that employing an adaptively learned distance function is more effective for detection than using a fixed distance. Empirically, we conduct 大量实验 with over 100 settings, and find that our approach demonstrates superior performance over 用于对比的基准方法（undefined） algorithms in the majority of scenarios. In particular, it achieves relative improvements from 57.8\\% to 80.6\\% over the strongest 用于对比的基准方法（undefined） across different target LLMs (e.g., GPT, Claude, and Gemini).",
      "oneSentenceSummary": "【cs.CL】Hongyi Zhou等Learn-to-Distance，使用Theoretically, we demonstrate ...，在cs.CL取得新进展。",
      "authors": [
        {
          "original": "Hongyi Zhou",
          "chinese": null
        },
        {
          "original": "Jin Zhu",
          "chinese": null
        },
        {
          "original": "Erhan Xu",
          "chinese": null
        },
        {
          "original": "Kai Ye",
          "chinese": null
        },
        {
          "original": "Ying Yang",
          "chinese": null
        },
        {
          "original": "Chengchun Shi",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:55:15Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "stat.ML"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2601.21895v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21895v1",
      "keyInfo": {
        "contributions": [
          "In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their generalization ability",
          "Building on this insight, we introduce a 创新的、前人未做过的（undefined） rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text"
        ],
        "methods": [
          "Theoretically, we demonstrate that employing an adaptively learned distance function is more effective for detection than using a fixed distance"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21887v1",
      "title": "VSE: Variational state estimation of complex model-free process",
      "originalTitle": "VSE: Variational state estimation of complex model-free process",
      "summary": "We design a variational state estimation (VSE) method that provides a closed-form Gaussian 观察到数据后的概率（undefined） of an underlying complex dynamical process from (noisy) nonlinear measurements. The complex process is model-free. That is, we do not have a suitable physics-based model characterizing the temporal evolution of the process state. The closed-form Gaussian 观察到数据后的概率（undefined） is provided ...",
      "plainSummary": "We design a variational state estimation (VSE) method that provides a closed-form Gaussian 观察到数据后的概率（undefined） of an underlying complex dynamical process from (noisy) nonlinear measurements. The complex process is model-free. That is, we do not have a suitable physics-based model characterizing the temporal evolution of the process state. The closed-form Gaussian 观察到数据后的概率（undefined） is provided by a recurrent 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） (RNN). The use of RNN is computationally simple in the inference phase. For learning the RNN, an additional RNN is used in the learning phase. Both RNNs help each other learn better based on variational inference principles. The VSE is demonstrated for a tracking application - state estimation of a stochastic Lorenz system (a 用于比较性能的标准数据集或方法（undefined） process) using a 2-D camera measurement model. The VSE is shown to be competitive against a particle filter that knows the Lorenz system model and a recently proposed data-driven state estimation method that does not know the Lorenz system model.",
      "oneSentenceSummary": "【eess.SP】Gustav Norén等VSE，使用Both RNNs help each other lear...，在eess.SP取得新进展。",
      "authors": [
        {
          "original": "Gustav Norén",
          "chinese": null
        },
        {
          "original": "Anubhab Ghosh",
          "chinese": null
        },
        {
          "original": "Fredrik Cumlin",
          "chinese": null
        },
        {
          "original": "Saikat Chatterjee",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:47:28Z",
      "categories": [
        "eess.SP",
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "eess.SP",
      "pdfUrl": "https://arxiv.org/pdf/2601.21887v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21887v1",
      "keyInfo": {
        "contributions": [
          "We design a variational state estimation (VSE) method that provides a closed-form Gaussian 观察到数据后的概率（undefined） of an underlying complex dynamical process from (noisy) nonlinear measurements",
          "The VSE is shown to be competitive against a particle filter that knows the Lorenz system model and a recently proposed data-driven state estimation method that does not know the Lorenz system model"
        ],
        "methods": [
          "Both RNNs help each other learn better based on variational inference principles",
          "The VSE is demonstrated for a tracking application - state estimation of a stochastic Lorenz system (a 用于比较性能的标准数据集或方法（undefined） process) using a 2-D camera measurement model"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21885v1",
      "title": "Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed when Solving Expensive Unconstrained Multi-Objective Optimisation Problems",
      "originalTitle": "Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed when Solving Expensive Unconstrained Multi-Objective Optimisation Problems",
      "summary": "Multi-Objective Evolutionary Algorithms (MOEAs) have proven effective at solving Multi-Objective Optimisation Problems (MOOPs). However, their performance can be significantly hindered when applied to computationally intensive industrial problems. To address this limitation, we propose an adaptive surrogate modelling approach designed to accelerate the early-stage convergence speed of 当前最好的、领先的方法（...",
      "plainSummary": "Multi-Objective Evolutionary Algorithms (MOEAs) have proven effective at solving Multi-Objective Optimisation Problems (MOOPs). However, their performance can be significantly hindered when applied to computationally intensive industrial problems. To address this limitation, 我们提出 an adaptive surrogate modelling approach designed to accelerate the early-stage convergence speed of 当前最好的、领先的方法（undefined） MOEAs. This is important because it ensures that a solver can identify optimal or near-optimal solutions with relatively few fitness function evaluations, thereby saving both time and computational resources. 我们的方法 employs a two-loop architecture. The outer loop runs a (用于对比的基准方法（undefined）) host MOEA which carries out true fitness evaluations. The inner loop contains an Adaptive Accelerator that leverages data-driven 让计算机通过数据自动学习和改进的技术（undefined） (ML) surrogate models to approximate fitness functions. Integrated with NSGA-II and MOEA/D, our approach was tested on 31 widely known 用于比较性能的标准数据集或方法（undefined） problems and a real-world North Sea fish abundance modelling case study. The results demonstrate that by incorporating Gaussian Process Regression, one-dimensional Convolutional Neural Networks, and Random Forest Regression, our proposed approach significantly accelerates the convergence speed of MOEAs in the early phases of optimisation.",
      "oneSentenceSummary": "【cs.NE】Tiwonge Msulira Banda等Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed when Solving Expensive Unconstrained Multi-Objective Optimisation Problems，使用Our method employs a two-loop ...，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Tiwonge Msulira Banda",
          "chinese": null
        },
        {
          "original": "Alexandru-Ciprian Zăvoianu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:46:52Z",
      "categories": [
        "cs.NE"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.21885v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21885v1",
      "keyInfo": {
        "contributions": [
          "To address this limitation, we propose an adaptive surrogate modelling approach designed to accelerate the early-stage convergence speed of 当前最好的、领先的方法（undefined） MOEAs",
          "The results demonstrate that by incorporating Gaussian Process Regression, one-dimensional Convolutional Neural Networks, and Random Forest Regression, our proposed approach significantly accelerates the convergence speed of MOEAs in the early phases of optimisation"
        ],
        "methods": [
          "Our method employs a two-loop architecture",
          "The inner loop contains an Adaptive Accelerator that leverages data-driven 让计算机通过数据自动学习和改进的技术（undefined） (ML) surrogate models to approximate fitness functions"
        ],
        "applications": [
          "However, their performance can be significantly hindered when applied to computationally intensive industrial problems"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21884v1",
      "title": "Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation",
      "originalTitle": "Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation",
      "summary": "Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses th...",
      "plainSummary": "Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a 能够处理更大规模数据（undefined）, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and 能够处理更大规模数据（undefined） variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.",
      "oneSentenceSummary": "【cs.RO】Pratik Ingle等Multi-Modular MANTA-RAY，使用The proposed multi-module MANT...，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Pratik Ingle",
          "chinese": null
        },
        {
          "original": "Jørn Lambertsen",
          "chinese": null
        },
        {
          "original": "Kasper Støy",
          "chinese": null
        },
        {
          "original": "Andres Faina",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:46:50Z",
      "categories": [
        "cs.RO"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21884v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21884v1",
      "keyInfo": {
        "contributions": [
          "While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability",
          "In this work, we present a distributed, modular, and 能够处理更大规模数据（undefined） variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density"
        ],
        "methods": [
          "The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training"
        ],
        "applications": [
          "Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a 能够处理更大规模数据（undefined）, multi-module configuration remain unexplored",
          "The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21877v1",
      "title": "Evolution of 用于比较性能的标准数据集或方法（undefined）: Black-Box 寻找最佳参数或解决方案的过程（undefined） 用于比较性能的标准数据集或方法（undefined） Design through 基于海量文本训练的语言模型，如GPT（undefined）",
      "originalTitle": "Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model",
      "summary": "用于比较性能的标准数据集或方法（undefined） Design in Black-Box 寻找最佳参数或解决方案的过程（undefined） (BBO) is a fundamental yet open-ended topic. Early BBO benchmarks are predominantly human-crafted, introducing expert bias and constraining diversity. Automating this design process can relieve the human-in-the-loop burden while enhancing diversity and objectivity. We propose Evolution of 用于比较性能的标准数据集或方法（undefined） (EoB), an ...",
      "plainSummary": "用于比较性能的标准数据集或方法（undefined） Design in Black-Box 寻找最佳参数或解决方案的过程（undefined） (BBO) is a fundamental yet open-ended topic. Early BBO benchmarks are predominantly human-crafted, introducing expert bias and constraining diversity. Automating this design process can relieve the human-in-the-loop burden while enhancing diversity and objectivity. 我们提出 Evolution of 用于比较性能的标准数据集或方法（undefined） (EoB), an automated BBO 用于比较性能的标准数据集或方法（undefined） designer empowered by the 基于海量文本训练的语言模型，如GPT（undefined） (LLM) and its program evolution capability. Specifically, we formulate 用于比较性能的标准数据集或方法（undefined） design as a bi-objective 寻找最佳参数或解决方案的过程（undefined） problem towards maximizing (i) landscape diversity and (ii) algorithm-differentiation ability across a portfolio of BBO solvers. Under this paradigm, EoB iteratively prompts LLM to evolve a population of 用于比较性能的标准数据集或方法（undefined） programs and employs a reflection-based scheme to co-evolve the landscape and its corresponding program. 覆盖广泛的、详细的（undefined） experiments validate our EoB is a competitive candidate in multi-dimensional usages: 1) Benchmarking BBO algorithms; 2) Training and testing learning-assisted BBO algorithms; 3) Extending proxy for expensive real-world problems.",
      "oneSentenceSummary": "【cs.NE】Chen Wang等Evolution of Benchmark，使用Under this paradigm, EoB itera...，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Chen Wang",
          "chinese": null
        },
        {
          "original": "Sijie Ma",
          "chinese": null
        },
        {
          "original": "Zeyuan Ma",
          "chinese": null
        },
        {
          "original": "Yue-Jiao Gong",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:45:11Z",
      "categories": [
        "cs.NE"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.21877v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21877v1",
      "keyInfo": {
        "contributions": [
          "用于比较性能的标准数据集或方法（undefined） Design in Black-Box 寻找最佳参数或解决方案的过程（undefined） (BBO) is a fundamental yet open-ended topic",
          "Automating this design process can relieve the human-in-the-loop burden while enhancing diversity and objectivity"
        ],
        "methods": [
          "Under this paradigm, EoB iteratively prompts LLM to evolve a population of 用于比较性能的标准数据集或方法（undefined） programs and employs a reflection-based scheme to co-evolve the landscape and its corresponding program"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21876v1",
      "title": "LLM-Driven Scenario-Aware Planning for Autonomous Driving",
      "originalTitle": "LLM-Driven Scenario-Aware Planning for Autonomous Driving",
      "summary": "Hybrid planner switching 提供结构的基础代码库（undefined） (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain 速度快、资源消耗少（undefined） driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, t...",
      "plainSummary": "Hybrid planner switching 提供结构的基础代码库（undefined） (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain 速度快、资源消耗少（undefined） driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a 基于海量文本训练的语言模型，如GPT（undefined） (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint 寻找最佳参数或解决方案的过程（undefined） of mode configuration and motion planning. The joint 寻找最佳参数或解决方案的过程（undefined） is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.",
      "oneSentenceSummary": "【cs.RO】He Li等LLM-Driven Scenario-Aware Planning for Autonomous Driving，使用The joint 寻找最佳参数或解决方案的过程（undef...，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "He Li",
          "chinese": null
        },
        {
          "original": "Zhaowei Chen",
          "chinese": null
        },
        {
          "original": "Rui Gao",
          "chinese": null
        },
        {
          "original": "Guoliang Li",
          "chinese": null
        },
        {
          "original": "Qi Hao",
          "chinese": null
        },
        {
          "original": "Shuai Wang",
          "chinese": null
        },
        {
          "original": "Chengzhong Xu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:42:13Z",
      "categories": [
        "cs.RO"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21876v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21876v1",
      "keyInfo": {
        "contributions": [
          "To address the limitation, this paper proposes LAP, a 基于海量文本训练的语言模型，如GPT（undefined） (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps",
          "High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate"
        ],
        "methods": [
          "The joint 寻找最佳参数或解决方案的过程（undefined） is solved using tree-search model predictive control and alternating minimization"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21873v1",
      "title": "Low-Rank Plus Sparse Matrix 将在一个任务学到的知识应用到其他任务（undefined） under Growing Representations and Ambient Dimensions",
      "originalTitle": "Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions",
      "summary": "Learning systems often expand their ambient features or latent representations over time, 将离散数据转换为连续向量表示（undefined） earlier representations into larger spaces with limited new latent structure. We study 将在一个任务学到的知识应用到其他任务（undefined） for structured matrix estimation under simultaneous growth of the ambient dimension and the intrinsic representation, where a well-estimated source task is embedded as...",
      "plainSummary": "Learning systems often expand their ambient features or latent representations over time, 将离散数据转换为连续向量表示（undefined） earlier representations into larger spaces with limited new latent structure. We study 将在一个任务学到的知识应用到其他任务（undefined） for structured matrix estimation under simultaneous growth of the ambient dimension and the intrinsic representation, where a well-estimated source task is embedded as a subspace of a higher-dimensional target task. 我们提出 a general transfer 提供结构的基础代码库（undefined） in which the target parameter decomposes into an embedded source component, low-dimensional low-rank innovations, and sparse edits, and develop an anchored alternating projection estimator that preserves transferred subspaces while estimating only low-dimensional innovations and sparse modifications. We establish deterministic error bounds that separate target noise, representation growth, and source estimation error, yielding strictly improved rates when rank and sparsity increments are small. We demonstrate the generality of the 提供结构的基础代码库（undefined） by applying it to two canonical problems. For Markov transition matrix estimation from a single trajectory, we derive end-to-end 基于数学推导的（undefined） guarantees under dependent noise. For structured covariance estimation under enlarged dimensions, we provide complementary 基于数学推导的（undefined） analysis in the appendix and empirically validate consistent transfer gains.",
      "oneSentenceSummary": "【cs.LG】Jinhang Chai等Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Jinhang Chai",
          "chinese": null
        },
        {
          "original": "Xuyuan Liu",
          "chinese": null
        },
        {
          "original": "Elynn Chen",
          "chinese": null
        },
        {
          "original": "Yujun Yan",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:40:05Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.21873v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21873v1",
      "keyInfo": {
        "contributions": [
          "Learning systems often expand their ambient features or latent representations over time, 将离散数据转换为连续向量表示（undefined） earlier representations into larger spaces with limited new latent structure",
          "We study 将在一个任务学到的知识应用到其他任务（undefined） for structured matrix estimation under simultaneous growth of the ambient dimension and the intrinsic representation, where a well-estimated source task is embedded as a subspace of a higher-dimensional target task"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21868v1",
      "title": "On Forgetting and Stability of Score-based Generative models",
      "originalTitle": "On Forgetting and Stability of Score-based Generative models",
      "summary": "Understanding the stability and long-time behavior of generative models is a fundamental problem in modern 让计算机通过数据自动学习和改进的技术（undefined）. This paper provides quantitative bounds on the sampling error of score-based generative models by leveraging stability and forgetting properties of the Markov chain associated with the reverse-time dynamics. Under weak assumptions, we provide the two structural ...",
      "plainSummary": "Understanding the stability and long-time behavior of generative models is a fundamental problem in modern 让计算机通过数据自动学习和改进的技术（undefined）. This paper provides quantitative bounds on the sampling error of score-based generative models by leveraging stability and forgetting properties of the Markov chain associated with the reverse-time dynamics. Under weak assumptions, we provide the two structural properties to ensure the propagation of initialization and discretization errors of the backward process: a Lyapunov drift condition and a Doeblin-type minorization condition. A practical consequence is quantitative stability of the sampling procedure, as the reverse diffusion dynamics induces a contraction mechanism along the sampling trajectory. Our results clarify the role of stochastic dynamics in score-based models and provide a principled 提供结构的基础代码库（undefined） for analyzing propagation of errors in such approaches.",
      "oneSentenceSummary": "【stat.ML】Stanislas Strasman等On Forgetting and Stability of Score-based Generative models，在stat.ML取得新进展。",
      "authors": [
        {
          "original": "Stanislas Strasman",
          "chinese": null
        },
        {
          "original": "Gabriel Cardoso",
          "chinese": null
        },
        {
          "original": "Sylvain Le Corff",
          "chinese": null
        },
        {
          "original": "Vincent Lemaire",
          "chinese": null
        },
        {
          "original": "Antonio Ocello",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:37:50Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2601.21868v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21868v1",
      "keyInfo": {
        "contributions": [],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21860v1",
      "title": "Pathwise Learning of Stochastic Dynamical Systems with Partial Observations",
      "originalTitle": "Pathwise Learning of Stochastic Dynamical Systems with Partial Observations",
      "summary": "The reconstruction and inference of stochastic dynamical systems from data is a fundamental task in inverse problems and statistical learning. While surrogate modeling advances computational methods to approximate these dynamics, standard approaches typically require high-fidelity training data. In many practical settings, the data are indirectly observed through noisy and nonlinear measurement. T...",
      "plainSummary": "The reconstruction and inference of stochastic dynamical systems from data is a fundamental task in inverse problems and statistical learning. While surrogate modeling advances computational methods to approximate these dynamics, standard approaches typically require high-fidelity training data. In many practical settings, the data are indirectly observed through noisy and nonlinear measurement. The challenge lies not only in approximating the coefficients of the SDEs, but in simultaneously inferring the 观察到数据后的概率（undefined） updates given the observations. In this work, we present a neural path estimation approach to solve stochastic dynamical systems based on variational inference. We first derive a stochastic control problem that solve filtering 观察到数据后的概率（undefined） path measure corresponding to a pathwise Zakai equation. We then construct a 能够创建新数据的AI模型（undefined） that maps the 观察到数据前的概率（undefined） path measure to 观察到数据后的概率（undefined） measure through the controlled diffusion and the associated Randon-Nykodym derivative. Through an amortization of sample paths of the observation process, the control is learned by an 将离散数据转换为连续向量表示（undefined） of the noisy observation paths. Thus, we learn the unknown 观察到数据前的概率（undefined） SDE and the control can recover the conditional path measure given the observation sample paths and we learn an associated SDE which induces the same path measure. In the end, we perform experiments on nonlinear dynamical systems, demonstrating the model's ability to learn multimodal, chaotic, or high dimensional systems.",
      "oneSentenceSummary": "【math.OC】Nicole Tianjiao Yang等Pathwise Learning of Stochastic Dynamical Systems with Partial Observations，使用In this work, we present a neu...，在math.OC取得新进展。",
      "authors": [
        {
          "original": "Nicole Tianjiao Yang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:30:13Z",
      "categories": [
        "math.OC",
        "stat.ML"
      ],
      "primaryCategory": "math.OC",
      "pdfUrl": "https://arxiv.org/pdf/2601.21860v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21860v1",
      "keyInfo": {
        "contributions": [
          "In this work, we present a neural path estimation approach to solve stochastic dynamical systems based on variational inference"
        ],
        "methods": [
          "In this work, we present a neural path estimation approach to solve stochastic dynamical systems based on variational inference"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21856v1",
      "title": "Blind Ultrasound Image Enhancement via Self-Supervised Physics-Guided Degradation Modeling",
      "originalTitle": "Blind Ultrasound Image Enhancement via Self-Supervised Physics-Guided Degradation Modeling",
      "summary": "Ultrasound (US) interpretation is hampered by multiplicative speckle, acquisition blur from the point-spread function (PSF), and scanner- and operator-dependent artifacts. Supervised enhancement methods assume access to clean targets or known degradations; conditions rarely met in practice. We present a blind, self-supervised enhancement 提供结构的基础代码库（undefined） that jointly deconvolves and denoises ...",
      "plainSummary": "Ultrasound (US) interpretation is hampered by multiplicative speckle, acquisition blur from the point-spread function (PSF), and scanner- and operator-dependent artifacts. Supervised enhancement methods assume access to clean targets or known degradations; conditions rarely met in practice. We present a blind, self-supervised enhancement 提供结构的基础代码库（undefined） that jointly deconvolves and denoises B-mode images using a Swin Convolutional U-Net trained with a \\emph{physics-guided} degradation model. From each training frame, we extract rotated/cropped patches and synthesize inputs by (i) convolving with a Gaussian PSF surrogate and (ii) injecting noise via either spatial additive Gaussian noise or complex Fourier-domain perturbations that emulate phase/magnitude distortions. For US scans, clean-like targets are obtained via non-local low-rank (NLLR) denoising, removing the need for ground truth; for natural images, the originals serve as targets. Trained and validated on UDIAT~B, JNU-IFM, and XPIE Set-P, and evaluated additionally on a 700-image PSFHS test set, the method achieves the highest PSNR/SSIM across Gaussian and speckle noise levels, with margins that widen under stronger corruption. Relative to MSANN, Restormer, and DnCNN, it typically preserves an extra 1--4\\,dB PSNR and 0.05--0.15 SSIM in heavy Gaussian noise, and 2--5\\,dB PSNR and 0.05--0.20 SSIM under severe speckle. Controlled PSF studies show reduced FWHM and higher peak gradients, evidence of resolution recovery without edge erosion. Used as a plug-and-play preprocessor, it consistently boosts Dice for fetal head and pubic symphysis segmentation. Overall, the approach offers a practical, assumption-light path to 对噪声和扰动不敏感（undefined） US enhancement that generalizes across datasets, scanners, and degradation types.",
      "oneSentenceSummary": "【eess.IV】Shujaat Khan等Blind Ultrasound Image Enhancement via Self-Supervised Physics-Guided Degradation Modeling，使用We present a blind, self-super...，在eess.IV取得新进展。",
      "authors": [
        {
          "original": "Shujaat Khan",
          "chinese": null
        },
        {
          "original": "Syed Muhammad Atif",
          "chinese": null
        },
        {
          "original": "Jaeyoung Huh",
          "chinese": null
        },
        {
          "original": "Syed Saad Azhar",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:28:25Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "stat.ML"
      ],
      "primaryCategory": "eess.IV",
      "pdfUrl": "https://arxiv.org/pdf/2601.21856v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21856v1",
      "keyInfo": {
        "contributions": [
          "We present a blind, self-supervised enhancement 提供结构的基础代码库（undefined） that jointly deconvolves and denoises B-mode images using a Swin Convolutional U-Net trained with a \\emph{physics-guided} degradation model"
        ],
        "methods": [
          "We present a blind, self-supervised enhancement 提供结构的基础代码库（undefined） that jointly deconvolves and denoises B-mode images using a Swin Convolutional U-Net trained with a \\emph{physics-guided} degradation model"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21847v1",
      "title": "READY: Reward Discovery for Meta-Black-Box 寻找最佳参数或解决方案的过程（undefined）",
      "originalTitle": "READY: Reward Discovery for Meta-Black-Box Optimization",
      "summary": "Meta-Black-Box 寻找最佳参数或解决方案的过程（undefined） (MetaBBO) is an emerging avenue within 寻找最佳参数或解决方案的过程（undefined） community, where algorithm design policy could be meta-learned by 通过试错学习最佳策略的机器学习方法（undefined） to enhance 寻找最佳参数或解决方案的过程（undefined） performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In...",
      "plainSummary": "Meta-Black-Box 寻找最佳参数或解决方案的过程（undefined） (MetaBBO) is an emerging avenue within 寻找最佳参数或解决方案的过程（undefined） community, where algorithm design policy could be meta-learned by 通过试错学习最佳策略的机器学习方法（undefined） to enhance 寻找最佳参数或解决方案的过程（undefined） performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use 基于海量文本训练的语言模型，如GPT（undefined）~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. 基于实验和观察的（undefined） results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY's project at https://anonymous.4open.science/r/ICML_READY-747F.",
      "oneSentenceSummary": "【cs.LG】Zechuan Huang等READY，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Zechuan Huang",
          "chinese": null
        },
        {
          "original": "Zhiguang Cao",
          "chinese": null
        },
        {
          "original": "Hongshu Guo",
          "chinese": null
        },
        {
          "original": "Yue-Jiao Gong",
          "chinese": null
        },
        {
          "original": "Zeyuan Ma",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:23:18Z",
      "categories": [
        "cs.LG",
        "cs.NE"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.21847v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21847v1",
      "keyInfo": {
        "contributions": [
          "Meta-Black-Box 寻找最佳参数或解决方案的过程（undefined） (MetaBBO) is an emerging avenue within 寻找最佳参数或解决方案的过程（undefined） community, where algorithm design policy could be meta-learned by 通过试错学习最佳策略的机器学习方法（undefined） to enhance 寻找最佳参数或解决方案的过程（undefined） performance",
          "So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking"
        ],
        "methods": [],
        "applications": [
          "Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21831v1",
      "title": "Generative Modeling of Discrete Data Using Geometric Latent Subspaces",
      "originalTitle": "Generative Modeling of Discrete Data Using Geometric Latent Subspaces",
      "summary": "We introduce the use of latent subspaces in the exponential parameter space of product manifolds of categorial distributions, as a tool for learning generative models of discrete data. The low-dimensional 数据的压缩表示空间（undefined） encodes statistical dependencies and removes redundant degrees of freedom among the categorial variables. We equip the parameter domain with a Riemannian geometry such that t...",
      "plainSummary": "We introduce the use of latent subspaces in the exponential parameter space of product manifolds of categorial distributions, as a tool for learning generative models of discrete data. The low-dimensional 数据的压缩表示空间（undefined） encodes statistical dependencies and removes redundant degrees of freedom among the categorial variables. We equip the parameter domain with a Riemannian geometry such that the spaces and distances are related by isometries which enables consistent flow matching. In particular, geodesics become straight lines which makes model training by flow matching effective. 基于实验和观察的（undefined） results demonstrate that reduced latent dimensions suffice to represent data for generative modeling.",
      "oneSentenceSummary": "【stat.ML】Daniel Gonzalez-Alvarado等Generative Modeling of Discrete Data Using Geometric Latent Subspaces，在stat.ML取得新进展。",
      "authors": [
        {
          "original": "Daniel Gonzalez-Alvarado",
          "chinese": null
        },
        {
          "original": "Jonas Cassel",
          "chinese": null
        },
        {
          "original": "Stefania Petra",
          "chinese": null
        },
        {
          "original": "Christoph Schnörr",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:14:15Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2601.21831v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21831v1",
      "keyInfo": {
        "contributions": [
          "We introduce the use of latent subspaces in the exponential parameter space of product manifolds of categorial distributions, as a tool for learning generative models of discrete data",
          "基于实验和观察的（undefined） results demonstrate that reduced latent dimensions suffice to represent data for generative modeling"
        ],
        "methods": [],
        "applications": [
          "We equip the parameter domain with a Riemannian geometry such that the spaces and distances are related by isometries which enables consistent flow matching"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21829v1",
      "title": "GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration",
      "originalTitle": "GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration",
      "summary": "This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccade...",
      "plainSummary": "This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition 信息不确定性的度量（undefined）, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and 用于比较性能的标准数据集或方法（undefined） algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.",
      "oneSentenceSummary": "【cs.RO】Bsher Karbouj等GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Bsher Karbouj",
          "chinese": null
        },
        {
          "original": "Baha Eddin Gaaloul",
          "chinese": null
        },
        {
          "original": "Jorg Kruger",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:12:58Z",
      "categories": [
        "cs.RO"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21829v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21829v1",
      "keyInfo": {
        "contributions": [
          "These data can be used to develop and 用于比较性能的标准数据集或方法（undefined） algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers"
        ],
        "methods": [],
        "applications": [
          "These data can be used to develop and 用于比较性能的标准数据集或方法（undefined） algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21823v1",
      "title": "General Self-Prediction Enhancement for Spiking Neurons",
      "originalTitle": "General Self-Prediction Enhancement for Spiking Neurons",
      "summary": "Spiking Neural Networks (SNNs) are highly energy-速度快、资源消耗少（undefined） due to event-driven, sparse computation, but their training is challenged by spike non-differentiability and trade-offs among performance, efficiency, and biological plausibility. Crucially, mainstream SNNs ignore predictive coding, a core cortical mechanism where the brain predicts inputs and encodes errors for 速度快、资源消耗少（undefi...",
      "plainSummary": "Spiking Neural Networks (SNNs) are highly energy-速度快、资源消耗少（undefined） due to event-driven, sparse computation, but their training is challenged by spike non-differentiability and trade-offs among performance, efficiency, and biological plausibility. Crucially, mainstream SNNs ignore predictive coding, a core cortical mechanism where the brain predicts inputs and encodes errors for 速度快、资源消耗少（undefined） perception. Inspired by this, 我们提出 a self-prediction enhanced spiking neuron method that generates an internal prediction current from its input-output history to modulate membrane potential. This design offers dual advantages, it creates a continuous gradient path that alleviates vanishing gradients and boosts training stability and 正确预测占总预测的比例（undefined）, while also aligning with biological principles, which resembles distal dendritic modulation and error-driven synaptic plasticity. Experiments show consistent performance gains across diverse architectures, neuron types, time steps, and tasks demonstrating broad applicability for enhancing SNNs.",
      "oneSentenceSummary": "【cs.NE】Zihan Huang等General Self-Prediction Enhancement for Spiking Neurons，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Zihan Huang",
          "chinese": null
        },
        {
          "original": "Zijie Xu",
          "chinese": null
        },
        {
          "original": "Yihan Huang",
          "chinese": null
        },
        {
          "original": "Shanshan Jia",
          "chinese": null
        },
        {
          "original": "Tong Bu",
          "chinese": null
        },
        {
          "original": "Yiting Dong",
          "chinese": null
        },
        {
          "original": "Wenxuan Liu",
          "chinese": null
        },
        {
          "original": "Jianhao Ding",
          "chinese": null
        },
        {
          "original": "Zhaofei Yu",
          "chinese": null
        },
        {
          "original": "Tiejun Huang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:08:48Z",
      "categories": [
        "cs.NE"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.21823v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21823v1",
      "keyInfo": {
        "contributions": [
          "Inspired by this, we propose a self-prediction enhanced spiking neuron method that generates an internal prediction current from its input-output history to modulate membrane potential",
          "This design offers dual advantages, it creates a continuous gradient path that alleviates vanishing gradients and boosts training stability and 正确预测占总预测的比例（undefined）, while also aligning with biological principles, which resembles distal dendritic modulation and error-driven synaptic plasticity"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21817v1",
      "title": "A Judge-Aware Ranking 提供结构的基础代码库（undefined） for Evaluating Large Language Models without Ground Truth",
      "originalTitle": "A Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth",
      "summary": "Evaluating large language models (LLMs) on open-ended tasks without ground-truth labels is increasingly done via the LLM-as-a-judge paradigm. A critical but under-modeled issue is that judge LLMs differ substantially in reliability; treating all judges equally can yield biased leaderboards and misleading uncertainty estimates. More data can make evaluation more confidently wrong under misspecified...",
      "plainSummary": "Evaluating large language models (LLMs) on open-ended tasks without ground-truth labels is increasingly done via the LLM-as-a-judge paradigm. A critical but under-modeled issue is that judge LLMs differ substantially in reliability; treating all judges equally can yield biased leaderboards and misleading uncertainty estimates. More data can make evaluation more confidently wrong under misspecified aggregation. 我们提出 a judge-aware ranking 提供结构的基础代码库（undefined） that extends the Bradley-Terry-Luce model by introducing judge-specific discrimination parameters, jointly estimating latent model quality and judge reliability from pairwise comparisons without reference labels. We establish identifiability up to natural normalizations and prove consistency and asymptotic normality of the maximum 给定参数下观察到数据的概率（undefined） estimator, enabling confidence intervals for score differences and rank comparisons. Across multiple public benchmarks and a newly collected dataset, 我们的方法 improves agreement with human preferences, achieves higher data efficiency than unweighted baselines, and produces calibrated uncertainty quantification for LLM rankings.",
      "oneSentenceSummary": "【stat.ML】Mingyuan Xu等A Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth，在stat.ML取得新进展。",
      "authors": [
        {
          "original": "Mingyuan Xu",
          "chinese": null
        },
        {
          "original": "Xinzi Tan",
          "chinese": null
        },
        {
          "original": "Jiawei Wu",
          "chinese": null
        },
        {
          "original": "Doudou Zhou",
          "chinese": null
        }
      ],
      "published": "2026-01-29T15:01:28Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2601.21817v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21817v1",
      "keyInfo": {
        "contributions": [
          "We propose a judge-aware ranking 提供结构的基础代码库（undefined） that extends the Bradley-Terry-Luce model by introducing judge-specific discrimination parameters, jointly estimating latent model quality and judge reliability from pairwise comparisons without reference labels"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21812v1",
      "title": "A Decomposable Forward Process in Diffusion Models for Time-Series Forecasting",
      "originalTitle": "A Decomposable Forward Process in Diffusion Models for Time-Series Forecasting",
      "summary": "We introduce a model-agnostic forward diffusion process for time-series forecasting that decomposes signals into spectral components, preserving structured temporal patterns such as seasonality more effectively than standard diffusion. Unlike 观察到数据前的概率（undefined） work that modifies the network architecture or diffuses directly in the frequency domain, our proposed method alters only the diffusion ...",
      "plainSummary": "We introduce a model-agnostic forward diffusion process for time-series forecasting that decomposes signals into spectral components, preserving structured temporal patterns such as seasonality more effectively than standard diffusion. Unlike 观察到数据前的概率（undefined） work that modifies the network architecture or diffuses directly in the frequency domain, our proposed method alters only the diffusion process itself, making it compatible with existing diffusion backbones (e.g., DiffWave, TimeGrad, CSDI). By staging noise injection according to component energy, it maintains high signal-to-noise ratios for dominant frequencies throughout the diffusion trajectory, thereby improving the recoverability of long-term patterns. This strategy enables the model to maintain the signal structure for a longer period in the forward process, leading to improved forecast quality. Across standard forecasting benchmarks, we show that applying spectral decomposition strategies, such as the Fourier or Wavelet transform, consistently improves upon diffusion models using the 用于对比的基准方法（undefined） forward process, with negligible computational overhead. The code for this paper is available at https://anonymous.4open.science/r/D-FDP-4A29.",
      "oneSentenceSummary": "【stat.ML】Francisco Caldas等A Decomposable Forward Process in Diffusion Models for Time-Series Forecasting，使用Across standard forecasting be...，在stat.ML取得新进展。",
      "authors": [
        {
          "original": "Francisco Caldas",
          "chinese": null
        },
        {
          "original": "Sahil Kumar",
          "chinese": null
        },
        {
          "original": "Cláudia Soares",
          "chinese": null
        }
      ],
      "published": "2026-01-29T14:55:43Z",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2601.21812v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21812v1",
      "keyInfo": {
        "contributions": [
          "We introduce a model-agnostic forward diffusion process for time-series forecasting that decomposes signals into spectral components, preserving structured temporal patterns such as seasonality more effectively than standard diffusion",
          "Unlike 观察到数据前的概率（undefined） work that modifies the network architecture or diffuses directly in the frequency domain, our proposed method alters only the diffusion process itself, making it compatible with existing diffusion backbones (e"
        ],
        "methods": [
          "Across standard forecasting benchmarks, we show that applying spectral decomposition strategies, such as the Fourier or Wavelet transform, consistently improves upon diffusion models using the 用于对比的基准方法（undefined） forward process, with negligible computational overhead"
        ],
        "applications": [
          "This strategy enables the model to maintain the signal structure for a longer period in the forward process, leading to improved forecast quality"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21789v1",
      "title": "ECSEL: Explainable Classification via Signomial Equation Learning",
      "originalTitle": "ECSEL: Explainable Classification via Signomial Equation Learning",
      "summary": "We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure. ECSEL directly constructs a structural, closed-form expression that serves as both a classifier and an explanation. On standard symbolic regression benchmarks, our method ...",
      "plainSummary": "We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure. ECSEL directly constructs a structural, closed-form expression that serves as both a classifier and an explanation. On standard symbolic regression benchmarks, 我们的方法 recovers a larger fraction of target equations than competing 当前最好的、领先的方法（undefined） approaches while requiring substantially less computation. Leveraging this efficiency, ECSEL achieves classification 正确预测占总预测的比例（undefined） competitive with established 让计算机通过数据自动学习和改进的技术（undefined） models without sacrificing interpretability. Further, we show that ECSEL satisfies some desirable properties regarding global feature behavior, decision-boundary analysis, and local feature attributions. Experiments on 用于比较性能的标准数据集或方法（undefined） datasets and two real-world case studies i.e., e-commerce and fraud detection, demonstrate that the learned equations expose dataset biases, support counterfactual reasoning, and yield actionable insights.",
      "oneSentenceSummary": "【cs.LG】Adia Lumadjeng等ECSEL，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Adia Lumadjeng",
          "chinese": null
        },
        {
          "original": "Ilker Birbil",
          "chinese": null
        },
        {
          "original": "Erman Acar",
          "chinese": null
        }
      ],
      "published": "2026-01-29T14:35:43Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.21789v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21789v1",
      "keyInfo": {
        "contributions": [
          "We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21778v1",
      "title": "Error Amplification Limits ANN-to-SNN Conversion in Continuous Control",
      "originalTitle": "Error Amplification Limits ANN-to-SNN Conversion in Continuous Control",
      "summary": "Spiking Neural Networks (SNNs) can achieve competitive performance by converting already existing well-trained Artificial Neural Networks (ANNs), avoiding further costly training. This property is particularly attractive in 通过试错学习最佳策略的机器学习方法（undefined） (RL), where training through environment interaction is expensive and potentially unsafe. However, existing conversion methods perform poorly in co...",
      "plainSummary": "Spiking Neural Networks (SNNs) can achieve competitive performance by converting already existing well-trained Artificial Neural Networks (ANNs), avoiding further costly training. This property is particularly attractive in 通过试错学习最佳策略的机器学习方法（undefined） (RL), where training through environment interaction is expensive and potentially unsafe. However, existing conversion methods perform poorly in continuous control, where suitable baselines are largely absent. We identify error amplification as the key cause: small action approximation errors become temporally correlated across decision steps, inducing cumulative state distribution shift and severe performance degradation. To address this issue, 我们提出 Cross-Step Residual Potential Initialization (CRPI), a lightweight training-free mechanism that carries over residual membrane potentials across decision steps to suppress temporally correlated errors. Experiments on continuous control benchmarks with both vector and visual observations demonstrate that CRPI can be integrated into existing conversion pipelines and substantially recovers lost performance. Our results highlight continuous control as a critical and challenging 用于比较性能的标准数据集或方法（undefined） for ANN-to-SNN conversion, where small errors can be strongly amplified and impact performance.",
      "oneSentenceSummary": "【cs.NE】Zijie Xu等Error Amplification Limits ANN-to-SNN Conversion in Continuous Control，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Zijie Xu",
          "chinese": null
        },
        {
          "original": "Zihan Huang",
          "chinese": null
        },
        {
          "original": "Yiting Dong",
          "chinese": null
        },
        {
          "original": "Kang Chen",
          "chinese": null
        },
        {
          "original": "Wenxuan Liu",
          "chinese": null
        },
        {
          "original": "Zhaofei Yu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T14:28:00Z",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.21778v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21778v1",
      "keyInfo": {
        "contributions": [
          "To address this issue, we propose Cross-Step Residual Potential Initialization (CRPI), a lightweight training-free mechanism that carries over residual membrane potentials across decision steps to suppress temporally correlated errors"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21772v1",
      "title": "Flocking behavior for dynamic and complex swarm structures",
      "originalTitle": "Flocking behavior for dynamic and complex swarm structures",
      "summary": "Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a 基于数学推导的（undefined） 提供结构的基础代码库（und...",
      "plainSummary": "Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a 基于数学推导的（undefined） 提供结构的基础代码库（undefined） for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.",
      "oneSentenceSummary": "【cs.RO】Carmen D. R. Pita-Romero等Flocking behavior for dynamic and complex swarm structures，使用This work presents an algorith...，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Carmen D. R. Pita-Romero",
          "chinese": null
        },
        {
          "original": "Pedro Arias-Perez",
          "chinese": null
        },
        {
          "original": "Miguel Fernandez-Cortizas",
          "chinese": null
        },
        {
          "original": "Rafael Perez-Segui",
          "chinese": null
        },
        {
          "original": "Pascual Campoy",
          "chinese": null
        }
      ],
      "published": "2026-01-29T14:22:53Z",
      "categories": [
        "cs.RO"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21772v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21772v1",
      "keyInfo": {
        "contributions": [
          "This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock"
        ],
        "methods": [
          "This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21765v1",
      "title": "Mean-field Variational Bayes for Sparse Probit Regression",
      "originalTitle": "Mean-field Variational Bayes for Sparse Probit Regression",
      "summary": "We consider Bayesian variable selection for binary outcomes under a probit link with a spike-and-slab 观察到数据前的概率（undefined） on the regression coefficients. Motivated by the computational challenges encountered by Markov chain Monte Carlo (MCMC) samplers in high-dimensional regimes, we develop a mean-field variational Bayes approximation in which all variational factors admit closed-form updates, an...",
      "plainSummary": "We consider Bayesian variable selection for binary outcomes under a probit link with a spike-and-slab 观察到数据前的概率（undefined） on the regression coefficients. Motivated by the computational challenges encountered by Markov chain Monte Carlo (MCMC) samplers in high-dimensional regimes, we develop a mean-field variational Bayes approximation in which all variational factors admit closed-form updates, and the evidence lower bound is available in closed form. This, in turn, allows the development of an 速度快、资源消耗少（undefined） coordinate ascent variational inference algorithm to find the optimal values of the variational parameters. The approach produces 观察到数据后的概率（undefined） inclusion probabilities and parameter estimates, enabling 能够解释其决策过程（undefined） selection and prediction within a single 提供结构的基础代码库（undefined）. As shown in both simulated and real data applications, the proposed method successfully identifies the important variables and is orders of magnitude faster than MCMC, while maintaining comparable 正确预测占总预测的比例（undefined）.",
      "oneSentenceSummary": "【stat.CO】Augusto Fasano等Mean-field Variational Bayes for Sparse Probit Regression，在stat.CO取得新进展。",
      "authors": [
        {
          "original": "Augusto Fasano",
          "chinese": null
        },
        {
          "original": "Giovanni Rebaudo",
          "chinese": null
        }
      ],
      "published": "2026-01-29T14:16:31Z",
      "categories": [
        "stat.CO",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.CO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21765v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21765v1",
      "keyInfo": {
        "contributions": [
          "Motivated by the computational challenges encountered by Markov chain Monte Carlo (MCMC) samplers in high-dimensional regimes, we develop a mean-field variational Bayes approximation in which all variational factors admit closed-form updates, and the evidence lower bound is available in closed form",
          "This, in turn, allows the development of an 速度快、资源消耗少（undefined） coordinate ascent variational inference algorithm to find the optimal values of the variational parameters"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21739v1",
      "title": "Why Adam Works Better with $β_1 = β_2$: The Missing Gradient Scale Invariance Principle",
      "originalTitle": "Why Adam Works Better with $β_1 = β_2$: The Missing Gradient Scale Invariance Principle",
      "summary": "Adam has been at the core of large-scale training for almost a decade, yet a simple 基于实验和观察的（undefined） fact remains unaccounted for: both validation scores and the qualitative behaviour of the training runs improve when the momentum parameters satisfy . Some recent studies have reported this pattern, but there is still no explanation for why this choice helps. We show that this choice is closely ...",
      "plainSummary": "Adam has been at the core of large-scale training for almost a decade, yet a simple 基于实验和观察的（undefined） fact remains unaccounted for: both validation scores and the qualitative behaviour of the training runs improve when the momentum parameters satisfy . Some recent studies have reported this pattern, but there is still no explanation for why this choice helps. We show that this choice is closely tied to a structural property that we refer to as \\textit{gradient scale invariance}. We formalize this notion and prove that Adam becomes gradient scale invariant of first order if and only if . This perspective places the balanced regime of Adam in direct alignment with the design principles underlying several recent optimizers that explicitly enforce scale-对噪声和扰动不敏感（undefined） updates. The theory is supported by experiments across vision and language tasks, and across different architectural families, in which rescaling the gradient has a markedly smoother effect on the update when . Overall, our results offer a coherent explanation for an open question in the behavior of Adam and provide a simple principle that helps guide the design of future optimizers.",
      "oneSentenceSummary": "【cs.LG】Alberto Fernández-Hernández等Why Adam Works Better with $β_1 = β_2$，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Alberto Fernández-Hernández",
          "chinese": null
        },
        {
          "original": "Cristian Pérez-Corral",
          "chinese": null
        },
        {
          "original": "Jose I. Mestre",
          "chinese": null
        },
        {
          "original": "Manuel F. Dolz",
          "chinese": null
        },
        {
          "original": "Enrique S. Quintana-Ortí",
          "chinese": null
        }
      ],
      "published": "2026-01-29T13:56:11Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.21739v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21739v1",
      "keyInfo": {
        "contributions": [
          "This perspective places the balanced regime of Adam in direct alignment with the design principles underlying several recent optimizers that explicitly enforce scale-对噪声和扰动不敏感（undefined） updates",
          "Overall, our results offer a coherent explanation for an open question in the behavior of Adam and provide a simple principle that helps guide the design of future optimizers"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21719v1",
      "title": "LoRA and Privacy: When Random Projections Help (and When They Don't)",
      "originalTitle": "LoRA and Privacy: When Random Projections Help (and When They Don't)",
      "summary": "We introduce the (Wishart) projection mechanism, a randomized map of the form with and study its differential privacy properties. For vector-valued queries , we prove non-asymptotic DP guarantees without any additive noise, showing that Wishart randomness alone can suffice. For matrix-valued queries, however, we establish a sharp negative result: in the noise-free setting, the mechanism is not DP,...",
      "plainSummary": "We introduce the (Wishart) projection mechanism, a randomized map of the form with and study its differential privacy properties. For vector-valued queries , we prove non-asymptotic DP guarantees without any additive noise, showing that Wishart randomness alone can suffice. For matrix-valued queries, however, we establish a sharp negative result: in the noise-free setting, the mechanism is not DP, and we demonstrate its vulnerability by implementing a near perfect membership inference attack (AUC ). We then analyze a noisy variant and prove privacy amplification due to randomness and low rank projection, in both large- and small-rank regimes, yielding stronger privacy guarantees than additive noise alone. Finally, we show that LoRA-style updates are an instance of the matrix-valued mechanism, implying that LoRA is not inherently private despite its built-in randomness, but that low-rank 在预训练模型基础上进行小幅调整（undefined） can be more private than full 在预训练模型基础上进行小幅调整（undefined） at the same noise level. Preliminary experiments suggest that tighter accounting enables lower noise and improved 正确预测占总预测的比例（undefined） in practice.",
      "oneSentenceSummary": "【cs.LG】Yaxi Hu等LoRA and Privacy，在cs.LG取得新进展。",
      "authors": [
        {
          "original": "Yaxi Hu",
          "chinese": null
        },
        {
          "original": "Johanna Düngler",
          "chinese": null
        },
        {
          "original": "Bernhard Schölkopf",
          "chinese": null
        },
        {
          "original": "Amartya Sanyal",
          "chinese": null
        }
      ],
      "published": "2026-01-29T13:43:37Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2601.21719v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21719v1",
      "keyInfo": {
        "contributions": [
          "We introduce the (Wishart) projection mechanism, a randomized map of the form $S \\mapsto M f(S)$ with $M \\sim W_d(1/r I_d, r)$ and study its differential privacy properties"
        ],
        "methods": [],
        "applications": [
          "Preliminary experiments suggest that tighter accounting enables lower noise and improved 正确预测占总预测的比例（undefined） in practice"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21713v1",
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "originalTitle": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "summary": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide 对噪声和扰动不敏感（undefined） and general manipulation policies...",
      "plainSummary": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide 对噪声和扰动不敏感（undefined） and general manipulation policies, 通过试错学习最佳策略的机器学习方法（undefined） (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of 对噪声和扰动不敏感（undefined） state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world 在预训练模型基础上进行小幅调整（undefined）, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an 速度快、资源消耗少（undefined） and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym 用于比较性能的标准数据集或方法（undefined） and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.",
      "oneSentenceSummary": "【cs.RO】Donatien Delehelle等Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations，使用We evaluate our approach on th...，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Donatien Delehelle",
          "chinese": null
        },
        {
          "original": "Fei Chen",
          "chinese": null
        },
        {
          "original": "Darwin Caldwell",
          "chinese": null
        }
      ],
      "published": "2026-01-29T13:41:35Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21713v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21713v1",
      "keyInfo": {
        "contributions": [
          "The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics",
          "The resulting computational cost significantly hampers the development and adoption of these methods"
        ],
        "methods": [
          "We evaluate our approach on the SoftGym 用于比较性能的标准数据集或方法（undefined） and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model"
        ],
        "applications": [
          "While this approach enables a conceptually straightforward sim-to-real transfer via real-world 在预训练模型基础上进行小幅调整（undefined）, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21712v1",
      "title": "CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation",
      "originalTitle": "CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation",
      "summary": "Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision 给定参数下观察到数据的概率（undefined） from proprioception, visual embeddings, and planned actions...",
      "plainSummary": "Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision 给定参数下观察到数据的概率（undefined） from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.",
      "oneSentenceSummary": "【cs.RO】Xuanran Zhai等CoFreeVLA，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Xuanran Zhai",
          "chinese": null
        },
        {
          "original": "Binkai Ou",
          "chinese": null
        },
        {
          "original": "Yemin Wang",
          "chinese": null
        },
        {
          "original": "Hui Yi Leong",
          "chinese": null
        },
        {
          "original": "Qiaojun Yu",
          "chinese": null
        },
        {
          "original": "Ce Hao",
          "chinese": null
        },
        {
          "original": "Yaohua Liu",
          "chinese": null
        }
      ],
      "published": "2026-01-29T13:40:46Z",
      "categories": [
        "cs.RO"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21712v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21712v1",
      "keyInfo": {
        "contributions": [
          "We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision 给定参数下观察到数据的概率（undefined） from proprioception, visual embeddings, and planned actions"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21667v1",
      "title": "From Instruction to Event: Sound-Triggered Mobile Manipulation",
      "originalTitle": "From Instruction to Event: Sound-Triggered Mobile Manipulation",
      "summary": "Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively ...",
      "plainSummary": "Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a 用于对比的基准方法（undefined） comprising a high-level task planner and low-level policy models to complete these tasks. 大量实验 show that the proposed 用于对比的基准方法（undefined） empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the 用于对比的基准方法（undefined）.",
      "oneSentenceSummary": "【cs.RO】Hao Ju等From Instruction to Event，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Hao Ju",
          "chinese": null
        },
        {
          "original": "Shaofei Huang",
          "chinese": null
        },
        {
          "original": "Hongyu Li",
          "chinese": null
        },
        {
          "original": "Zihan Ding",
          "chinese": null
        },
        {
          "original": "Si Liu",
          "chinese": null
        },
        {
          "original": "Meng Wang",
          "chinese": null
        },
        {
          "original": "Zhedong Zheng",
          "chinese": null
        }
      ],
      "published": "2026-01-29T13:02:10Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21667v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21667v1",
      "keyInfo": {
        "contributions": [
          "To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions",
          "To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21602v1",
      "title": "AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation",
      "originalTitle": "AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation",
      "summary": "While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose s...",
      "plainSummary": "While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, 我们提出 AIR-VLA, the first VLA 用于比较性能的标准数据集或方法（undefined） specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object & spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and 当前最好的、领先的方法（undefined） VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.",
      "oneSentenceSummary": "【cs.RO】Jianli Sun等AIR-VLA，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Jianli Sun",
          "chinese": null
        },
        {
          "original": "Bin Tian",
          "chinese": null
        },
        {
          "original": "Qiyao Zhang",
          "chinese": null
        },
        {
          "original": "Chengxiang Li",
          "chinese": null
        },
        {
          "original": "Zihan Song",
          "chinese": null
        },
        {
          "original": "Zhiyong Cui",
          "chinese": null
        },
        {
          "original": "Yisheng Lv",
          "chinese": null
        },
        {
          "original": "Yonglin Tian",
          "chinese": null
        }
      ],
      "published": "2026-01-29T12:09:00Z",
      "categories": [
        "cs.RO"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21602v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21602v1",
      "keyInfo": {
        "contributions": [
          "The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases",
          "To bridge this gap, we propose AIR-VLA, the first VLA 用于比较性能的标准数据集或方法（undefined） specifically tailored for aerial manipulation"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21570v1",
      "title": "EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots",
      "originalTitle": "EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots",
      "summary": "The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in softwar...",
      "plainSummary": "The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \\textsc{EmboCoach-Bench}, a 用于比较性能的标准数据集或方法（undefined） evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our 提供结构的基础代码库（undefined） posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to 能够处理更大规模数据（undefined）, autonomous engineering in embodied AI field.",
      "oneSentenceSummary": "【cs.AI】Zixing Lei等EmboCoach-Bench，在cs.AI取得新进展。",
      "authors": [
        {
          "original": "Zixing Lei",
          "chinese": null
        },
        {
          "original": "Genjia Liu",
          "chinese": null
        },
        {
          "original": "Yuanshuo Zhang",
          "chinese": null
        },
        {
          "original": "Qipeng Liu",
          "chinese": null
        },
        {
          "original": "Chuan Wen",
          "chinese": null
        },
        {
          "original": "Shanghang Zhang",
          "chinese": null
        },
        {
          "original": "Wenzhao Lian",
          "chinese": null
        },
        {
          "original": "Siheng Chen",
          "chinese": null
        }
      ],
      "published": "2026-01-29T11:33:49Z",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2601.21570v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21570v1",
      "keyInfo": {
        "contributions": [
          "Inspired by LLMs' success in software automation and science discovery, we introduce \\textsc{EmboCoach-Bench}, a 用于比较性能的标准数据集或方法（undefined） evaluating the capacity of LLM agents to autonomously engineer embodied policies",
          "We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21557v1",
      "title": "Meta Context Engineering via Agentic Skill Evolution",
      "originalTitle": "Meta Context Engineering via Agentic Skill Evolution",
      "summary": "The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context 寻找最佳参数或解决方案的过程（undef...",
      "plainSummary": "The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context 寻找最佳参数或解决方案的过程（undefined） to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level 提供结构的基础代码库（undefined） that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over 当前最好的、领先的方法（undefined） agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.",
      "oneSentenceSummary": "【cs.AI】Haoran Ye等Meta Context Engineering via Agentic Skill Evolution，在cs.AI取得新进展。",
      "authors": [
        {
          "original": "Haoran Ye",
          "chinese": null
        },
        {
          "original": "Xuning He",
          "chinese": null
        },
        {
          "original": "Vincent Arak",
          "chinese": null
        },
        {
          "original": "Haonan Dong",
          "chinese": null
        },
        {
          "original": "Guojie Song",
          "chinese": null
        }
      ],
      "published": "2026-01-29T11:22:02Z",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2601.21557v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21557v1",
      "keyInfo": {
        "contributions": [
          "They impose structural biases and restrict context 寻找最佳参数或解决方案的过程（undefined） to a narrow, intuition-bound design space",
          "To address this, we introduce Meta Context Engineering (MCE), a bi-level 提供结构的基础代码库（undefined） that supersedes static CE heuristics by co-evolving CE skills and context artifacts"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21548v1",
      "title": "Training slow silicon neurons to control extremely fast robots with spiking 通过试错学习最佳策略的机器学习方法（undefined）",
      "originalTitle": "Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning",
      "summary": "Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through 通过试错学习最佳策略的机器学习方法（undefined） in a remarkably small number of trials. The network leve...",
      "plainSummary": "Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through 通过试错学习最佳策略的机器学习方法（undefined） in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and 速度快、资源消耗少（undefined） learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.",
      "oneSentenceSummary": "【cs.RO】Irene Ambrosini等Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning，使用The network leverages fixed ra...，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Irene Ambrosini",
          "chinese": null
        },
        {
          "original": "Ingo Blakowski",
          "chinese": null
        },
        {
          "original": "Dmitrii Zendrikov",
          "chinese": null
        },
        {
          "original": "Cristiano Capone",
          "chinese": null
        },
        {
          "original": "Luna Gava",
          "chinese": null
        },
        {
          "original": "Giacomo Indiveri",
          "chinese": null
        },
        {
          "original": "Chiara De Luca",
          "chinese": null
        },
        {
          "original": "Chiara Bartolozzi",
          "chinese": null
        }
      ],
      "published": "2026-01-29T11:05:23Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.ET"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21548v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21548v1",
      "keyInfo": {
        "contributions": [
          "By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through 通过试错学习最佳策略的机器学习方法（undefined） in a remarkably small number of trials"
        ],
        "methods": [
          "The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and 速度快、资源消耗少（undefined） learning"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21511v1",
      "title": "LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI",
      "originalTitle": "LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI",
      "summary": "Large language models have enabled automated algorithm design (AAD) by generating 寻找最佳参数或解决方案的过程（undefined） algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated co...",
      "plainSummary": "Large language models have enabled automated algorithm design (AAD) by generating 寻找最佳参数或解决方案的过程（undefined） algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused. 我们提出 a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions. Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity. 我们提出 LLaMEA-SAGE, which integrates this feature-driven guidance into LLaMEA, and evaluate it across several benchmarks. We show that the proposed structured guidance achieves the same performance faster than vanilla LLaMEA in a small controlled experiment. In a larger-scale experiment using the MA-BBOB suite from the GECCO-MA-BBOB competition, our guided approach achieves superior performance compared to 当前最好的、领先的方法（undefined） AAD methods. These results demonstrate that signals derived from code can effectively bias LLM-driven algorithm evolution, bridging the gap between code structure and human-understandable performance feedback in automated algorithm design.",
      "oneSentenceSummary": "【cs.AI】Niki van Stein等LLaMEA-SAGE，使用We propose a mechanism for gui...，在cs.AI取得新进展。",
      "authors": [
        {
          "original": "Niki van Stein",
          "chinese": null
        },
        {
          "original": "Anna V. Kononova",
          "chinese": null
        },
        {
          "original": "Lars Kotthoff",
          "chinese": null
        },
        {
          "original": "Thomas Bäck",
          "chinese": null
        }
      ],
      "published": "2026-01-29T10:27:29Z",
      "categories": [
        "cs.AI",
        "cs.NE",
        "cs.SE"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2601.21511v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21511v1",
      "keyInfo": {
        "contributions": [
          "Large language models have enabled automated algorithm design (AAD) by generating 寻找最佳参数或解决方案的过程（undefined） algorithms directly from natural-language prompts",
          "While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused"
        ],
        "methods": [
          "We propose a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions",
          "Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21506v1",
      "title": "IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation",
      "originalTitle": "IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation",
      "summary": "Indoor mobile robot navigation requires fast responsiveness and 对噪声和扰动不敏感（undefined） semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introdu...",
      "plainSummary": "Indoor mobile robot navigation requires fast responsiveness and 对噪声和扰动不敏感（undefined） semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation 提供结构的基础代码库（undefined） that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers 对噪声和扰动不敏感（undefined）, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision 正确预测占总预测的比例（undefined） and reduces latency by 66% compared to continuous VLM-based navigation.",
      "oneSentenceSummary": "【cs.RO】Joonhee Lee等IROS，在cs.RO取得新进展。",
      "authors": [
        {
          "original": "Joonhee Lee",
          "chinese": null
        },
        {
          "original": "Hyunseung Shin",
          "chinese": null
        },
        {
          "original": "Jeonggil Ko",
          "chinese": null
        }
      ],
      "published": "2026-01-29T10:25:14Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primaryCategory": "cs.RO",
      "pdfUrl": "https://arxiv.org/pdf/2601.21506v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21506v1",
      "keyInfo": {
        "contributions": [
          "Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues",
          "In this work, we present IROS, a real-time navigation 提供结构的基础代码库（undefined） that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware"
        ],
        "methods": [],
        "applications": [
          "Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21503v1",
      "title": "MAR: 速度快、资源消耗少（undefined） Large Language Models via Module-aware Architecture Refinement",
      "originalTitle": "MAR: Efficient Large Language Models via Module-aware Architecture Refinement",
      "summary": "Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage 提供结构的基础代码库（undefined） that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to r...",
      "plainSummary": "Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, 我们提出 Module-aware Architecture Refinement (MAR), a two-stage 提供结构的基础代码库（undefined） that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). 大量实验 demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms 速度快、资源消耗少（undefined） models of comparable or even larger scale, underscoring its potential for building 速度快、资源消耗少（undefined） and practical LLMs.",
      "oneSentenceSummary": "【cs.AI】Junhong Cai等MAR，使用To address these issues, we pr...，在cs.AI取得新进展。",
      "authors": [
        {
          "original": "Junhong Cai",
          "chinese": null
        },
        {
          "original": "Guiqin Wang",
          "chinese": null
        },
        {
          "original": "Kejie Zhao",
          "chinese": null
        },
        {
          "original": "Jianxiong Tang",
          "chinese": null
        },
        {
          "original": "Xiang Wang",
          "chinese": null
        },
        {
          "original": "Luziwei Leng",
          "chinese": null
        },
        {
          "original": "Ran Cheng",
          "chinese": null
        },
        {
          "original": "Yuxin Ma",
          "chinese": null
        },
        {
          "original": "Qinghai Guo",
          "chinese": null
        }
      ],
      "published": "2026-01-29T10:21:28Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.NE"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2601.21503v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21503v1",
      "keyInfo": {
        "contributions": [
          "To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage 提供结构的基础代码库（undefined） that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs",
          "In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS)"
        ],
        "methods": [
          "To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage 提供结构的基础代码库（undefined） that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs"
        ],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21475v1",
      "title": "Task-free Adaptive Meta Black-box 寻找最佳参数或解决方案的过程（undefined）",
      "originalTitle": "Task-free Adaptive Meta Black-box Optimization",
      "summary": "Handcrafted optimizers become prohibitively inefficient for complex black-box 寻找最佳参数或解决方案的过程（undefined） (BBO) tasks. MetaBBO addresses this challenge by meta-learning to automatically configure optimizers for low-level BBO tasks, thereby eliminating heuristic dependencies. However, existing methods typically require extensive handcrafted training tasks to learn meta-strategies that generalize to t...",
      "plainSummary": "Handcrafted optimizers become prohibitively inefficient for complex black-box 寻找最佳参数或解决方案的过程（undefined） (BBO) tasks. MetaBBO addresses this challenge by meta-learning to automatically configure optimizers for low-level BBO tasks, thereby eliminating heuristic dependencies. However, existing methods typically require extensive handcrafted training tasks to learn meta-strategies that generalize to target tasks, which poses a critical limitation for realistic applications with unknown task distributions. To overcome the issue, 我们提出 the Adaptive meta Black-box 寻找最佳参数或解决方案的过程（undefined） Model (ABOM), which performs online parameter adaptation using solely 寻找最佳参数或解决方案的过程（undefined） data from the target task, obviating the need for predefined task distributions. Unlike conventional metaBBO frameworks that decouple meta-training and 寻找最佳参数或解决方案的过程（undefined） phases, ABOM introduces a closed-loop adaptive parameter learning mechanism, where parameterized evolutionary operators continuously self-update by leveraging generated populations during 寻找最佳参数或解决方案的过程（undefined）. This paradigm shift enables zero-shot 寻找最佳参数或解决方案的过程（undefined）: ABOM achieves competitive performance on synthetic BBO benchmarks and realistic unmanned aerial vehicle path planning problems without any handcrafted training tasks. Visualization studies reveal that parameterized evolutionary operators exhibit statistically significant search patterns, including natural selection and genetic recombination.",
      "oneSentenceSummary": "【cs.NE】Chao Wang等Task-free Adaptive Meta Black-box Optimization，使用To overcome the issue, we prop...，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Chao Wang",
          "chinese": null
        },
        {
          "original": "Licheng Jiao",
          "chinese": null
        },
        {
          "original": "Lingling Li",
          "chinese": null
        },
        {
          "original": "Jiaxuan Zhao",
          "chinese": null
        },
        {
          "original": "Guanchun Wang",
          "chinese": null
        },
        {
          "original": "Fang Liu",
          "chinese": null
        },
        {
          "original": "Shuyuan Yang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T09:54:10Z",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.21475v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21475v1",
      "keyInfo": {
        "contributions": [
          "To overcome the issue, we propose the Adaptive meta Black-box 寻找最佳参数或解决方案的过程（undefined） Model (ABOM), which performs online parameter adaptation using solely 寻找最佳参数或解决方案的过程（undefined） data from the target task, obviating the need for predefined task distributions",
          "Unlike conventional metaBBO frameworks that decouple meta-training and 寻找最佳参数或解决方案的过程（undefined） phases, ABOM introduces a closed-loop adaptive parameter learning mechanism, where parameterized evolutionary operators continuously self-update by leveraging generated populations during 寻找最佳参数或解决方案的过程（undefined）"
        ],
        "methods": [
          "To overcome the issue, we propose the Adaptive meta Black-box 寻找最佳参数或解决方案的过程（undefined） Model (ABOM), which performs online parameter adaptation using solely 寻找最佳参数或解决方案的过程（undefined） data from the target task, obviating the need for predefined task distributions"
        ],
        "applications": [
          "This paradigm shift enables zero-shot 寻找最佳参数或解决方案的过程（undefined）: ABOM achieves competitive performance on synthetic BBO benchmarks and realistic unmanned aerial vehicle path planning problems without any handcrafted training tasks"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21407v1",
      "title": "BrainFuse: a unified infrastructure integrating realistic biological modeling and core AI methodology",
      "originalTitle": "BrainFuse: a unified infrastructure integrating realistic biological modeling and core AI methodology",
      "summary": "Neuroscience and 让机器模拟人类智能的技术（undefined） represent distinct yet complementary pathways to general intelligence. However, amid the ongoing boom in AI research and applications, the translational synergy between these two fields has grown increasingly elusive-hampered by a widening infrastructural incompatibility: modern AI frameworks lack native support for biophysical realism, while neural simulat...",
      "plainSummary": "Neuroscience and 让机器模拟人类智能的技术（undefined） represent distinct yet complementary pathways to general intelligence. However, amid the ongoing boom in AI research and applications, the translational synergy between these two fields has grown increasingly elusive-hampered by a widening infrastructural incompatibility: modern AI frameworks lack native support for biophysical realism, while neural simulation tools are poorly suited for gradient-based 寻找最佳参数或解决方案的过程（undefined） and neuromorphic hardware deployment. To bridge this gap, we introduce BrainFuse, a unified infrastructure that provides 覆盖广泛的、详细的（undefined） support for biophysical neural simulation and gradient-based learning. By addressing algorithmic, computational, and deployment challenges, BrainFuse exhibits three core capabilities: (1) algorithmic integration of detailed neuronal dynamics into a differentiable learning 提供结构的基础代码库（undefined）; (2) system-level 寻找最佳参数或解决方案的过程（undefined） that accelerates customizable ion-channel dynamics by up to 3,000x on GPUs; and (3) 能够处理更大规模数据（undefined） computation with highly compatible pipelines for neuromorphic hardware deployment. We demonstrate this full-stack design through both AI and neuroscience tasks, from foundational neuron simulation and functional cylinder modeling to real-world deployment and application scenarios. For neuroscience, BrainFuse supports multiscale biological modeling, enabling the deployment of approximately 38,000 Hodgkin-Huxley neurons with 100 million synapses on a single neuromorphic chip while consuming as low as 1.98 W. For AI, BrainFuse facilitates the synergistic application of realistic biological neuron models, demonstrating enhanced robustness to input noise and improved temporal processing endowed by complex HH dynamics. BrainFuse therefore serves as a foundational engine to facilitate cross-disciplinary research and accelerate the development of next-generation bio-inspired intelligent systems.",
      "oneSentenceSummary": "【cs.NE】Baiyu Chen等BrainFuse，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Baiyu Chen",
          "chinese": null
        },
        {
          "original": "Yujie Wu",
          "chinese": null
        },
        {
          "original": "Siyuan Xu",
          "chinese": null
        },
        {
          "original": "Peng Qu",
          "chinese": null
        },
        {
          "original": "Dehua Wu",
          "chinese": null
        },
        {
          "original": "Xu Chu",
          "chinese": null
        },
        {
          "original": "Haodong Bian",
          "chinese": null
        },
        {
          "original": "Shuo Zhang",
          "chinese": null
        },
        {
          "original": "Bo Xu",
          "chinese": null
        },
        {
          "original": "Youhui Zhang",
          "chinese": null
        },
        {
          "original": "Zhengyu Ma",
          "chinese": null
        },
        {
          "original": "Guoqi Li",
          "chinese": null
        }
      ],
      "published": "2026-01-29T08:44:01Z",
      "categories": [
        "cs.NE",
        "q-bio.NC"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.21407v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21407v1",
      "keyInfo": {
        "contributions": [
          "Neuroscience and 让机器模拟人类智能的技术（undefined） represent distinct yet complementary pathways to general intelligence",
          "To bridge this gap, we introduce BrainFuse, a unified infrastructure that provides 覆盖广泛的、详细的（undefined） support for biophysical neural simulation and gradient-based learning"
        ],
        "methods": [],
        "applications": [
          "For AI, BrainFuse facilitates the synergistic application of realistic biological neuron models, demonstrating enhanced robustness to input noise and improved temporal processing endowed by complex HH dynamics"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21279v1",
      "title": "NEXUS: Bit-Exact ANN-to-SNN Equivalence via Neuromorphic Gate Circuits with Surrogate-Free Training",
      "originalTitle": "NEXUS: Bit-Exact ANN-to-SNN Equivalence via Neuromorphic Gate Circuits with Surrogate-Free Training",
      "summary": "Spiking Neural Networks (SNNs) promise energy-速度快、资源消耗少（undefined） computing through event-driven sparsity, yet all existing approaches sacrifice 正确预测占总预测的比例（undefined） by approximating continuous values with discrete spikes. We propose NEXUS, a 提供结构的基础代码库（undefined） that achieves bit-exact ANN-to-SNN equivalence -- not approximate, but mathematically identical outputs. Our key insight is construc...",
      "plainSummary": "Spiking Neural Networks (SNNs) promise energy-速度快、资源消耗少（undefined） computing through event-driven sparsity, yet all existing approaches sacrifice 正确预测占总预测的比例（undefined） by approximating continuous values with discrete spikes. 我们提出 NEXUS, a 提供结构的基础代码库（undefined） that achieves bit-exact ANN-to-SNN equivalence -- not approximate, but mathematically identical outputs. Our key insight is constructing all arithmetic operations, both linear and nonlinear, from pure IF neuron logic gates that implement IEEE-754 compliant floating-point arithmetic. Through spatial bit encoding (zero encoding error by construction), hierarchical neuromorphic gate circuits (from basic logic gates to complete 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined） layers), and surrogate-free STE training (exact identity mapping rather than heuristic approximation), NEXUS produces outputs identical to standard ANNs up to machine 预测为正例中真正正例的比例（undefined）. Experiments on models up to LLaMA-2 70B demonstrate identical task 正确预测占总预测的比例（undefined） (0.00\\% degradation) with mean ULP error of only 6.19, while achieving 27-168,000 energy reduction on neuromorphic hardware. Crucially, spatial bit encoding's single-timestep design renders the 提供结构的基础代码库（undefined） inherently immune to membrane potential leakage (100\\% 正确预测占总预测的比例（undefined） across all decay factors ), while tolerating synaptic noise up to with >98\\% gate-level 正确预测占总预测的比例（undefined）.",
      "oneSentenceSummary": "【cs.NE】Zhengzheng Tang等NEXUS，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Zhengzheng Tang",
          "chinese": null
        }
      ],
      "published": "2026-01-29T05:23:56Z",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.21279v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21279v1",
      "keyInfo": {
        "contributions": [
          "We propose NEXUS, a 提供结构的基础代码库（undefined） that achieves bit-exact ANN-to-SNN equivalence -- not approximate, but mathematically identical outputs",
          "Crucially, spatial bit encoding's single-timestep design renders the 提供结构的基础代码库（undefined） inherently immune to membrane potential leakage (100\\% 正确预测占总预测的比例（undefined） across all decay factors $β\\in[0"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.21268v1",
      "title": "通过试错学习最佳策略的机器学习方法（undefined） from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels",
      "originalTitle": "Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels",
      "summary": "Most 通过试错学习最佳策略的机器学习方法（undefined） (RL) methods for training large language models (LLMs) require ground-truth labels or task-specific verifiers, limiting scalability when correctness is ambiguous or expensive to obtain. We introduce 通过试错学习最佳策略的机器学习方法（undefined） from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator's answers to natural-language meta-questio...",
      "plainSummary": "Most 通过试错学习最佳策略的机器学习方法（undefined） (RL) methods for training large language models (LLMs) require ground-truth labels or task-specific verifiers, limiting scalability when correctness is ambiguous or expensive to obtain. We introduce 通过试错学习最佳策略的机器学习方法（undefined） from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator's answers to natural-language meta-questions (e.g., \"Is the answer correct?\" or \"Is the reasoning logically consistent?\"). RLME treats the evaluator's probability of a positive judgment as a reward and updates the generator via group-relative policy 寻找最佳参数或解决方案的过程（undefined）, enabling learning without labels. Across a suite of experiments, we show that RLME achieves 正确预测占总预测的比例（undefined） and sample efficiency comparable to label-based training, enables controllable trade-offs among multiple objectives, steers models toward reliable reasoning patterns rather than post-hoc rationalization, and generalizes to open-domain settings where ground-truth labels are unavailable, broadening the domains in which LLMs may be trained with RL.",
      "oneSentenceSummary": "【cs.NE】Micah Rentschler等Reinforcement Learning from Meta-Evaluation，使用We introduce 通过试错学习最佳策略的机器学习方法...，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Micah Rentschler",
          "chinese": null
        },
        {
          "original": "Jesse Roberts",
          "chinese": null
        }
      ],
      "published": "2026-01-29T05:02:08Z",
      "categories": [
        "cs.NE"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.21268v1",
      "abstractUrl": "https://arxiv.org/abs/2601.21268v1",
      "keyInfo": {
        "contributions": [
          "We introduce 通过试错学习最佳策略的机器学习方法（undefined） from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator's answers to natural-language meta-questions (e"
        ],
        "methods": [
          "We introduce 通过试错学习最佳策略的机器学习方法（undefined） from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator's answers to natural-language meta-questions (e"
        ],
        "applications": [
          "Across a suite of experiments, we show that RLME achieves 正确预测占总预测的比例（undefined） and sample efficiency comparable to label-based training, enables controllable trade-offs among multiple objectives, steers models toward reliable reasoning patterns rather than post-hoc rationalization, and generalizes to open-domain settings where ground-truth labels are unavailable, broadening the domains in which LLMs may be trained with RL"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.20981v1",
      "title": "Diversifying Toxicity Search in Large Language Models Through Speciation",
      "originalTitle": "Diversifying Toxicity Search in Large Language Models Through Speciation",
      "summary": "Evolutionary prompt search is a practical black-box approach for red teaming large language models (LLMs), but existing methods often collapse onto a small family of high-performing prompts, limiting coverage of distinct failure modes. We present a speciated quality-diversity (QD) extension of ToxSearch that maintains multiple high-toxicity prompt niches in parallel rather than optimizing a single...",
      "plainSummary": "Evolutionary prompt search is a practical black-box approach for red teaming large language models (LLMs), but existing methods often collapse onto a small family of high-performing prompts, limiting coverage of distinct failure modes. We present a speciated quality-diversity (QD) extension of ToxSearch that maintains multiple high-toxicity prompt niches in parallel rather than optimizing a single best prompt. ToxSearch-S introduces unsupervised prompt speciation via a search methodology that maintains capacity-limited species with exemplar leaders, a reserve pool for outliers and emerging niches, and species-aware parent selection that trades off within-niche exploitation and cross-niche exploration. ToxSearch-S is found to reach higher peak toxicity ( vs.\\ ) and a extreme heavier tail (top-10 median vs.\\ ) than the 用于对比的基准方法（undefined）, while maintaining comparable performance on moderately toxic prompts. Speciation also yields broader semantic coverage under a topic-as-species analysis (higher effective topic diversity and larger unique topic coverage ). Finally, species formed are well-separated in 将离散数据转换为连续向量表示（undefined） space (mean separation ratio ) and exhibit distinct toxicity distributions, indicating that speciation partitions the adversarial space into behaviorally differentiated niches rather than superficial lexical variants. This suggests our approach uncovers a wider range of attack strategies.",
      "oneSentenceSummary": "【cs.NE】Onkar Shelar等Diversifying Toxicity Search in Large Language Models Through Speciation，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Onkar Shelar",
          "chinese": null
        },
        {
          "original": "Travis Desell",
          "chinese": null
        }
      ],
      "published": "2026-01-28T19:29:54Z",
      "categories": [
        "cs.NE",
        "q-bio.PE"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.20981v1",
      "abstractUrl": "https://arxiv.org/abs/2601.20981v1",
      "keyInfo": {
        "contributions": [
          "We present a speciated quality-diversity (QD) extension of ToxSearch that maintains multiple high-toxicity prompt niches in parallel rather than optimizing a single best prompt",
          "ToxSearch-S introduces unsupervised prompt speciation via a search methodology that maintains capacity-limited species with exemplar leaders, a reserve pool for outliers and emerging niches, and species-aware parent selection that trades off within-niche exploitation and cross-niche exploration"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.19562v1",
      "title": "Tournament Informed Adversarial Quality Diversity",
      "originalTitle": "Tournament Informed Adversarial Quality Diversity",
      "summary": "Quality diversity (QD) is a branch of evolutionary computation that seeks high-quality and behaviorally diverse solutions to a problem. While adversarial problems are common, classical QD cannot be easily applied to them, as both the fitness and the behavior depend on the opposing solutions. Recently, Generational Adversarial MAP-Elites (GAME) has been proposed to coevolve both sides of an adversa...",
      "plainSummary": "Quality diversity (QD) is a branch of evolutionary computation that seeks high-quality and behaviorally diverse solutions to a problem. While adversarial problems are common, classical QD cannot be easily applied to them, as both the fitness and the behavior depend on the opposing solutions. Recently, Generational Adversarial MAP-Elites (GAME) has been proposed to coevolve both sides of an adversarial problem by alternating the execution of a multi-task QD algorithm against previous elites, called tasks. The original algorithm selects new tasks based on a behavioral criterion, which may lead to undesired dynamics due to inter-side dependencies. In addition, comparing sets of solutions cannot be done directly using classical QD measures due to side dependencies. In this paper, we (1) use an inter-variants tournament to compare the sets of solutions, ensuring a fair comparison, with 6 measures of quality and diversity, and (2) propose two tournament-informed task selection methods to promote higher quality and diversity at each generation. We evaluate the variants across three adversarial problems: Pong, a Cat-and-mouse game, and a Pursuers-and-evaders game. We show that the tournament-informed task selection method leads to higher adversarial quality and diversity. We hope that this work will help further advance adversarial quality diversity. Code, videos, and supplementary material are available at https://github.com/Timothee-ANNE/GAME_tournament_informed.",
      "oneSentenceSummary": "【cs.NE】Timothée Anne等Tournament Informed Adversarial Quality Diversity，使用The original algorithm selects...，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Timothée Anne",
          "chinese": null
        },
        {
          "original": "Noah Syrkis",
          "chinese": null
        },
        {
          "original": "Meriem Elhosni",
          "chinese": null
        },
        {
          "original": "Florian Turati",
          "chinese": null
        },
        {
          "original": "Alexandre Manai",
          "chinese": null
        },
        {
          "original": "Franck Legendre",
          "chinese": null
        },
        {
          "original": "Alain Jaquier",
          "chinese": null
        },
        {
          "original": "Sebastian Risi",
          "chinese": null
        }
      ],
      "published": "2026-01-27T12:55:23Z",
      "categories": [
        "cs.NE"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.19562v1",
      "abstractUrl": "https://arxiv.org/abs/2601.19562v1",
      "keyInfo": {
        "contributions": [
          "Recently, Generational Adversarial MAP-Elites (GAME) has been proposed to coevolve both sides of an adversarial problem by alternating the execution of a multi-task QD algorithm against previous elites, called tasks",
          "In this paper, we (1) use an inter-variants tournament to compare the sets of solutions, ensuring a fair comparison, with 6 measures of quality and diversity, and (2) propose two tournament-informed task selection methods to promote higher quality and diversity at each generation"
        ],
        "methods": [
          "The original algorithm selects new tasks based on a behavioral criterion, which may lead to undesired dynamics due to inter-side dependencies",
          "In addition, comparing sets of solutions cannot be done directly using classical QD measures due to side dependencies"
        ],
        "applications": [
          "While adversarial problems are common, classical QD cannot be easily applied to them, as both the fitness and the behavior depend on the opposing solutions"
        ]
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.19508v1",
      "title": "Rethinking Intelligence: Brain-like Neuron Network",
      "originalTitle": "Rethinking Intelligence: Brain-like Neuron Network",
      "summary": "Since their inception, artificial neural networks have relied on manually designed architectures and inductive biases to better adapt to data and tasks. With the rise of 使用多层神经网络来处理复杂模式的技术（undefined） and the expansion of parameter spaces, they have begun to exhibit brain-like functional behaviors. Nevertheless, artificial neural networks remain fundamentally different from biological neural system...",
      "plainSummary": "Since their inception, artificial neural networks have relied on manually designed architectures and inductive biases to better adapt to data and tasks. With the rise of 使用多层神经网络来处理复杂模式的技术（undefined） and the expansion of parameter spaces, they have begun to exhibit brain-like functional behaviors. Nevertheless, artificial neural networks remain fundamentally different from biological neural systems in structural organization, learning mechanisms, and evolutionary pathways. From the perspective of neuroscience, we rethink the formation and evolution of intelligence and proposes a new 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） paradigm, Brain-like 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） (BNN). We further present the first instantiation of a BNN termed LuminaNet that operates without convolutions or self-attention and is capable of autonomously modifying its architecture. We conduct 大量实验 demonstrating that LuminaNet can achieve self-evolution through dynamic architectural changes. On the CIFAR-10, LuminaNet achieves top-1 正确预测占总预测的比例（undefined） improvements of 11.19%, 5.46% over LeNet-5 and AlexNet, respectively, outperforming MLP-Mixer, ResMLP, and DeiT-Tiny among MLP/ViT architectures. On the TinyStories AI自动产生文字内容的技术（undefined） task, LuminaNet attains a 语言模型预测能力的度量，越低越好（undefined） of 8.4, comparable to a single-layer GPT-2-style 一种处理序列数据的神经网络架构，特别擅长处理语言（undefined）, while reducing computational cost by approximately 25% and peak memory usage by nearly 50%. Code and interactive structures are available at https://github.com/aaroncomo/LuminaNet.",
      "oneSentenceSummary": "【cs.NE】Weifeng Liu等Rethinking Intelligence，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Weifeng Liu",
          "chinese": null
        }
      ],
      "published": "2026-01-27T11:52:40Z",
      "categories": [
        "cs.NE"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.19508v1",
      "abstractUrl": "https://arxiv.org/abs/2601.19508v1",
      "keyInfo": {
        "contributions": [
          "Since their inception, artificial neural networks have relied on manually designed architectures and inductive biases to better adapt to data and tasks",
          "From the perspective of neuroscience, we rethink the formation and evolution of intelligence and proposes a new 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） paradigm, Brain-like 一种受人脑启发的计算模型，由许多互相连接的节点组成（undefined） (BNN)"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.19481v1",
      "title": "观察到数据后的概率（undefined） Distribution-assisted Evolutionary Dynamic 寻找最佳参数或解决方案的过程（undefined） as an Online Calibrator for Complex Social Simulations",
      "originalTitle": "Posterior Distribution-assisted Evolutionary Dynamic Optimization as an Online Calibrator for Complex Social Simulations",
      "summary": "The calibration of simulators for complex social systems aims to identify the optimal parameter that drives the output of the simulator best matching the target data observed from the system. As many social systems may change internally over time, calibration naturally becomes an online task, requiring parameters to be updated continuously to maintain the simulator's fidelity. In this work, the on...",
      "plainSummary": "The calibration of simulators for complex social systems aims to identify the optimal parameter that drives the output of the simulator best matching the target data observed from the system. As many social systems may change internally over time, calibration naturally becomes an online task, requiring parameters to be updated continuously to maintain the simulator's fidelity. In this work, the online setting is first formulated as a dynamic 寻找最佳参数或解决方案的过程（undefined） problem (DOP), requiring the search for a sequence of optimal parameters that fit the simulator to real system changes. However, in contrast to traditional DOP formulations, online calibration explicitly incorporates the observational data as the driver of environmental dynamics. Due to this fundamental difference, existing Evolutionary Dynamic 寻找最佳参数或解决方案的过程（undefined） (EDO) methods, despite being extensively studied for black-box DOPs, are ill-equipped to handle such a scenario. As a result, online calibration problems constitute a new set of challenging DOPs. Here, 我们提出 to explicitly learn the 观察到数据后的概率（undefined） distributions of the parameters and the observational data, thereby facilitating both change detection and environmental adaptation of existing EDOs for this scenario. We thus present a pretrained 观察到数据后的概率（undefined） model for implementation, and fine-tune it during the 寻找最佳参数或解决方案的过程（undefined）. Extensive tests on both economic and financial simulators verify that the 观察到数据后的概率（undefined） distribution strongly promotes EDOs in such DOPs widely existed in social science.",
      "oneSentenceSummary": "【cs.NE】Peng Yang等Posterior Distribution-assisted Evolutionary Dynamic Optimization as an Online Calibrator for Complex Social Simulations，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Peng Yang",
          "chinese": null
        },
        {
          "original": "Zhenhua Yang",
          "chinese": null
        },
        {
          "original": "Boquan Jiang",
          "chinese": null
        },
        {
          "original": "Chenkai Wang",
          "chinese": null
        },
        {
          "original": "Ke Tang",
          "chinese": null
        },
        {
          "original": "Xin Yao",
          "chinese": null
        }
      ],
      "published": "2026-01-27T11:15:06Z",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.19481v1",
      "abstractUrl": "https://arxiv.org/abs/2601.19481v1",
      "keyInfo": {
        "contributions": [
          "Here, we propose to explicitly learn the 观察到数据后的概率（undefined） distributions of the parameters and the observational data, thereby facilitating both change detection and environmental adaptation of existing EDOs for this scenario",
          "We thus present a pretrained 观察到数据后的概率（undefined） model for implementation, and fine-tune it during the 寻找最佳参数或解决方案的过程（undefined）"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.19477v1",
      "title": "ROIDS: 对噪声和扰动不敏感（undefined） Outlier-Aware Informed Down-Sampling",
      "originalTitle": "ROIDS: Robust Outlier-Aware Informed Down-Sampling",
      "summary": "Informed down-sampling (IDS) is known to improve performance in symbolic regression when combined with various selection strategies, especially tournament selection. However, recent work found that IDS's gains are not consistent across all problems. Our analysis reveals that IDS performance is worse for problems containing outliers. IDS systematically favors including outliers in subsets which pus...",
      "plainSummary": "Informed down-sampling (IDS) is known to improve performance in symbolic regression when combined with various selection strategies, especially tournament selection. However, recent work found that IDS's gains are not consistent across all problems. Our analysis reveals that IDS performance is worse for problems containing outliers. IDS systematically favors including outliers in subsets which pushes GP towards finding solutions that overfit to outliers. To address this, we introduce ROIDS (对噪声和扰动不敏感（undefined） Outlier-Aware Informed Down-Sampling), which excludes potential outliers from the sampling process of IDS. With ROIDS it is possible to keep the advantages of IDS without 模型在训练数据上表现好但在新数据上表现差（undefined） to outliers and to compete on a wide range of 用于比较性能的标准数据集或方法（undefined） problems. This is also reflected in our experiments in which ROIDS shows the desired behavior on all studied 用于比较性能的标准数据集或方法（undefined） problems. ROIDS consistently outperforms IDS on synthetic problems with added outliers as well as on a wide range of complex real-world problems, surpassing IDS on over 80% of the real-world 用于比较性能的标准数据集或方法（undefined） problems. Moreover, compared to all studied 用于对比的基准方法（undefined） approaches, ROIDS achieves the best average rank across all tested 用于比较性能的标准数据集或方法（undefined） problems. This 对噪声和扰动不敏感（undefined） behavior makes ROIDS a reliable down-sampling method for selection in symbolic regression, especially when outliers may be included in the data set.",
      "oneSentenceSummary": "【cs.NE】Alina Geiger等ROIDS，在cs.NE取得新进展。",
      "authors": [
        {
          "original": "Alina Geiger",
          "chinese": null
        },
        {
          "original": "Martin Briesch",
          "chinese": null
        },
        {
          "original": "Dominik Sobania",
          "chinese": null
        },
        {
          "original": "Franz Rothlauf",
          "chinese": null
        }
      ],
      "published": "2026-01-27T11:07:47Z",
      "categories": [
        "cs.NE"
      ],
      "primaryCategory": "cs.NE",
      "pdfUrl": "https://arxiv.org/pdf/2601.19477v1",
      "abstractUrl": "https://arxiv.org/abs/2601.19477v1",
      "keyInfo": {
        "contributions": [
          "To address this, we introduce ROIDS (对噪声和扰动不敏感（undefined） Outlier-Aware Informed Down-Sampling), which excludes potential outliers from the sampling process of IDS"
        ],
        "methods": [],
        "applications": []
      }
    },
    {
      "id": "http://arxiv.org/abs/2601.19955v1",
      "title": "NeuroAI and Beyond",
      "originalTitle": "NeuroAI and Beyond",
      "summary": "Neuroscience and 让机器模拟人类智能的技术（undefined） (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering t...",
      "plainSummary": "Neuroscience and 让机器模拟人类智能的技术（undefined） (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed 让机器模拟人类智能的技术（undefined） that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.",
      "oneSentenceSummary": "【cs.AI】Jean-Marc Fellous等NeuroAI and Beyond，使用Based on a workshop held in Au...，在cs.AI取得新进展。",
      "authors": [
        {
          "original": "Jean-Marc Fellous",
          "chinese": null
        },
        {
          "original": "Gert Cauwenberghs",
          "chinese": null
        },
        {
          "original": "Cornelia Fermüller",
          "chinese": null
        },
        {
          "original": "Yulia Sandamisrkaya",
          "chinese": null
        },
        {
          "original": "Terrence Sejnowski",
          "chinese": null
        }
      ],
      "published": "2026-01-27T01:57:51Z",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2601.19955v1",
      "abstractUrl": "https://arxiv.org/abs/2601.19955v1",
      "keyInfo": {
        "contributions": [
          "Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed 让机器模拟人类智能的技术（undefined） that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations"
        ],
        "methods": [
          "Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields"
        ],
        "applications": [
          "Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI"
        ]
      }
    }
  ]
}