# ğŸ“š arXiv AI æ¯æ—¥è®ºæ–‡æŠ¥å‘Š
**æ—¥æœŸ:** 2026å¹´2æœˆ2æ—¥æ˜ŸæœŸä¸€
**è®ºæ–‡æ•°:** 116 ç¯‡

---

## 1. cs.CV ğŸ‘ï¸

**VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation**

ğŸ‘¥ **ä½œè€…:** Hongyang Du, Junjie Ye, Xiaoyan Cong ç­‰10äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.CVã€‘Hongyang Duç­‰VideoGPAï¼Œä½¿ç”¨To address this, we introduce ...ï¼Œåœ¨cs.CVå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-é«˜æ•ˆçš„ï¼ˆé€Ÿåº¦å¿«ã€èµ„æºæ¶ˆè€—å°‘ï¼‰ self-supervised æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference ä¼˜åŒ–ï¼ˆå¯»æ‰¾æœ€ä½³å‚æ•°æˆ–è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼‰ (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming æœ€å…ˆè¿›ï¼ˆå½“å‰æœ€å¥½çš„ã€é¢†å…ˆçš„æ–¹æ³•ï¼‰ baselines in å¤§é‡å®éªŒ.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-é«˜æ•ˆçš„ï¼ˆé€Ÿåº¦å¿«ã€èµ„æºæ¶ˆè€—å°‘ï¼‰ self-supervised æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ that leverages a g...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.23286v1) | [PDF](https://arxiv.org/pdf/2601.23286v1)

---

## 2. cs.RO ğŸ¦¾

**End-to-end ä¼˜åŒ–ï¼ˆå¯»æ‰¾æœ€ä½³å‚æ•°æˆ–è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼‰ of Belief and Policy Learning in Shared Autonomy Paradigms**

ğŸ‘¥ **ä½œè€…:** MH Farhadi, Ali Rabiee, Sima Ghafoori ç­‰6äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.ROã€‘MH Farhadiç­‰End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigmsï¼Œä½¿ç”¨We validated our algorithm aga...ï¼Œåœ¨cs.ROå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a æ–°é¢–çš„ï¼ˆåˆ›æ–°çš„ã€å‰äººæœªåšè¿‡çš„ï¼‰ æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our æµç¨‹ï¼ˆæ•°æ®å¤„ç†æˆ–æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼‰ conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated ä¼˜åŒ–ï¼ˆå¯»æ‰¾æœ€ä½³å‚æ•°æˆ–è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼‰ is most beneficial in complex, goal-ambiguous scenarios, and is å¯æ³›åŒ–çš„ï¼ˆèƒ½å¤Ÿé€‚ç”¨äºæ–°åœºæ™¯ï¼‰ across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a æ–°é¢–çš„ï¼ˆåˆ›æ–°çš„ã€å‰äººæœªåšè¿‡çš„ï¼‰ æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ that fine-tunes Bayesian intent inferen...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.23285v1) | [PDF](https://arxiv.org/pdf/2601.23285v1)

---

## 3. cs.CV ğŸ‘ï¸

**User Prompting Strategies and Prompt Enhancement Methods for Open-Set ç›®æ ‡æ£€æµ‹ï¼ˆåœ¨å›¾åƒä¸­è¯†åˆ«å’Œå®šä½ç‰¹å®šç‰©ä½“ï¼‰ in XR Environments**

ğŸ‘¥ **ä½œè€…:** Junfeng Lin, Yanming Xiu, Maria Gorlatova

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.CVã€‘Junfeng Linç­‰User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environmentsï¼Œä½¿ç”¨To study prompt-conditioned ro...ï¼Œåœ¨cs.CVå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Open-set ç›®æ ‡æ£€æµ‹ï¼ˆåœ¨å›¾åƒä¸­è¯†åˆ«å’Œå®šä½ç‰¹å®šç‰©ä½“ï¼‰ (OSOD) localizes objects while identifying and rejecting unknown classes at inference. While recent OSOD models perform well on benchmarks, their behavior under realistic user prompting remains underexplored. In interactive XR settings, user-generated prompts are often ambiguous, underspecified, or overly detailed. To study prompt-conditioned robustness, we evaluate two OSOD models, GroundingDINO and YOLO-E, on real-world XR images and simulate diverse user prompting behaviors using vision-language models. We consider four prompt types: standard, underdetailed, overdetailed, and pragmatically ambiguous, and examine the impact of two enhancement strategies on these prompts. Results show that both models exhibit stable performance under underdetailed and standard prompts, while they suffer degradation under ambiguous prompts. Overdetailed prompts primarily affect GroundingDINO. Prompt enhancement substantially improves robustness under ambiguity, yielding gains exceeding 55% mIoU and 41% average confidence. Based on the findings, æˆ‘ä»¬æå‡º several prompting strategies and prompt enhancement methods for OSOD models in XR environments.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- Based on the findings, we propose several prompting strategies and prompt enhancement methods for OSOD models in XR environments

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.23281v1) | [PDF](https://arxiv.org/pdf/2601.23281v1)

---

## 4. cs.LG ğŸ§ 

**Decoupled Diffusion Sampling for Inverse Problems on Function Spaces**

ğŸ‘¥ **ä½œè€…:** Thomas Y. L. Lin, Jiachen Yao, Lufang Chiang ç­‰5äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.LGã€‘Thomas Y. L. Linç­‰Decoupled Diffusion Sampling for Inverse Problems on Function Spacesï¼Œä½¿ç”¨In contrast, our Decoupled Dif...ï¼Œåœ¨cs.LGå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
æˆ‘ä»¬æå‡º a data-é«˜æ•ˆçš„ï¼ˆé€Ÿåº¦å¿«ã€èµ„æºæ¶ˆè€—å°‘ï¼‰, physics-aware generative æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ in function space for inverse PDE problems. Existing plug-and-play diffusion åéªŒæ¦‚ç‡ï¼ˆè§‚å¯Ÿåˆ°æ•°æ®åçš„æ¦‚ç‡ï¼‰ samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient å…ˆéªŒæ¦‚ç‡ï¼ˆè§‚å¯Ÿåˆ°æ•°æ®å‰çš„æ¦‚ç‡ï¼‰, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing åéªŒæ¦‚ç‡ï¼ˆè§‚å¯Ÿåˆ°æ•°æ®åçš„æ¦‚ç‡ï¼‰ Sampling (DAPS) to avoid over-smoothing in Diffusion åéªŒæ¦‚ç‡ï¼ˆè§‚å¯Ÿåˆ°æ•°æ®åçš„æ¦‚ç‡ï¼‰ Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves æœ€å…ˆè¿›ï¼ˆå½“å‰æœ€å¥½çš„ã€é¢†å…ˆçš„æ–¹æ³•ï¼‰ performance under sparse observation, improving error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains å‡†ç¡®ç‡ï¼ˆæ­£ç¡®é¢„æµ‹å æ€»é¢„æµ‹çš„æ¯”ä¾‹ï¼‰ with 40% advantage in error compared to joint models.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- We propose a data-é«˜æ•ˆçš„ï¼ˆé€Ÿåº¦å¿«ã€èµ„æºæ¶ˆè€—å°‘ï¼‰, physics-aware generative æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ in function space for inverse PDE problems
- Existing plug-and-play diffusion åéªŒæ¦‚ç‡ï¼ˆè§‚å¯Ÿåˆ°æ•°æ®åçš„æ¦‚ç‡ï¼‰ samplers represent physics implicitly through joint coefficient-solution modeling, requiring substant...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.23280v1) | [PDF](https://arxiv.org/pdf/2601.23280v1)

---

## 5. cs.LG ğŸ§ 

**FOCUS: DLLMs Know How to Tame Their Compute Bound**

ğŸ‘¥ **ä½œè€…:** Kaihua Liang, Xin Tan, An Zhong ç­‰5äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.LGã€‘Kaihua Liangç­‰FOCUSï¼Œä½¿ç”¨In this work, we identify a ke...ï¼Œåœ¨cs.LGå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, æˆ‘ä»¬æå‡º FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling å¯æ‰©å±•çš„ï¼ˆèƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡æ•°æ®ï¼‰ throughput. ç»éªŒæ€§çš„ï¼ˆåŸºäºå®éªŒå’Œè§‚å¯Ÿçš„ï¼‰ evaluations demonstrate that FOCUS achieves up to 3.52 throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- Based on this insight, we propose FOCUS -- an inference system designed for DLLMs

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.23278v1) | [PDF](https://arxiv.org/pdf/2601.23278v1)

---

## 6. astro-ph.IM ğŸ“„

**Denoising the Deep Sky: Physics-Based CCD Noise Formation for Astronomical Imaging**

ğŸ‘¥ **ä½œè€…:** Shuhong Liu, Xining Ge, Ziying Gu ç­‰9äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€astro-ph.IMã€‘Shuhong Liuç­‰Denoising the Deep Skyï¼Œä½¿ç”¨Realistic noisy counterparts s...ï¼Œåœ¨astro-ph.IMå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Astronomical imaging remains noise-limited under practical observing constraints, while standard calibration pipelines mainly remove structured artifacts and leave stochastic noise largely unresolved. Learning-based denoising is promising, yet progress is hindered by scarce paired training data and the need for physically å¯è§£é‡Šçš„ï¼ˆèƒ½å¤Ÿè§£é‡Šå…¶å†³ç­–è¿‡ç¨‹ï¼‰ and reproducible models in scientific workflows. æˆ‘ä»¬æå‡º a physics-based noise synthesis æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ tailored to CCD noise formation. The æµç¨‹ï¼ˆæ•°æ®å¤„ç†æˆ–æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼‰ models photon shot noise, photo-response non-uniformity, dark-current noise, readout effects, and localized outliers arising from cosmic-ray hits and hot pixels. To obtain low-noise inputs for synthesis, we average multiple unregistered exposures to produce high-SNR bases. Realistic noisy counterparts synthesized from these bases using our noise model enable the construction of abundant paired datasets for ç›‘ç£å­¦ä¹ ï¼ˆä½¿ç”¨æ ‡æ³¨æ•°æ®è®­ç»ƒæ¨¡å‹ï¼‰. We further introduce a real-world dataset across multi-bands acquired with two twin ground-based telescopes, providing paired raw frames and instrument-æµç¨‹ï¼ˆæ•°æ®å¤„ç†æˆ–æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼‰ calibrated frames, together with calibration data and stacked high-SNR bases for real-world evaluation.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- We propose a physics-based noise synthesis æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ tailored to CCD noise formation
- We further introduce a real-world dataset across multi-bands acquired with two twin ground-based telescopes, providing paired raw frames and instrumen...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.23276v1) | [PDF](https://arxiv.org/pdf/2601.23276v1)

---

## 7. cs.CL ğŸ’¬

**UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection**

ğŸ‘¥ **ä½œè€…:** Siran Peng, Weisong Zhao, Tianyu Fu ç­‰9äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.CLã€‘Siran Pengç­‰UPAï¼Œåœ¨cs.CLå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Prompt agents have recently emerged as a promising paradigm for automated prompt ä¼˜åŒ–ï¼ˆå¯»æ‰¾æœ€ä½³å‚æ•°æˆ–è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼‰, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, æˆ‘ä»¬æå‡º UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ grounded in the Bradley-Terry-Luce (BTL) model. This æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt ä¼˜åŒ–ï¼ˆå¯»æ‰¾æœ€ä½³å‚æ•°æˆ–è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼‰ methods, showing that agent-style ä¼˜åŒ–ï¼ˆå¯»æ‰¾æœ€ä½³å‚æ•°æˆ–è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼‰ remains highly effective even in fully unsupervised settings.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.23273v1) | [PDF](https://arxiv.org/pdf/2601.23273v1)

---

## 8. cs.RO ğŸ¦¾

**IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models**

ğŸ‘¥ **ä½œè€…:** Seyed Ahmad Hosseini Miangoleh, Amin Jalal Aghdasian, Farzaneh Abdollahi

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.ROã€‘Seyed Ahmad Hosseini Miangolehç­‰IRL-DALï¼Œä½¿ç”¨This paper proposes a æ–°é¢–çš„ï¼ˆåˆ›æ–°çš„ã€...ï¼Œåœ¨cs.ROå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
This paper proposes a æ–°é¢–çš„ï¼ˆåˆ›æ–°çš„ã€å‰äººæœªåšè¿‡çš„ï¼‰ inverse å¼ºåŒ–å­¦ä¹ ï¼ˆé€šè¿‡è¯•é”™å­¦ä¹ æœ€ä½³ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼‰ æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. å¼ºåŒ–å­¦ä¹ ï¼ˆé€šè¿‡è¯•é”™å­¦ä¹ æœ€ä½³ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼‰ (RL) is then performed with a hybrid reward that combines diffuse environmental feedback and targeted IRL rewards. A conditional diffusion model, which acts as a safety supervisor, plans safe paths. It stays in its lane, avoids obstacles, and moves smoothly. Then, a learnable adaptive mask (LAM) improves perception. It shifts visual attention based on vehicle speed and nearby hazards. After FSM-based imitation, the policy is fine-tuned with Proximal Policy ä¼˜åŒ–ï¼ˆå¯»æ‰¾æœ€ä½³å‚æ•°æˆ–è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼‰ (PPO). Training is run in the Webots simulator with a two-stage curriculum. A 96\% success rate is reached, and collisions are reduced to 0.05 per 1k steps, marking a new åŸºå‡†ï¼ˆç”¨äºæ¯”è¾ƒæ€§èƒ½çš„æ ‡å‡†æ•°æ®é›†æˆ–æ–¹æ³•ï¼‰ for safe navigation. By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness.We make our code publicly available.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- This paper proposes a æ–°é¢–çš„ï¼ˆåˆ›æ–°çš„ã€å‰äººæœªåšè¿‡çš„ï¼‰ inverse å¼ºåŒ–å­¦ä¹ ï¼ˆé€šè¿‡è¯•é”™å­¦ä¹ æœ€ä½³ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼‰ æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ using a diffusion-based adaptive lookahead planner (IRL-DAL) for ...
- By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.23266v1) | [PDF](https://arxiv.org/pdf/2601.23266v1)

---

## 9. cs.CL ğŸ’¬

**PaperBanana: Automating Academic Illustration for AI Scientists**

ğŸ‘¥ **ä½œè€…:** Dawei Zhu, Rui Meng, Yale Song ç­‰7äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.CLã€‘Dawei Zhuç­‰PaperBananaï¼Œåœ¨cs.CLå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ for automated generation of publication-ready academic illustrations. Powered by æœ€å…ˆè¿›ï¼ˆå½“å‰æœ€å¥½çš„ã€é¢†å…ˆçš„æ–¹æ³•ï¼‰ VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. å…¨é¢çš„ï¼ˆè¦†ç›–å¹¿æ³›çš„ã€è¯¦ç»†çš„ï¼‰ experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that æˆ‘ä»¬çš„æ–¹æ³• effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- To lift this burden, we introduce PaperBanana, an agentic æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰ for automated generation of publication-ready academic illustrations
- To rigorously evaluate our æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.23265v1) | [PDF](https://arxiv.org/pdf/2601.23265v1)

---

## 10. cs.LG ğŸ§ 

**Particle-Guided Diffusion Models for Partial Differential Equations**

ğŸ‘¥ **ä½œè€…:** Andrew Millard, Fredrik Lindsten, Zheng Zhao

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.LGã€‘Andrew Millardç­‰Particle-Guided Diffusion Models for Partial Differential Equationsï¼Œåœ¨cs.LGå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
We introduce a guided stochastic sampling method that augments sampling from diffusion models with physics-based guidance derived from partial differential equation (PDE) residuals and observational constraints, ensuring generated samples remain physically admissible. We embed this sampling procedure within a new Sequential Monte Carlo (SMC) æ¡†æ¶ï¼ˆæä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼‰, yielding a å¯æ‰©å±•çš„ï¼ˆèƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡æ•°æ®ï¼‰ generative PDE solver. Across multiple åŸºå‡†ï¼ˆç”¨äºæ¯”è¾ƒæ€§èƒ½çš„æ ‡å‡†æ•°æ®é›†æˆ–æ–¹æ³•ï¼‰ PDE systems as well as multiphysics and interacting PDE systems, æˆ‘ä»¬çš„æ–¹æ³• produces solution fields with lower numerical error than existing æœ€å…ˆè¿›ï¼ˆå½“å‰æœ€å¥½çš„ã€é¢†å…ˆçš„æ–¹æ³•ï¼‰ generative methods.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- We introduce a guided stochastic sampling method that augments sampling from diffusion models with physics-based guidance derived from partial differe...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.23262v1) | [PDF](https://arxiv.org/pdf/2601.23262v1)

---

## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯

**åˆ†ç±»åˆ†å¸ƒ:**
- ğŸ§  cs.LG: 24 ç¯‡
- ğŸ‘ï¸ cs.CV: 16 ç¯‡
- ğŸ’¬ cs.CL: 15 ç¯‡
- ğŸ§¬ cs.NE: 15 ç¯‡
- ğŸ¦¾ cs.RO: 14 ç¯‡
- ğŸ“ˆ stat.ML: 12 ç¯‡
- ğŸ¤– cs.AI: 8 ç¯‡
- ğŸ“„ eess.IV: 4 ç¯‡
- ğŸ“„ astro-ph.IM: 1 ç¯‡
- ğŸ“„ stat.CO: 1 ç¯‡
- ğŸ“„ cs.MA: 1 ç¯‡
- ğŸ“„ q-bio.BM: 1 ç¯‡
- ğŸ“„ cs.SD: 1 ç¯‡
- ğŸ“„ cs.GR: 1 ç¯‡
- â™¿ cs.HC: 1 ç¯‡
- ğŸ“„ stat.ME: 1 ç¯‡

