# ğŸ“š arXiv AI æ¯æ—¥è®ºæ–‡æŠ¥å‘Š
**æ—¥æœŸ:** 2026å¹´2æœˆ1æ—¥æ˜ŸæœŸæ—¥
**è®ºæ–‡æ•°:** 116 ç¯‡

---

## 1. cs.CR ğŸ“„

**RedSage: A Cybersecurity Generalist LLM**

ğŸ‘¥ **ä½œè€…:** Naufal Suryanto, Muzammal Naseer, Pengfei Li ç­‰8äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.CRã€‘Naufal Suryantoç­‰RedSageï¼Œåœ¨cs.CRå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation æ•°æ®å¤„ç†æˆ–æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼ˆundefinedï¼‰ that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œå°å¹…è°ƒæ•´ï¼ˆundefinedï¼‰. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a ç”¨äºæ¯”è¾ƒæ€§èƒ½çš„æ ‡å‡†æ•°æ®é›†æˆ–æ–¹æ³•ï¼ˆundefinedï¼‰ with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the ç”¨äºå¯¹æ¯”çš„åŸºå‡†æ–¹æ³•ï¼ˆundefinedï¼‰ models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- Building on this, we design an agentic augmentation æ•°æ®å¤„ç†æˆ–æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼ˆundefinedï¼‰ that simulates expert workflows to generate 266K multi-turn cybersecurit...
- To rigorously evaluate the models, we introduce RedSage-Bench, a ç”¨äºæ¯”è¾ƒæ€§èƒ½çš„æ ‡å‡†æ•°æ®é›†æˆ–æ–¹æ³•ï¼ˆundefinedï¼‰ with 30K multiple-choice and 240 open-ended Q&A items cove...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.22159v1) | [PDF](https://arxiv.org/pdf/2601.22159v1)

---

## 2. cs.CV ğŸ‘ï¸

**One-step Latent-free Image Generation with Pixel Mean Flows**

ğŸ‘¥ **ä½œè€…:** Yiyang Lu, Susie Lu, Qiao Sun ç­‰9äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.CVã€‘Yiyang Luç­‰One-step Latent-free Image Generation with Pixel Mean Flowsï¼Œä½¿ç”¨Modern diffusion/flow-based mo...ï¼Œåœ¨cs.CVå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a æ•°æ®çš„å‹ç¼©è¡¨ç¤ºç©ºé—´ï¼ˆundefinedï¼‰. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF)
- The network target is designed to be on a presumed low-dimensional image manifold (i

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.22158v1) | [PDF](https://arxiv.org/pdf/2601.22158v1)

---

## 3. cs.LG ğŸ§ 

**Discovering Hidden Gems in Model Repositories**

ğŸ‘¥ **ä½œè€…:** Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.LGã€‘Jonathan Kahanaç­‰Discovering Hidden Gems in Model Repositoriesï¼Œä½¿ç”¨We therefore formulate model d...ï¼Œåœ¨cs.LGå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects é€Ÿåº¦å¿«ã€èµ„æºæ¶ˆè€—å°‘ï¼ˆundefinedï¼‰ market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of "hidden gems", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. æˆ‘ä»¬çš„æ–¹æ³• retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.22157v1) | [PDF](https://arxiv.org/pdf/2601.22157v1)

---

## 4. cs.CL ğŸ’¬

**Hybrid Linear Attention Done Right: é€Ÿåº¦å¿«ã€èµ„æºæ¶ˆè€—å°‘ï¼ˆundefinedï¼‰ Distillation and Effective Architectures for Extremely Long Contexts**

ğŸ‘¥ **ä½œè€…:** Yingfa Chen, Zhen Leng Thai, Zihan Zhou ç­‰9äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.CLã€‘Yingfa Chenç­‰Hybrid Linear Attention Done Rightï¼Œä½¿ç”¨We convert the Qwen3 series in...ï¼Œåœ¨cs.CLå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Hybrid ä¸€ç§å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†è¯­è¨€ï¼ˆundefinedï¼‰ architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over ä¸€ç§å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†è¯­è¨€ï¼ˆundefinedï¼‰-based models. In this paper, we present HALO (Hybrid Attention via Layer å¯»æ‰¾æœ€ä½³å‚æ•°æˆ–è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼ˆundefinedï¼‰), a æ•°æ®å¤„ç†æˆ–æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼ˆundefinedï¼‰ for distilling ä¸€ç§å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†è¯­è¨€ï¼ˆundefinedï¼‰ models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a åˆ›æ–°çš„ã€å‰äººæœªåšè¿‡çš„ï¼ˆundefinedï¼‰ position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original ä¸€ç§å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†è¯­è¨€ï¼ˆundefinedï¼‰ models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- In this paper, we present HALO (Hybrid Attention via Layer å¯»æ‰¾æœ€ä½³å‚æ•°æˆ–è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼ˆundefinedï¼‰), a æ•°æ®å¤„ç†æˆ–æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼ˆundefinedï¼‰ for distilling ä¸€ç§å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç‰¹åˆ«æ“…é•¿...
- We then present HypeNet, a hybrid architecture with superior length generalization enabled by a åˆ›æ–°çš„ã€å‰äººæœªåšè¿‡çš„ï¼ˆundefinedï¼‰ position encoding scheme (named ...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.22156v1) | [PDF](https://arxiv.org/pdf/2601.22156v1)

---

## 5. cs.AI ğŸ¤–

**Exploring Reasoning Reward Model for Agents**

ğŸ‘¥ **ä½œè€…:** Kaixuan Fan, Kaituo Feng, Manyuan Zhang ç­‰10äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.AIã€‘Kaixuan Fanç­‰Exploring Reasoning Reward Model for Agentsï¼Œåœ¨cs.AIå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Agentic é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä½³ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆundefinedï¼‰ (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic traje...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.22154v1) | [PDF](https://arxiv.org/pdf/2601.22154v1)

---

## 6. cs.CV ğŸ‘ï¸

**UEval: A ç”¨äºæ¯”è¾ƒæ€§èƒ½çš„æ ‡å‡†æ•°æ®é›†æˆ–æ–¹æ³•ï¼ˆundefinedï¼‰ for Unified Multimodal Generation**

ğŸ‘¥ **ä½œè€…:** Bo Li, Yida Yin, Wenhao Chai ç­‰5äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.CVã€‘Bo Liç­‰UEvalï¼Œåœ¨cs.CVå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
We introduce UEval, a ç”¨äºæ¯”è¾ƒæ€§èƒ½çš„æ ‡å‡†æ•°æ®é›†æˆ–æ–¹æ³•ï¼ˆundefinedï¼‰ to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text æ­£ç¡®é¢„æµ‹å æ€»é¢„æµ‹çš„æ¯”ä¾‹ï¼ˆundefinedï¼‰, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling èƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡æ•°æ®ï¼ˆundefinedï¼‰ and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- We introduce UEval, a ç”¨äºæ¯”è¾ƒæ€§èƒ½çš„æ ‡å‡†æ•°æ®é›†æˆ–æ–¹æ³•ï¼ˆundefinedï¼‰ to evaluate unified models, i
- Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text æ­£ç¡®é¢„æµ‹å æ€»é¢„æµ‹çš„æ¯”ä¾‹ï¼ˆundefinedï¼‰, we design a ...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.22155v1) | [PDF](https://arxiv.org/pdf/2601.22155v1)

---

## 7. cs.RO ğŸ¦¾

**DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation**

ğŸ‘¥ **ä½œè€…:** Haozhe Xie, Beichen Wen, Jiarui Zheng ç­‰7äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.ROã€‘Haozhe Xieç­‰DynamicVLAï¼Œä½¿ç”¨4B VLA using a convolutional v...ï¼Œåœ¨cs.ROå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a æä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼ˆundefinedï¼‰ for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially é€Ÿåº¦å¿«ã€èµ„æºæ¶ˆè€—å°‘ï¼ˆundefinedï¼‰, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) ç”¨äºæ¯”è¾ƒæ€§èƒ½çš„æ ‡å‡†æ•°æ®é›†æˆ–æ–¹æ³•ï¼ˆundefinedï¼‰, built from scratch with an auto data collection æ•°æ®å¤„ç†æˆ–æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼ˆundefinedï¼‰ that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified æä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼ˆundefinedï¼‰ for general dynamic object manipulation across embodiments.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- We present DynamicVLA, a æä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼ˆundefinedï¼‰ for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through t...
- To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) ç”¨äºæ¯”è¾ƒæ€§èƒ½çš„æ ‡å‡†æ•°æ®é›†æˆ–æ–¹æ³•ï¼ˆundefinedï¼‰, built from...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.22153v1) | [PDF](https://arxiv.org/pdf/2601.22153v1)

---

## 8. cs.LG ğŸ§ 

**Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing**

ğŸ‘¥ **ä½œè€…:** Daniel Stein, Shaoyi Huang, Rolf Drechsler ç­‰5äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.LGã€‘Daniel Steinç­‰Late Breaking Resultsï¼Œåœ¨cs.LGå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. å½“å‰æœ€å¥½çš„ã€é¢†å…ˆçš„æ–¹æ³•ï¼ˆundefinedï¼‰ research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, æˆ‘ä»¬æå‡º to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any æ­£ç¡®é¢„æµ‹å æ€»é¢„æµ‹çš„æ¯”ä¾‹ï¼ˆundefinedï¼‰ degradation. The code is open source at https://github.com/TUDa-HWAI/NN2Logic

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.22151v1) | [PDF](https://arxiv.org/pdf/2601.22151v1)

---

## 9. cs.CV ğŸ‘ï¸

**Do VLMs Perceive or çœŸæ­£æ­£ä¾‹ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼ˆundefinedï¼‰? Probing Visual Perception vs. Memory with Classic Visual Illusions**

ğŸ‘¥ **ä½œè€…:** Xiaoxiao Sun, Mingyang Li, Kun yuan ç­‰10äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.CVã€‘Xiaoxiao Sunç­‰Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusionsï¼Œä½¿ç”¨Unlike è§‚å¯Ÿåˆ°æ•°æ®å‰çš„æ¦‚ç‡ï¼ˆundefinedï¼‰ wo...ï¼Œåœ¨cs.CVå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
Large Vision-Language Models (VLMs) often answer classic visual illusions "correctly" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely çœŸæ­£æ­£ä¾‹ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼ˆundefinedï¼‰ memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion æä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼ˆundefinedï¼‰ with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven çœŸæ­£æ­£ä¾‹ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼ˆundefinedï¼‰. Unlike è§‚å¯Ÿåˆ°æ•°æ®å‰çš„æ¦‚ç‡ï¼ˆundefinedï¼‰ work that focuses on averaged æ­£ç¡®é¢„æµ‹å æ€»é¢„æµ‹çš„æ¯”ä¾‹ï¼ˆundefinedï¼‰, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion æä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼ˆundefinedï¼‰ with grade...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.22150v1) | [PDF](https://arxiv.org/pdf/2601.22150v1)

---

## 10. cs.CL ğŸ’¬

**DynaWeb: Model-Based é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä½³ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆundefinedï¼‰ of Web Agents**

ğŸ‘¥ **ä½œè€…:** Hang Ding, Peidong Liu, Junqiao Wang ç­‰10äºº

ğŸ“ **ä¸€å¥è¯æ€»ç»“:** ã€cs.CLã€‘Hang Dingç­‰DynaWebï¼Œåœ¨cs.CLå–å¾—æ–°è¿›å±•ã€‚

ğŸ“– **é€šä¿—è§£è¯»:**
The development of autonomous web agents, powered by Large Language Models (LLMs) and é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä½³ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆundefinedï¼‰ (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä½³ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆundefinedï¼‰ (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a åˆ›æ–°çš„ã€å‰äººæœªåšè¿‡çš„ï¼ˆundefinedï¼‰ MBRL æä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼ˆundefinedï¼‰ that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for é€Ÿåº¦å¿«ã€èµ„æºæ¶ˆè€—å°‘ï¼ˆundefinedï¼‰ online é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä½³ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆundefinedï¼‰. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of å½“å‰æœ€å¥½çš„ã€é¢†å…ˆçš„æ–¹æ³•ï¼ˆundefinedï¼‰ open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a èƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡æ•°æ®ï¼ˆundefinedï¼‰ and é€Ÿåº¦å¿«ã€èµ„æºæ¶ˆè€—å°‘ï¼ˆundefinedï¼‰ way to scale up online agentic RL.

ğŸ’¡ **æ ¸å¿ƒè´¡çŒ®:**
- The development of autonomous web agents, powered by Large Language Models (LLMs) and é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä½³ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆundefinedï¼‰ (RL), represents a significant step...
- This paper introduces DynaWeb, a åˆ›æ–°çš„ã€å‰äººæœªåšè¿‡çš„ï¼ˆundefinedï¼‰ MBRL æä¾›ç»“æ„çš„åŸºç¡€ä»£ç åº“ï¼ˆundefinedï¼‰ that trains web agents through interacting with a web world model tr...

ğŸ”— **é“¾æ¥:** [è®ºæ–‡](https://arxiv.org/abs/2601.22149v1) | [PDF](https://arxiv.org/pdf/2601.22149v1)

---

## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯

**åˆ†ç±»åˆ†å¸ƒ:**
- ğŸ§  cs.LG: 21 ç¯‡
- ğŸ¦¾ cs.RO: 17 ç¯‡
- ğŸ‘ï¸ cs.CV: 16 ç¯‡
- ğŸ’¬ cs.CL: 16 ç¯‡
- ğŸ§¬ cs.NE: 14 ç¯‡
- ğŸ¤– cs.AI: 10 ç¯‡
- ğŸ“ˆ stat.ML: 8 ç¯‡
- ğŸ“„ math.OC: 2 ç¯‡
- ğŸ“„ cs.CR: 1 ç¯‡
- ğŸ“„ cs.GR: 1 ç¯‡
- ğŸ“„ cs.SE: 1 ç¯‡
- ğŸ“„ q-fin.CP: 1 ç¯‡
- ğŸ“„ q-fin.TR: 1 ç¯‡
- ğŸ“„ cs.CY: 1 ç¯‡
- ğŸ“„ physics.flu-dyn: 1 ç¯‡
- ğŸ“„ cs.MA: 1 ç¯‡
- ğŸ“„ quant-ph: 1 ç¯‡
- ğŸ“„ eess.SP: 1 ç¯‡
- ğŸ“„ eess.IV: 1 ç¯‡
- ğŸ“„ stat.CO: 1 ç¯‡

